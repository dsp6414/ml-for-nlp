Loading feature files...
SOS: 215
EOS: 302
All scenes loaded.
Hyperparameters: Namespace(LR=0.01, alternatives=1, batch_size=100, dec='MLP', dropout=0.0, epochs=20, hidden_sz=50, log_interval=10, model='s0', no_cuda=False, seed=1)
Speaker0:  Listener0Model(
  (scene_encoder): LinearSceneEncoder(
    (fc): Linear(in_features=280, out_features=50)
  )
  (string_encoder): LinearStringEncoder(
    (fc): Linear(in_features=2713, out_features=50)
  )
  (scorer): MLPScorer(
    (linear_4): Linear(in_features=50, out_features=50)
    (linear_5): Linear(in_features=50, out_features=50)
    (linear_3): Linear(in_features=50, out_features=1)
  )
)
Epoch [1/20], Step[0/483], loss: 7.8999
Epoch [1/20], Step[10/483], loss: 4.0939
Epoch [1/20], Step[20/483], loss: 3.5032
Epoch [1/20], Step[30/483], loss: 3.3509
Epoch [1/20], Step[40/483], loss: 3.2884
Epoch [1/20], Step[50/483], loss: 3.5589
Epoch [1/20], Step[60/483], loss: 3.5256
Epoch [1/20], Step[70/483], loss: 3.4029
Epoch [1/20], Step[80/483], loss: 3.6978
Epoch [1/20], Step[90/483], loss: 2.8643
Epoch [1/20], Step[100/483], loss: 2.7546
Epoch [1/20], Step[110/483], loss: 3.1591
Epoch [1/20], Step[120/483], loss: 3.4846
Epoch [1/20], Step[130/483], loss: 3.2872
Epoch [1/20], Step[140/483], loss: 3.4302
Epoch [1/20], Step[150/483], loss: 3.5021
Epoch [1/20], Step[160/483], loss: 3.5265
Epoch [1/20], Step[170/483], loss: 2.5800
Epoch [1/20], Step[180/483], loss: 3.4286
Epoch [1/20], Step[190/483], loss: 3.4580
Epoch [1/20], Step[200/483], loss: 2.5704
Epoch [1/20], Step[210/483], loss: 3.1095
Epoch [1/20], Step[220/483], loss: 2.9705
Epoch [1/20], Step[230/483], loss: 2.7809
Epoch [1/20], Step[240/483], loss: 3.2354
Epoch [1/20], Step[250/483], loss: 3.4581
Epoch [1/20], Step[260/483], loss: 2.4057
Epoch [1/20], Step[270/483], loss: 3.3412
Epoch [1/20], Step[280/483], loss: 2.8760
Epoch [1/20], Step[290/483], loss: 2.5209
Epoch [1/20], Step[300/483], loss: 3.3878
Epoch [1/20], Step[310/483], loss: 3.3691
Epoch [1/20], Step[320/483], loss: 3.4870
Epoch [1/20], Step[330/483], loss: 3.3040
Epoch [1/20], Step[340/483], loss: 3.6117
Epoch [1/20], Step[350/483], loss: 3.2779
Epoch [1/20], Step[360/483], loss: 2.7004
Epoch [1/20], Step[370/483], loss: 2.3857
Epoch [1/20], Step[380/483], loss: 3.1475
Epoch [1/20], Step[390/483], loss: 3.3278
Epoch [1/20], Step[400/483], loss: 3.3266
Epoch [1/20], Step[410/483], loss: 2.7015
Epoch [1/20], Step[420/483], loss: 2.9597
Epoch [1/20], Step[430/483], loss: 2.9179
Epoch [1/20], Step[440/483], loss: 3.4173
Epoch [1/20], Step[450/483], loss: 2.6200
Epoch [1/20], Step[460/483], loss: 3.3027
Epoch [1/20], Step[470/483], loss: 3.5442
Epoch [1/20], Step[480/483], loss: 2.3660
====> Epoch 1: Training loss: 1577.5527
Epoch [2/20], Step[0/483], loss: 2.9704
Epoch [2/20], Step[10/483], loss: 3.0702
Epoch [2/20], Step[20/483], loss: 3.2977
Epoch [2/20], Step[30/483], loss: 3.1770
Epoch [2/20], Step[40/483], loss: 3.1882
Epoch [2/20], Step[50/483], loss: 3.5068
Epoch [2/20], Step[60/483], loss: 3.3947
Epoch [2/20], Step[70/483], loss: 3.3881
Epoch [2/20], Step[80/483], loss: 3.6809
Epoch [2/20], Step[90/483], loss: 2.8509
Epoch [2/20], Step[100/483], loss: 2.7233
Epoch [2/20], Step[110/483], loss: 3.0836
Epoch [2/20], Step[120/483], loss: 3.4578
Epoch [2/20], Step[130/483], loss: 3.2784
Epoch [2/20], Step[140/483], loss: 3.4812
Epoch [2/20], Step[150/483], loss: 3.4462
Epoch [2/20], Step[160/483], loss: 3.4703
Epoch [2/20], Step[170/483], loss: 2.5789
Epoch [2/20], Step[180/483], loss: 3.3768
Epoch [2/20], Step[190/483], loss: 3.4217
Epoch [2/20], Step[200/483], loss: 2.5429
Epoch [2/20], Step[210/483], loss: 3.0625
Epoch [2/20], Step[220/483], loss: 2.9533
Epoch [2/20], Step[230/483], loss: 2.6649
Epoch [2/20], Step[240/483], loss: 3.2041
Epoch [2/20], Step[250/483], loss: 3.4444
Epoch [2/20], Step[260/483], loss: 2.3829
Epoch [2/20], Step[270/483], loss: 3.3198
Epoch [2/20], Step[280/483], loss: 2.8468
Epoch [2/20], Step[290/483], loss: 2.4861
Epoch [2/20], Step[300/483], loss: 3.3730
Epoch [2/20], Step[310/483], loss: 3.3497
Epoch [2/20], Step[320/483], loss: 3.4753
Epoch [2/20], Step[330/483], loss: 3.2762
Epoch [2/20], Step[340/483], loss: 3.5762
Epoch [2/20], Step[350/483], loss: 3.2427
Epoch [2/20], Step[360/483], loss: 2.6594
Epoch [2/20], Step[370/483], loss: 2.2872
Epoch [2/20], Step[380/483], loss: 3.0596
Epoch [2/20], Step[390/483], loss: 3.2792
Epoch [2/20], Step[400/483], loss: 3.3092
Epoch [2/20], Step[410/483], loss: 2.6731
Epoch [2/20], Step[420/483], loss: 2.9420
Epoch [2/20], Step[430/483], loss: 2.8908
Epoch [2/20], Step[440/483], loss: 3.3777
Epoch [2/20], Step[450/483], loss: 2.5611
Epoch [2/20], Step[460/483], loss: 3.2700
Epoch [2/20], Step[470/483], loss: 3.5238
Epoch [2/20], Step[480/483], loss: 2.3135
====> Epoch 2: Training loss: 1535.1774
Epoch [3/20], Step[0/483], loss: 2.9601
Epoch [3/20], Step[10/483], loss: 3.0629
Epoch [3/20], Step[20/483], loss: 3.2827
Epoch [3/20], Step[30/483], loss: 3.1792
Epoch [3/20], Step[40/483], loss: 3.1927
Epoch [3/20], Step[50/483], loss: 3.4889
Epoch [3/20], Step[60/483], loss: 3.3895
Epoch [3/20], Step[70/483], loss: 3.3430
Epoch [3/20], Step[80/483], loss: 3.6791
Epoch [3/20], Step[90/483], loss: 2.8523
Epoch [3/20], Step[100/483], loss: 2.7169
Epoch [3/20], Step[110/483], loss: 3.0769
Epoch [3/20], Step[120/483], loss: 3.4360
Epoch [3/20], Step[130/483], loss: 3.2626
Epoch [3/20], Step[140/483], loss: 3.4020
Epoch [3/20], Step[150/483], loss: 3.4366
Epoch [3/20], Step[160/483], loss: 3.4496
Epoch [3/20], Step[170/483], loss: 2.5663
Epoch [3/20], Step[180/483], loss: 3.3785
Epoch [3/20], Step[190/483], loss: 3.4144
Epoch [3/20], Step[200/483], loss: 2.5436
Epoch [3/20], Step[210/483], loss: 3.0573
Epoch [3/20], Step[220/483], loss: 2.9622
Epoch [3/20], Step[230/483], loss: 2.6640
Epoch [3/20], Step[240/483], loss: 3.2047
Epoch [3/20], Step[250/483], loss: 3.4621
Epoch [3/20], Step[260/483], loss: 2.3887
Epoch [3/20], Step[270/483], loss: 3.3065
Epoch [3/20], Step[280/483], loss: 2.8455
Epoch [3/20], Step[290/483], loss: 2.4874
Epoch [3/20], Step[300/483], loss: 3.3691
Epoch [3/20], Step[310/483], loss: 3.3563
Epoch [3/20], Step[320/483], loss: 3.4410
Epoch [3/20], Step[330/483], loss: 3.2528
Epoch [3/20], Step[340/483], loss: 3.7239
Epoch [3/20], Step[350/483], loss: 3.2263
Epoch [3/20], Step[360/483], loss: 2.6618
Epoch [3/20], Step[370/483], loss: 2.3256
Epoch [3/20], Step[380/483], loss: 3.0983
Epoch [3/20], Step[390/483], loss: 3.2740
Epoch [3/20], Step[400/483], loss: 3.3185
Epoch [3/20], Step[410/483], loss: 2.6740
Epoch [3/20], Step[420/483], loss: 2.9523
Epoch [3/20], Step[430/483], loss: 2.8781
Epoch [3/20], Step[440/483], loss: 3.3754
Epoch [3/20], Step[450/483], loss: 2.5605
Epoch [3/20], Step[460/483], loss: 3.2694
Epoch [3/20], Step[470/483], loss: 3.5305
Epoch [3/20], Step[480/483], loss: 2.3371
====> Epoch 3: Training loss: 1531.9912
Epoch [4/20], Step[0/483], loss: 2.9503
Epoch [4/20], Step[10/483], loss: 3.0620
Epoch [4/20], Step[20/483], loss: 3.2793
Epoch [4/20], Step[30/483], loss: 3.1801
Epoch [4/20], Step[40/483], loss: 3.1819
Epoch [4/20], Step[50/483], loss: 3.4949
Epoch [4/20], Step[60/483], loss: 3.3998
Epoch [4/20], Step[70/483], loss: 3.3322
Epoch [4/20], Step[80/483], loss: 3.6740
Epoch [4/20], Step[90/483], loss: 2.8468
Epoch [4/20], Step[100/483], loss: 2.7222
Epoch [4/20], Step[110/483], loss: 3.0713
Epoch [4/20], Step[120/483], loss: 3.4291
Epoch [4/20], Step[130/483], loss: 3.2642
Epoch [4/20], Step[140/483], loss: 3.3865
Epoch [4/20], Step[150/483], loss: 3.4254
Epoch [4/20], Step[160/483], loss: 3.4490
Epoch [4/20], Step[170/483], loss: 2.5748
Epoch [4/20], Step[180/483], loss: 3.3590
Epoch [4/20], Step[190/483], loss: 3.4245
Epoch [4/20], Step[200/483], loss: 2.5576
Epoch [4/20], Step[210/483], loss: 3.0423
Epoch [4/20], Step[220/483], loss: 2.9552
Epoch [4/20], Step[230/483], loss: 2.6689
Epoch [4/20], Step[240/483], loss: 3.1952
Epoch [4/20], Step[250/483], loss: 3.4487
Epoch [4/20], Step[260/483], loss: 2.3832
Epoch [4/20], Step[270/483], loss: 3.2926
Epoch [4/20], Step[280/483], loss: 2.8457
Epoch [4/20], Step[290/483], loss: 2.4869
Epoch [4/20], Step[300/483], loss: 3.3595
Epoch [4/20], Step[310/483], loss: 3.3433
Epoch [4/20], Step[320/483], loss: 3.4294
Epoch [4/20], Step[330/483], loss: 3.2548
Epoch [4/20], Step[340/483], loss: 3.5710
Epoch [4/20], Step[350/483], loss: 3.2238
Epoch [4/20], Step[360/483], loss: 2.6357
Epoch [4/20], Step[370/483], loss: 2.2747
Epoch [4/20], Step[380/483], loss: 3.0470
Epoch [4/20], Step[390/483], loss: 3.2697
Epoch [4/20], Step[400/483], loss: 3.2891
Epoch [4/20], Step[410/483], loss: 2.6694
Epoch [4/20], Step[420/483], loss: 2.9436
Epoch [4/20], Step[430/483], loss: 2.8776
Epoch [4/20], Step[440/483], loss: 3.3628
Epoch [4/20], Step[450/483], loss: 2.5539
Epoch [4/20], Step[460/483], loss: 3.2616
Epoch [4/20], Step[470/483], loss: 3.5227
Epoch [4/20], Step[480/483], loss: 2.3079
====> Epoch 4: Training loss: 1528.8896
Epoch [5/20], Step[0/483], loss: 2.9470
Epoch [5/20], Step[10/483], loss: 3.0569
Epoch [5/20], Step[20/483], loss: 3.2723
Epoch [5/20], Step[30/483], loss: 3.1614
Epoch [5/20], Step[40/483], loss: 3.1666
Epoch [5/20], Step[50/483], loss: 3.4784
Epoch [5/20], Step[60/483], loss: 3.3781
Epoch [5/20], Step[70/483], loss: 3.3376
Epoch [5/20], Step[80/483], loss: 3.6430
Epoch [5/20], Step[90/483], loss: 2.8483
Epoch [5/20], Step[100/483], loss: 2.7146
Epoch [5/20], Step[110/483], loss: 3.0736
Epoch [5/20], Step[120/483], loss: 3.4369
Epoch [5/20], Step[130/483], loss: 3.2460
Epoch [5/20], Step[140/483], loss: 3.3854
Epoch [5/20], Step[150/483], loss: 3.4366
Epoch [5/20], Step[160/483], loss: 3.4532
Epoch [5/20], Step[170/483], loss: 2.5706
Epoch [5/20], Step[180/483], loss: 3.3644
Epoch [5/20], Step[190/483], loss: 3.4675
Epoch [5/20], Step[200/483], loss: 2.5322
Epoch [5/20], Step[210/483], loss: 3.0255
Epoch [5/20], Step[220/483], loss: 2.9422
Epoch [5/20], Step[230/483], loss: 2.6604
Epoch [5/20], Step[240/483], loss: 3.1851
Epoch [5/20], Step[250/483], loss: 3.4666
Epoch [5/20], Step[260/483], loss: 2.3761
Epoch [5/20], Step[270/483], loss: 3.3016
Epoch [5/20], Step[280/483], loss: 2.8452
Epoch [5/20], Step[290/483], loss: 2.4771
Epoch [5/20], Step[300/483], loss: 3.3507
Epoch [5/20], Step[310/483], loss: 3.3355
Epoch [5/20], Step[320/483], loss: 3.4325
Epoch [5/20], Step[330/483], loss: 3.2581
Epoch [5/20], Step[340/483], loss: 3.5624
Epoch [5/20], Step[350/483], loss: 3.2308
Epoch [5/20], Step[360/483], loss: 2.6508
Epoch [5/20], Step[370/483], loss: 2.2703
Epoch [5/20], Step[380/483], loss: 3.0550
Epoch [5/20], Step[390/483], loss: 3.2713
Epoch [5/20], Step[400/483], loss: 3.2930
Epoch [5/20], Step[410/483], loss: 2.6697
Epoch [5/20], Step[420/483], loss: 2.9293
Epoch [5/20], Step[430/483], loss: 2.8765
Epoch [5/20], Step[440/483], loss: 3.3652
Epoch [5/20], Step[450/483], loss: 2.5549
Epoch [5/20], Step[460/483], loss: 3.2592
Epoch [5/20], Step[470/483], loss: 3.5153
Epoch [5/20], Step[480/483], loss: 2.3270
====> Epoch 5: Training loss: 1527.8147
Epoch [6/20], Step[0/483], loss: 2.9460
Epoch [6/20], Step[10/483], loss: 3.0659
Epoch [6/20], Step[20/483], loss: 3.2735
Epoch [6/20], Step[30/483], loss: 3.1675
Epoch [6/20], Step[40/483], loss: 3.1729
Epoch [6/20], Step[50/483], loss: 3.4817
Epoch [6/20], Step[60/483], loss: 3.3889
Epoch [6/20], Step[70/483], loss: 3.3405
Epoch [6/20], Step[80/483], loss: 3.6430
Epoch [6/20], Step[90/483], loss: 2.8372
Epoch [6/20], Step[100/483], loss: 2.7167
Epoch [6/20], Step[110/483], loss: 3.0758
Epoch [6/20], Step[120/483], loss: 3.4460
Epoch [6/20], Step[130/483], loss: 3.2413
Epoch [6/20], Step[140/483], loss: 3.3748
Epoch [6/20], Step[150/483], loss: 3.4165
Epoch [6/20], Step[160/483], loss: 3.4481
Epoch [6/20], Step[170/483], loss: 2.5612
Epoch [6/20], Step[180/483], loss: 3.3674
Epoch [6/20], Step[190/483], loss: 3.4153
Epoch [6/20], Step[200/483], loss: 2.5718
Epoch [6/20], Step[210/483], loss: 3.0485
Epoch [6/20], Step[220/483], loss: 2.9466
Epoch [6/20], Step[230/483], loss: 2.6594
Epoch [6/20], Step[240/483], loss: 3.1889
Epoch [6/20], Step[250/483], loss: 3.4588
Epoch [6/20], Step[260/483], loss: 2.3667
Epoch [6/20], Step[270/483], loss: 3.2983
Epoch [6/20], Step[280/483], loss: 2.8496
Epoch [6/20], Step[290/483], loss: 2.4803
Epoch [6/20], Step[300/483], loss: 3.3466
Epoch [6/20], Step[310/483], loss: 3.3448
Epoch [6/20], Step[320/483], loss: 3.4674
Epoch [6/20], Step[330/483], loss: 3.2642
Epoch [6/20], Step[340/483], loss: 3.5515
Epoch [6/20], Step[350/483], loss: 3.2200
Epoch [6/20], Step[360/483], loss: 2.6351
Epoch [6/20], Step[370/483], loss: 2.2627
Epoch [6/20], Step[380/483], loss: 3.0455
Epoch [6/20], Step[390/483], loss: 3.2743
Epoch [6/20], Step[400/483], loss: 3.3025
Epoch [6/20], Step[410/483], loss: 2.6765
Epoch [6/20], Step[420/483], loss: 2.9474
Epoch [6/20], Step[430/483], loss: 2.8694
Epoch [6/20], Step[440/483], loss: 3.3629
Epoch [6/20], Step[450/483], loss: 2.5569
Epoch [6/20], Step[460/483], loss: 3.2509
Epoch [6/20], Step[470/483], loss: 3.5177
Epoch [6/20], Step[480/483], loss: 2.3138
====> Epoch 6: Training loss: 1527.3244
Epoch [7/20], Step[0/483], loss: 2.9445
Epoch [7/20], Step[10/483], loss: 3.0588
Epoch [7/20], Step[20/483], loss: 3.2799
Epoch [7/20], Step[30/483], loss: 3.1743
Epoch [7/20], Step[40/483], loss: 3.1516
Epoch [7/20], Step[50/483], loss: 3.4847
Epoch [7/20], Step[60/483], loss: 3.3821
Epoch [7/20], Step[70/483], loss: 3.3293
Epoch [7/20], Step[80/483], loss: 3.6440
Epoch [7/20], Step[90/483], loss: 2.8440
Epoch [7/20], Step[100/483], loss: 2.7079
Epoch [7/20], Step[110/483], loss: 3.0690
Epoch [7/20], Step[120/483], loss: 3.4343
Epoch [7/20], Step[130/483], loss: 3.2405
Epoch [7/20], Step[140/483], loss: 3.3886
Epoch [7/20], Step[150/483], loss: 3.4342
Epoch [7/20], Step[160/483], loss: 3.4645
Epoch [7/20], Step[170/483], loss: 2.5615
Epoch [7/20], Step[180/483], loss: 3.3721
Epoch [7/20], Step[190/483], loss: 3.4108
Epoch [7/20], Step[200/483], loss: 2.5507
Epoch [7/20], Step[210/483], loss: 3.0395
Epoch [7/20], Step[220/483], loss: 2.9457
Epoch [7/20], Step[230/483], loss: 2.6688
Epoch [7/20], Step[240/483], loss: 3.1830
Epoch [7/20], Step[250/483], loss: 3.4455
Epoch [7/20], Step[260/483], loss: 2.3723
Epoch [7/20], Step[270/483], loss: 3.2977
Epoch [7/20], Step[280/483], loss: 2.8430
Epoch [7/20], Step[290/483], loss: 2.4794
Epoch [7/20], Step[300/483], loss: 3.3526
Epoch [7/20], Step[310/483], loss: 3.3414
Epoch [7/20], Step[320/483], loss: 3.4310
Epoch [7/20], Step[330/483], loss: 3.2409
Epoch [7/20], Step[340/483], loss: 3.5804
Epoch [7/20], Step[350/483], loss: 3.2139
Epoch [7/20], Step[360/483], loss: 2.6312
Epoch [7/20], Step[370/483], loss: 2.2857
Epoch [7/20], Step[380/483], loss: 3.0461
Epoch [7/20], Step[390/483], loss: 3.2740
Epoch [7/20], Step[400/483], loss: 3.2948
Epoch [7/20], Step[410/483], loss: 2.6716
Epoch [7/20], Step[420/483], loss: 2.9378
Epoch [7/20], Step[430/483], loss: 2.8695
Epoch [7/20], Step[440/483], loss: 3.3642
Epoch [7/20], Step[450/483], loss: 2.5563
Epoch [7/20], Step[460/483], loss: 3.2522
Epoch [7/20], Step[470/483], loss: 3.5180
Epoch [7/20], Step[480/483], loss: 2.3069
====> Epoch 7: Training loss: 1525.7709
Epoch [8/20], Step[0/483], loss: 2.9472
Epoch [8/20], Step[10/483], loss: 3.0553
Epoch [8/20], Step[20/483], loss: 3.2672
Epoch [8/20], Step[30/483], loss: 3.1703
Epoch [8/20], Step[40/483], loss: 3.1486
Epoch [8/20], Step[50/483], loss: 3.4832
Epoch [8/20], Step[60/483], loss: 3.3782
Epoch [8/20], Step[70/483], loss: 3.3237
Epoch [8/20], Step[80/483], loss: 3.6515
Epoch [8/20], Step[90/483], loss: 2.8511
Epoch [8/20], Step[100/483], loss: 2.7125
Epoch [8/20], Step[110/483], loss: 3.0546
Epoch [8/20], Step[120/483], loss: 3.4313
Epoch [8/20], Step[130/483], loss: 3.2391
Epoch [8/20], Step[140/483], loss: 3.3909
Epoch [8/20], Step[150/483], loss: 3.4399
Epoch [8/20], Step[160/483], loss: 3.4359
Epoch [8/20], Step[170/483], loss: 2.5574
Epoch [8/20], Step[180/483], loss: 3.3632
Epoch [8/20], Step[190/483], loss: 3.4003
Epoch [8/20], Step[200/483], loss: 2.5323
Epoch [8/20], Step[210/483], loss: 3.0450
Epoch [8/20], Step[220/483], loss: 2.9540
Epoch [8/20], Step[230/483], loss: 2.6636
Epoch [8/20], Step[240/483], loss: 3.1764
Epoch [8/20], Step[250/483], loss: 3.4487
Epoch [8/20], Step[260/483], loss: 2.3729
Epoch [8/20], Step[270/483], loss: 3.2936
Epoch [8/20], Step[280/483], loss: 2.8404
Epoch [8/20], Step[290/483], loss: 2.4895
Epoch [8/20], Step[300/483], loss: 3.3521
Epoch [8/20], Step[310/483], loss: 3.3432
Epoch [8/20], Step[320/483], loss: 3.4207
Epoch [8/20], Step[330/483], loss: 3.2332
Epoch [8/20], Step[340/483], loss: 3.5577
Epoch [8/20], Step[350/483], loss: 3.2084
Epoch [8/20], Step[360/483], loss: 2.6358
Epoch [8/20], Step[370/483], loss: 2.2644
Epoch [8/20], Step[380/483], loss: 3.0392
Epoch [8/20], Step[390/483], loss: 3.2647
Epoch [8/20], Step[400/483], loss: 3.2926
Epoch [8/20], Step[410/483], loss: 2.6719
Epoch [8/20], Step[420/483], loss: 2.9553
Epoch [8/20], Step[430/483], loss: 2.8639
Epoch [8/20], Step[440/483], loss: 3.3607
Epoch [8/20], Step[450/483], loss: 2.5695
Epoch [8/20], Step[460/483], loss: 3.2480
Epoch [8/20], Step[470/483], loss: 3.5130
Epoch [8/20], Step[480/483], loss: 2.3027
====> Epoch 8: Training loss: 1525.1489
Epoch [9/20], Step[0/483], loss: 2.9441
Epoch [9/20], Step[10/483], loss: 3.0541
Epoch [9/20], Step[20/483], loss: 3.2586
Epoch [9/20], Step[30/483], loss: 3.1633
Epoch [9/20], Step[40/483], loss: 3.1258
Epoch [9/20], Step[50/483], loss: 3.4801
Epoch [9/20], Step[60/483], loss: 3.3772
Epoch [9/20], Step[70/483], loss: 3.3463
Epoch [9/20], Step[80/483], loss: 3.6473
Epoch [9/20], Step[90/483], loss: 2.8436
Epoch [9/20], Step[100/483], loss: 2.7269
Epoch [9/20], Step[110/483], loss: 3.0562
Epoch [9/20], Step[120/483], loss: 3.4189
Epoch [9/20], Step[130/483], loss: 3.2401
Epoch [9/20], Step[140/483], loss: 3.3911
Epoch [9/20], Step[150/483], loss: 3.4367
Epoch [9/20], Step[160/483], loss: 3.4352
Epoch [9/20], Step[170/483], loss: 2.5740
Epoch [9/20], Step[180/483], loss: 3.3685
Epoch [9/20], Step[190/483], loss: 3.3971
Epoch [9/20], Step[200/483], loss: 2.5280
Epoch [9/20], Step[210/483], loss: 3.0499
Epoch [9/20], Step[220/483], loss: 2.9584
Epoch [9/20], Step[230/483], loss: 2.6685
Epoch [9/20], Step[240/483], loss: 3.1882
Epoch [9/20], Step[250/483], loss: 3.4449
Epoch [9/20], Step[260/483], loss: 2.3635
Epoch [9/20], Step[270/483], loss: 3.3101
Epoch [9/20], Step[280/483], loss: 2.8389
Epoch [9/20], Step[290/483], loss: 2.4896
Epoch [9/20], Step[300/483], loss: 3.3550
Epoch [9/20], Step[310/483], loss: 3.3471
Epoch [9/20], Step[320/483], loss: 3.4351
Epoch [9/20], Step[330/483], loss: 3.2408
Epoch [9/20], Step[340/483], loss: 3.5769
Epoch [9/20], Step[350/483], loss: 3.2258
Epoch [9/20], Step[360/483], loss: 2.6283
Epoch [9/20], Step[370/483], loss: 2.2782
Epoch [9/20], Step[380/483], loss: 3.0423
Epoch [9/20], Step[390/483], loss: 3.2677
Epoch [9/20], Step[400/483], loss: 3.2984
Epoch [9/20], Step[410/483], loss: 2.6663
Epoch [9/20], Step[420/483], loss: 2.9420
Epoch [9/20], Step[430/483], loss: 2.8702
Epoch [9/20], Step[440/483], loss: 3.3544
Epoch [9/20], Step[450/483], loss: 2.5530
Epoch [9/20], Step[460/483], loss: 3.2471
Epoch [9/20], Step[470/483], loss: 3.5059
Epoch [9/20], Step[480/483], loss: 2.2960
====> Epoch 9: Training loss: 1524.6108
Epoch [10/20], Step[0/483], loss: 2.9452
Epoch [10/20], Step[10/483], loss: 3.0392
Epoch [10/20], Step[20/483], loss: 3.2536
Epoch [10/20], Step[30/483], loss: 3.1569
Epoch [10/20], Step[40/483], loss: 3.1372
Epoch [10/20], Step[50/483], loss: 3.4785
Epoch [10/20], Step[60/483], loss: 3.3741
Epoch [10/20], Step[70/483], loss: 3.3335
Epoch [10/20], Step[80/483], loss: 3.6409
Epoch [10/20], Step[90/483], loss: 2.8458
Epoch [10/20], Step[100/483], loss: 2.7256
Epoch [10/20], Step[110/483], loss: 3.0691
Epoch [10/20], Step[120/483], loss: 3.4299
Epoch [10/20], Step[130/483], loss: 3.2333
Epoch [10/20], Step[140/483], loss: 3.3780
Epoch [10/20], Step[150/483], loss: 3.4354
Epoch [10/20], Step[160/483], loss: 3.4532
Epoch [10/20], Step[170/483], loss: 2.5595
Epoch [10/20], Step[180/483], loss: 3.3676
Epoch [10/20], Step[190/483], loss: 3.4107
Epoch [10/20], Step[200/483], loss: 2.5366
Epoch [10/20], Step[210/483], loss: 3.0387
Epoch [10/20], Step[220/483], loss: 2.9482
Epoch [10/20], Step[230/483], loss: 2.6916
Epoch [10/20], Step[240/483], loss: 3.1852
Epoch [10/20], Step[250/483], loss: 3.4467
Epoch [10/20], Step[260/483], loss: 2.3618
Epoch [10/20], Step[270/483], loss: 3.2955
Epoch [10/20], Step[280/483], loss: 2.8456
Epoch [10/20], Step[290/483], loss: 2.4881
Epoch [10/20], Step[300/483], loss: 3.3487
Epoch [10/20], Step[310/483], loss: 3.3362
Epoch [10/20], Step[320/483], loss: 3.4185
Epoch [10/20], Step[330/483], loss: 3.2456
Epoch [10/20], Step[340/483], loss: 3.5553
Epoch [10/20], Step[350/483], loss: 3.2032
Epoch [10/20], Step[360/483], loss: 2.6344
Epoch [10/20], Step[370/483], loss: 2.2971
Epoch [10/20], Step[380/483], loss: 3.0618
Epoch [10/20], Step[390/483], loss: 3.2723
Epoch [10/20], Step[400/483], loss: 3.2954
Epoch [10/20], Step[410/483], loss: 2.6764
Epoch [10/20], Step[420/483], loss: 2.9190
Epoch [10/20], Step[430/483], loss: 2.8801
Epoch [10/20], Step[440/483], loss: 3.3722
Epoch [10/20], Step[450/483], loss: 2.5704
Epoch [10/20], Step[460/483], loss: 3.2532
Epoch [10/20], Step[470/483], loss: 3.5180
Epoch [10/20], Step[480/483], loss: 2.3087
====> Epoch 10: Training loss: 1525.3540
Epoch [11/20], Step[0/483], loss: 2.9448
Epoch [11/20], Step[10/483], loss: 3.0641
Epoch [11/20], Step[20/483], loss: 3.2806
Epoch [11/20], Step[30/483], loss: 3.1487
Epoch [11/20], Step[40/483], loss: 3.1520
Epoch [11/20], Step[50/483], loss: 3.4830
Epoch [11/20], Step[60/483], loss: 3.3748
Epoch [11/20], Step[70/483], loss: 3.3361
Epoch [11/20], Step[80/483], loss: 3.6363
Epoch [11/20], Step[90/483], loss: 2.8547
Epoch [11/20], Step[100/483], loss: 2.7233
Epoch [11/20], Step[110/483], loss: 3.0540
Epoch [11/20], Step[120/483], loss: 3.4310
Epoch [11/20], Step[130/483], loss: 3.2393
Epoch [11/20], Step[140/483], loss: 3.3821
Epoch [11/20], Step[150/483], loss: 3.4426
Epoch [11/20], Step[160/483], loss: 3.4623
Epoch [11/20], Step[170/483], loss: 2.5682
Epoch [11/20], Step[180/483], loss: 3.3779
Epoch [11/20], Step[190/483], loss: 3.4071
Epoch [11/20], Step[200/483], loss: 2.5464
Epoch [11/20], Step[210/483], loss: 3.0436
Epoch [11/20], Step[220/483], loss: 2.9593
Epoch [11/20], Step[230/483], loss: 2.6621
Epoch [11/20], Step[240/483], loss: 3.1868
Epoch [11/20], Step[250/483], loss: 3.4419
Epoch [11/20], Step[260/483], loss: 2.3689
Epoch [11/20], Step[270/483], loss: 3.3024
Epoch [11/20], Step[280/483], loss: 2.8436
Epoch [11/20], Step[290/483], loss: 2.4944
Epoch [11/20], Step[300/483], loss: 3.3405
Epoch [11/20], Step[310/483], loss: 3.3391
Epoch [11/20], Step[320/483], loss: 3.4210
Epoch [11/20], Step[330/483], loss: 3.2526
Epoch [11/20], Step[340/483], loss: 3.5515
Epoch [11/20], Step[350/483], loss: 3.2117
Epoch [11/20], Step[360/483], loss: 2.6328
Epoch [11/20], Step[370/483], loss: 2.2585
Epoch [11/20], Step[380/483], loss: 3.0506
Epoch [11/20], Step[390/483], loss: 3.2685
Epoch [11/20], Step[400/483], loss: 3.3010
Epoch [11/20], Step[410/483], loss: 2.6837
Epoch [11/20], Step[420/483], loss: 2.9505
Epoch [11/20], Step[430/483], loss: 2.8727
Epoch [11/20], Step[440/483], loss: 3.3664
Epoch [11/20], Step[450/483], loss: 2.5429
Epoch [11/20], Step[460/483], loss: 3.2489
Epoch [11/20], Step[470/483], loss: 3.5156
Epoch [11/20], Step[480/483], loss: 2.3027
====> Epoch 11: Training loss: 1524.8201
Epoch [12/20], Step[0/483], loss: 2.9576
Epoch [12/20], Step[10/483], loss: 3.0601
Epoch [12/20], Step[20/483], loss: 3.2619
Epoch [12/20], Step[30/483], loss: 3.1460
Epoch [12/20], Step[40/483], loss: 3.1567
Epoch [12/20], Step[50/483], loss: 3.4754
Epoch [12/20], Step[60/483], loss: 3.3933
Epoch [12/20], Step[70/483], loss: 3.3587
Epoch [12/20], Step[80/483], loss: 3.6272
Epoch [12/20], Step[90/483], loss: 2.8427
Epoch [12/20], Step[100/483], loss: 2.7137
Epoch [12/20], Step[110/483], loss: 3.0619
Epoch [12/20], Step[120/483], loss: 3.4570
Epoch [12/20], Step[130/483], loss: 3.2486
Epoch [12/20], Step[140/483], loss: 3.3781
Epoch [12/20], Step[150/483], loss: 3.4319
Epoch [12/20], Step[160/483], loss: 3.4469
Epoch [12/20], Step[170/483], loss: 2.5894
Epoch [12/20], Step[180/483], loss: 3.3607
Epoch [12/20], Step[190/483], loss: 3.3999
Epoch [12/20], Step[200/483], loss: 2.5429
Epoch [12/20], Step[210/483], loss: 3.0462
Epoch [12/20], Step[220/483], loss: 2.9398
Epoch [12/20], Step[230/483], loss: 2.6658
Epoch [12/20], Step[240/483], loss: 3.1771
Epoch [12/20], Step[250/483], loss: 3.4339
Epoch [12/20], Step[260/483], loss: 2.3715
Epoch [12/20], Step[270/483], loss: 3.2937
Epoch [12/20], Step[280/483], loss: 2.8457
Epoch [12/20], Step[290/483], loss: 2.4924
Epoch [12/20], Step[300/483], loss: 3.3358
Epoch [12/20], Step[310/483], loss: 3.3391
Epoch [12/20], Step[320/483], loss: 3.4180
Epoch [12/20], Step[330/483], loss: 3.2439
Epoch [12/20], Step[340/483], loss: 3.5406
Epoch [12/20], Step[350/483], loss: 3.2038
Epoch [12/20], Step[360/483], loss: 2.6297
Epoch [12/20], Step[370/483], loss: 2.2617
Epoch [12/20], Step[380/483], loss: 3.0503
Epoch [12/20], Step[390/483], loss: 3.2639
Epoch [12/20], Step[400/483], loss: 3.3127
Epoch [12/20], Step[410/483], loss: 2.6694
Epoch [12/20], Step[420/483], loss: 2.9603
Epoch [12/20], Step[430/483], loss: 2.8685
Epoch [12/20], Step[440/483], loss: 3.3586
Epoch [12/20], Step[450/483], loss: 2.5416
Epoch [12/20], Step[460/483], loss: 3.2426
Epoch [12/20], Step[470/483], loss: 3.5132
Epoch [12/20], Step[480/483], loss: 2.2906
====> Epoch 12: Training loss: 1524.5525
Epoch [13/20], Step[0/483], loss: 2.9484
Epoch [13/20], Step[10/483], loss: 3.0642
Epoch [13/20], Step[20/483], loss: 3.2614
Epoch [13/20], Step[30/483], loss: 3.1507
Epoch [13/20], Step[40/483], loss: 3.1506
Epoch [13/20], Step[50/483], loss: 3.4798
Epoch [13/20], Step[60/483], loss: 3.3869
Epoch [13/20], Step[70/483], loss: 3.3261
Epoch [13/20], Step[80/483], loss: 3.6294
Epoch [13/20], Step[90/483], loss: 2.8424
Epoch [13/20], Step[100/483], loss: 2.7350
Epoch [13/20], Step[110/483], loss: 3.0471
Epoch [13/20], Step[120/483], loss: 3.4538
Epoch [13/20], Step[130/483], loss: 3.2309
Epoch [13/20], Step[140/483], loss: 3.3708
Epoch [13/20], Step[150/483], loss: 3.4314
Epoch [13/20], Step[160/483], loss: 3.4412
Epoch [13/20], Step[170/483], loss: 2.5645
Epoch [13/20], Step[180/483], loss: 3.3676
Epoch [13/20], Step[190/483], loss: 3.4027
Epoch [13/20], Step[200/483], loss: 2.5453
Epoch [13/20], Step[210/483], loss: 3.0237
Epoch [13/20], Step[220/483], loss: 2.9494
Epoch [13/20], Step[230/483], loss: 2.6837
Epoch [13/20], Step[240/483], loss: 3.1741
Epoch [13/20], Step[250/483], loss: 3.4343
Epoch [13/20], Step[260/483], loss: 2.3637
Epoch [13/20], Step[270/483], loss: 3.2926
Epoch [13/20], Step[280/483], loss: 2.8533
Epoch [13/20], Step[290/483], loss: 2.4900
Epoch [13/20], Step[300/483], loss: 3.3295
Epoch [13/20], Step[310/483], loss: 3.3392
Epoch [13/20], Step[320/483], loss: 3.4140
Epoch [13/20], Step[330/483], loss: 3.2472
Epoch [13/20], Step[340/483], loss: 3.5495
Epoch [13/20], Step[350/483], loss: 3.2044
Epoch [13/20], Step[360/483], loss: 2.6266
Epoch [13/20], Step[370/483], loss: 2.2528
Epoch [13/20], Step[380/483], loss: 3.0449
Epoch [13/20], Step[390/483], loss: 3.2650
Epoch [13/20], Step[400/483], loss: 3.2996
Epoch [13/20], Step[410/483], loss: 2.6724
Epoch [13/20], Step[420/483], loss: 2.9391
Epoch [13/20], Step[430/483], loss: 2.8684
Epoch [13/20], Step[440/483], loss: 3.3579
Epoch [13/20], Step[450/483], loss: 2.5402
Epoch [13/20], Step[460/483], loss: 3.2447
Epoch [13/20], Step[470/483], loss: 3.5203
Epoch [13/20], Step[480/483], loss: 2.3031
====> Epoch 13: Training loss: 1523.2458
Epoch [14/20], Step[0/483], loss: 2.9444
Epoch [14/20], Step[10/483], loss: 3.0621
Epoch [14/20], Step[20/483], loss: 3.2603
Epoch [14/20], Step[30/483], loss: 3.1519
Epoch [14/20], Step[40/483], loss: 3.1472
Epoch [14/20], Step[50/483], loss: 3.4801
Epoch [14/20], Step[60/483], loss: 3.3846
Epoch [14/20], Step[70/483], loss: 3.3509
Epoch [14/20], Step[80/483], loss: 3.6364
Epoch [14/20], Step[90/483], loss: 2.8483
Epoch [14/20], Step[100/483], loss: 2.7242
Epoch [14/20], Step[110/483], loss: 3.0488
Epoch [14/20], Step[120/483], loss: 3.4269
Epoch [14/20], Step[130/483], loss: 3.2302
Epoch [14/20], Step[140/483], loss: 3.3669
Epoch [14/20], Step[150/483], loss: 3.4370
Epoch [14/20], Step[160/483], loss: 3.4389
Epoch [14/20], Step[170/483], loss: 2.5619
Epoch [14/20], Step[180/483], loss: 3.3641
Epoch [14/20], Step[190/483], loss: 3.4114
Epoch [14/20], Step[200/483], loss: 2.5518
Epoch [14/20], Step[210/483], loss: 3.0227
Epoch [14/20], Step[220/483], loss: 2.9511
Epoch [14/20], Step[230/483], loss: 2.6797
Epoch [14/20], Step[240/483], loss: 3.1812
Epoch [14/20], Step[250/483], loss: 3.4326
Epoch [14/20], Step[260/483], loss: 2.3637
Epoch [14/20], Step[270/483], loss: 3.2983
Epoch [14/20], Step[280/483], loss: 2.8546
Epoch [14/20], Step[290/483], loss: 2.4848
Epoch [14/20], Step[300/483], loss: 3.3242
Epoch [14/20], Step[310/483], loss: 3.3412
Epoch [14/20], Step[320/483], loss: 3.4115
Epoch [14/20], Step[330/483], loss: 3.2460
Epoch [14/20], Step[340/483], loss: 3.5426
Epoch [14/20], Step[350/483], loss: 3.1941
Epoch [14/20], Step[360/483], loss: 2.6300
Epoch [14/20], Step[370/483], loss: 2.2614
Epoch [14/20], Step[380/483], loss: 3.0483
Epoch [14/20], Step[390/483], loss: 3.2646
Epoch [14/20], Step[400/483], loss: 3.2943
Epoch [14/20], Step[410/483], loss: 2.6752
Epoch [14/20], Step[420/483], loss: 2.9348
Epoch [14/20], Step[430/483], loss: 2.8559
Epoch [14/20], Step[440/483], loss: 3.3607
Epoch [14/20], Step[450/483], loss: 2.5403
Epoch [14/20], Step[460/483], loss: 3.2405
Epoch [14/20], Step[470/483], loss: 3.5213
Epoch [14/20], Step[480/483], loss: 2.2973
====> Epoch 14: Training loss: 1523.0341
Epoch [15/20], Step[0/483], loss: 2.9652
Epoch [15/20], Step[10/483], loss: 3.0646
Epoch [15/20], Step[20/483], loss: 3.2734
Epoch [15/20], Step[30/483], loss: 3.1507
Epoch [15/20], Step[40/483], loss: 3.1466
Epoch [15/20], Step[50/483], loss: 3.4818
Epoch [15/20], Step[60/483], loss: 3.3800
Epoch [15/20], Step[70/483], loss: 3.3253
Epoch [15/20], Step[80/483], loss: 3.6463
Epoch [15/20], Step[90/483], loss: 2.8505
Epoch [15/20], Step[100/483], loss: 2.7230
Epoch [15/20], Step[110/483], loss: 3.0483
Epoch [15/20], Step[120/483], loss: 3.4189
Epoch [15/20], Step[130/483], loss: 3.2362
Epoch [15/20], Step[140/483], loss: 3.3759
Epoch [15/20], Step[150/483], loss: 3.4351
Epoch [15/20], Step[160/483], loss: 3.4308
Epoch [15/20], Step[170/483], loss: 2.5653
Epoch [15/20], Step[180/483], loss: 3.3715
Epoch [15/20], Step[190/483], loss: 3.3985
Epoch [15/20], Step[200/483], loss: 2.5395
Epoch [15/20], Step[210/483], loss: 3.0220
Epoch [15/20], Step[220/483], loss: 2.9419
Epoch [15/20], Step[230/483], loss: 2.6749
Epoch [15/20], Step[240/483], loss: 3.1778
Epoch [15/20], Step[250/483], loss: 3.4329
Epoch [15/20], Step[260/483], loss: 2.3619
Epoch [15/20], Step[270/483], loss: 3.2913
Epoch [15/20], Step[280/483], loss: 2.8499
Epoch [15/20], Step[290/483], loss: 2.4945
Epoch [15/20], Step[300/483], loss: 3.3016
Epoch [15/20], Step[310/483], loss: 3.3462
Epoch [15/20], Step[320/483], loss: 3.4141
Epoch [15/20], Step[330/483], loss: 3.2333
Epoch [15/20], Step[340/483], loss: 3.5357
Epoch [15/20], Step[350/483], loss: 3.1990
Epoch [15/20], Step[360/483], loss: 2.6294
Epoch [15/20], Step[370/483], loss: 2.2596
Epoch [15/20], Step[380/483], loss: 3.0545
Epoch [15/20], Step[390/483], loss: 3.2665
Epoch [15/20], Step[400/483], loss: 3.2903
Epoch [15/20], Step[410/483], loss: 2.6703
Epoch [15/20], Step[420/483], loss: 2.9311
Epoch [15/20], Step[430/483], loss: 2.8647
Epoch [15/20], Step[440/483], loss: 3.3511
Epoch [15/20], Step[450/483], loss: 2.5409
Epoch [15/20], Step[460/483], loss: 3.2442
Epoch [15/20], Step[470/483], loss: 3.5157
Epoch [15/20], Step[480/483], loss: 2.2971
====> Epoch 15: Training loss: 1522.4267
Epoch [16/20], Step[0/483], loss: 2.9521
Epoch [16/20], Step[10/483], loss: 3.0630
Epoch [16/20], Step[20/483], loss: 3.2625
Epoch [16/20], Step[30/483], loss: 3.1533
Epoch [16/20], Step[40/483], loss: 3.1394
Epoch [16/20], Step[50/483], loss: 3.4792
Epoch [16/20], Step[60/483], loss: 3.3811
Epoch [16/20], Step[70/483], loss: 3.3331
Epoch [16/20], Step[80/483], loss: 3.6484
Epoch [16/20], Step[90/483], loss: 2.8417
Epoch [16/20], Step[100/483], loss: 2.7233
Epoch [16/20], Step[110/483], loss: 3.0554
Epoch [16/20], Step[120/483], loss: 3.4272
Epoch [16/20], Step[130/483], loss: 3.2481
Epoch [16/20], Step[140/483], loss: 3.3709
Epoch [16/20], Step[150/483], loss: 3.4459
Epoch [16/20], Step[160/483], loss: 3.4421
Epoch [16/20], Step[170/483], loss: 2.5685
Epoch [16/20], Step[180/483], loss: 3.3713
Epoch [16/20], Step[190/483], loss: 3.4051
Epoch [16/20], Step[200/483], loss: 2.5456
Epoch [16/20], Step[210/483], loss: 3.0287
Epoch [16/20], Step[220/483], loss: 2.9429
Epoch [16/20], Step[230/483], loss: 2.6746
Epoch [16/20], Step[240/483], loss: 3.1701
Epoch [16/20], Step[250/483], loss: 3.4346
Epoch [16/20], Step[260/483], loss: 2.3699
Epoch [16/20], Step[270/483], loss: 3.2943
Epoch [16/20], Step[280/483], loss: 2.8639
Epoch [16/20], Step[290/483], loss: 2.4907
Epoch [16/20], Step[300/483], loss: 3.3510
Epoch [16/20], Step[310/483], loss: 3.3410
Epoch [16/20], Step[320/483], loss: 3.4162
Epoch [16/20], Step[330/483], loss: 3.2422
Epoch [16/20], Step[340/483], loss: 3.5523
Epoch [16/20], Step[350/483], loss: 3.1890
Epoch [16/20], Step[360/483], loss: 2.6314
Epoch [16/20], Step[370/483], loss: 2.2559
Epoch [16/20], Step[380/483], loss: 3.0535
Epoch [16/20], Step[390/483], loss: 3.2724
Epoch [16/20], Step[400/483], loss: 3.2885
Epoch [16/20], Step[410/483], loss: 2.6699
Epoch [16/20], Step[420/483], loss: 2.9327
Epoch [16/20], Step[430/483], loss: 2.8596
Epoch [16/20], Step[440/483], loss: 3.3497
Epoch [16/20], Step[450/483], loss: 2.5396
Epoch [16/20], Step[460/483], loss: 3.2375
Epoch [16/20], Step[470/483], loss: 3.5217
Epoch [16/20], Step[480/483], loss: 2.2950
====> Epoch 16: Training loss: 1522.9125
Epoch [17/20], Step[0/483], loss: 2.9431
Epoch [17/20], Step[10/483], loss: 3.0555
Epoch [17/20], Step[20/483], loss: 3.2592
Epoch [17/20], Step[30/483], loss: 3.1477
Epoch [17/20], Step[40/483], loss: 3.1468
Epoch [17/20], Step[50/483], loss: 3.4777
Epoch [17/20], Step[60/483], loss: 3.3804
Epoch [17/20], Step[70/483], loss: 3.3265
Epoch [17/20], Step[80/483], loss: 3.6414
Epoch [17/20], Step[90/483], loss: 2.8363
Epoch [17/20], Step[100/483], loss: 2.7182
Epoch [17/20], Step[110/483], loss: 3.0553
Epoch [17/20], Step[120/483], loss: 3.4263
Epoch [17/20], Step[130/483], loss: 3.2300
Epoch [17/20], Step[140/483], loss: 3.3702
Epoch [17/20], Step[150/483], loss: 3.4379
Epoch [17/20], Step[160/483], loss: 3.4350
Epoch [17/20], Step[170/483], loss: 2.5631
Epoch [17/20], Step[180/483], loss: 3.3701
Epoch [17/20], Step[190/483], loss: 3.4012
Epoch [17/20], Step[200/483], loss: 2.5421
Epoch [17/20], Step[210/483], loss: 3.0125
Epoch [17/20], Step[220/483], loss: 2.9401
Epoch [17/20], Step[230/483], loss: 2.6705
Epoch [17/20], Step[240/483], loss: 3.1708
Epoch [17/20], Step[250/483], loss: 3.4287
Epoch [17/20], Step[260/483], loss: 2.3623
Epoch [17/20], Step[270/483], loss: 3.2933
Epoch [17/20], Step[280/483], loss: 2.8499
Epoch [17/20], Step[290/483], loss: 2.4864
Epoch [17/20], Step[300/483], loss: 3.3554
Epoch [17/20], Step[310/483], loss: 3.3416
Epoch [17/20], Step[320/483], loss: 3.4134
Epoch [17/20], Step[330/483], loss: 3.2429
Epoch [17/20], Step[340/483], loss: 3.5587
Epoch [17/20], Step[350/483], loss: 3.1884
Epoch [17/20], Step[360/483], loss: 2.6275
Epoch [17/20], Step[370/483], loss: 2.2653
Epoch [17/20], Step[380/483], loss: 3.0512
Epoch [17/20], Step[390/483], loss: 3.2696
Epoch [17/20], Step[400/483], loss: 3.2860
Epoch [17/20], Step[410/483], loss: 2.6729
Epoch [17/20], Step[420/483], loss: 2.9377
Epoch [17/20], Step[430/483], loss: 2.8630
Epoch [17/20], Step[440/483], loss: 3.3456
Epoch [17/20], Step[450/483], loss: 2.5376
Epoch [17/20], Step[460/483], loss: 3.2487
Epoch [17/20], Step[470/483], loss: 3.5211
Epoch [17/20], Step[480/483], loss: 2.3092
====> Epoch 17: Training loss: 1522.7208
Epoch [18/20], Step[0/483], loss: 2.9579
Epoch [18/20], Step[10/483], loss: 3.0609
Epoch [18/20], Step[20/483], loss: 3.2637
Epoch [18/20], Step[30/483], loss: 3.1493
Epoch [18/20], Step[40/483], loss: 3.1392
Epoch [18/20], Step[50/483], loss: 3.4772
Epoch [18/20], Step[60/483], loss: 3.3765
Epoch [18/20], Step[70/483], loss: 3.3308
Epoch [18/20], Step[80/483], loss: 3.6459
Epoch [18/20], Step[90/483], loss: 2.8460
Epoch [18/20], Step[100/483], loss: 2.7136
Epoch [18/20], Step[110/483], loss: 3.0514
Epoch [18/20], Step[120/483], loss: 3.4328
Epoch [18/20], Step[130/483], loss: 3.2309
Epoch [18/20], Step[140/483], loss: 3.3728
Epoch [18/20], Step[150/483], loss: 3.4364
Epoch [18/20], Step[160/483], loss: 3.4331
Epoch [18/20], Step[170/483], loss: 2.5599
Epoch [18/20], Step[180/483], loss: 3.3715
Epoch [18/20], Step[190/483], loss: 3.4017
Epoch [18/20], Step[200/483], loss: 2.5383
Epoch [18/20], Step[210/483], loss: 3.0192
Epoch [18/20], Step[220/483], loss: 2.9423
Epoch [18/20], Step[230/483], loss: 2.6709
Epoch [18/20], Step[240/483], loss: 3.1837
Epoch [18/20], Step[250/483], loss: 3.4320
Epoch [18/20], Step[260/483], loss: 2.3658
Epoch [18/20], Step[270/483], loss: 3.2912
Epoch [18/20], Step[280/483], loss: 2.8444
Epoch [18/20], Step[290/483], loss: 2.4853
Epoch [18/20], Step[300/483], loss: 3.3572
Epoch [18/20], Step[310/483], loss: 3.3420
Epoch [18/20], Step[320/483], loss: 3.4143
Epoch [18/20], Step[330/483], loss: 3.2449
Epoch [18/20], Step[340/483], loss: 3.5193
Epoch [18/20], Step[350/483], loss: 3.1839
Epoch [18/20], Step[360/483], loss: 2.6282
Epoch [18/20], Step[370/483], loss: 2.2559
Epoch [18/20], Step[380/483], loss: 3.0499
Epoch [18/20], Step[390/483], loss: 3.2721
Epoch [18/20], Step[400/483], loss: 3.2895
Epoch [18/20], Step[410/483], loss: 2.6708
Epoch [18/20], Step[420/483], loss: 2.9279
Epoch [18/20], Step[430/483], loss: 2.8577
Epoch [18/20], Step[440/483], loss: 3.3493
Epoch [18/20], Step[450/483], loss: 2.5428
Epoch [18/20], Step[460/483], loss: 3.2413
Epoch [18/20], Step[470/483], loss: 3.5174
Epoch [18/20], Step[480/483], loss: 2.2958
====> Epoch 18: Training loss: 1522.3072
Epoch [19/20], Step[0/483], loss: 2.9495
Epoch [19/20], Step[10/483], loss: 3.0676
Epoch [19/20], Step[20/483], loss: 3.2663
Epoch [19/20], Step[30/483], loss: 3.1440
Epoch [19/20], Step[40/483], loss: 3.1555
Epoch [19/20], Step[50/483], loss: 3.4764
Epoch [19/20], Step[60/483], loss: 3.3796
Epoch [19/20], Step[70/483], loss: 3.3283
Epoch [19/20], Step[80/483], loss: 3.6421
Epoch [19/20], Step[90/483], loss: 2.8438
Epoch [19/20], Step[100/483], loss: 2.7116
Epoch [19/20], Step[110/483], loss: 3.0511
Epoch [19/20], Step[120/483], loss: 3.4300
Epoch [19/20], Step[130/483], loss: 3.2339
Epoch [19/20], Step[140/483], loss: 3.3768
Epoch [19/20], Step[150/483], loss: 3.4319
Epoch [19/20], Step[160/483], loss: 3.4400
Epoch [19/20], Step[170/483], loss: 2.5640
Epoch [19/20], Step[180/483], loss: 3.3665
Epoch [19/20], Step[190/483], loss: 3.4012
Epoch [19/20], Step[200/483], loss: 2.5401
Epoch [19/20], Step[210/483], loss: 3.0180
Epoch [19/20], Step[220/483], loss: 2.9460
Epoch [19/20], Step[230/483], loss: 2.6756
Epoch [19/20], Step[240/483], loss: 3.1797
Epoch [19/20], Step[250/483], loss: 3.4336
Epoch [19/20], Step[260/483], loss: 2.3692
Epoch [19/20], Step[270/483], loss: 3.2951
Epoch [19/20], Step[280/483], loss: 2.8433
Epoch [19/20], Step[290/483], loss: 2.4904
Epoch [19/20], Step[300/483], loss: 3.3550
Epoch [19/20], Step[310/483], loss: 3.3338
Epoch [19/20], Step[320/483], loss: 3.4083
Epoch [19/20], Step[330/483], loss: 3.2508
Epoch [19/20], Step[340/483], loss: 3.5346
Epoch [19/20], Step[350/483], loss: 3.2120
Epoch [19/20], Step[360/483], loss: 2.6331
Epoch [19/20], Step[370/483], loss: 2.2520
Epoch [19/20], Step[380/483], loss: 3.0500
Epoch [19/20], Step[390/483], loss: 3.2706
Epoch [19/20], Step[400/483], loss: 3.2779
Epoch [19/20], Step[410/483], loss: 2.6702
Epoch [19/20], Step[420/483], loss: 2.9208
Epoch [19/20], Step[430/483], loss: 2.8642
Epoch [19/20], Step[440/483], loss: 3.3484
Epoch [19/20], Step[450/483], loss: 2.5416
Epoch [19/20], Step[460/483], loss: 3.2336
Epoch [19/20], Step[470/483], loss: 3.5178
Epoch [19/20], Step[480/483], loss: 2.2995
====> Epoch 19: Training loss: 1522.3446
Epoch [20/20], Step[0/483], loss: 2.9420
Epoch [20/20], Step[10/483], loss: 3.0555
Epoch [20/20], Step[20/483], loss: 3.2693
Epoch [20/20], Step[30/483], loss: 3.1479
Epoch [20/20], Step[40/483], loss: 3.1392
Epoch [20/20], Step[50/483], loss: 3.4768
Epoch [20/20], Step[60/483], loss: 3.3773
Epoch [20/20], Step[70/483], loss: 3.3323
Epoch [20/20], Step[80/483], loss: 3.6413
Epoch [20/20], Step[90/483], loss: 2.8414
Epoch [20/20], Step[100/483], loss: 2.7072
Epoch [20/20], Step[110/483], loss: 3.0498
Epoch [20/20], Step[120/483], loss: 3.4261
Epoch [20/20], Step[130/483], loss: 3.2325
Epoch [20/20], Step[140/483], loss: 3.3737
Epoch [20/20], Step[150/483], loss: 3.4377
Epoch [20/20], Step[160/483], loss: 3.4358
Epoch [20/20], Step[170/483], loss: 2.5618
Epoch [20/20], Step[180/483], loss: 3.3693
Epoch [20/20], Step[190/483], loss: 3.4007
Epoch [20/20], Step[200/483], loss: 2.5428
Epoch [20/20], Step[210/483], loss: 3.0202
Epoch [20/20], Step[220/483], loss: 2.9412
Epoch [20/20], Step[230/483], loss: 2.6707
Epoch [20/20], Step[240/483], loss: 3.1831
Epoch [20/20], Step[250/483], loss: 3.4304
Epoch [20/20], Step[260/483], loss: 2.3683
Epoch [20/20], Step[270/483], loss: 3.2950
Epoch [20/20], Step[280/483], loss: 2.8469
Epoch [20/20], Step[290/483], loss: 2.4882
Epoch [20/20], Step[300/483], loss: 3.3575
Epoch [20/20], Step[310/483], loss: 3.3382
Epoch [20/20], Step[320/483], loss: 3.4076
Epoch [20/20], Step[330/483], loss: 3.2467
Epoch [20/20], Step[340/483], loss: 3.5179
Epoch [20/20], Step[350/483], loss: 3.1973
Epoch [20/20], Step[360/483], loss: 2.6254
Epoch [20/20], Step[370/483], loss: 2.2594
Epoch [20/20], Step[380/483], loss: 3.0478
Epoch [20/20], Step[390/483], loss: 3.2714
Epoch [20/20], Step[400/483], loss: 3.2839
Epoch [20/20], Step[410/483], loss: 2.6713
Epoch [20/20], Step[420/483], loss: 2.9247
Epoch [20/20], Step[430/483], loss: 2.8662
Epoch [20/20], Step[440/483], loss: 3.3527
Epoch [20/20], Step[450/483], loss: 2.5422
Epoch [20/20], Step[460/483], loss: 3.2377
Epoch [20/20], Step[470/483], loss: 3.5199
Epoch [20/20], Step[480/483], loss: 2.3023
====> Epoch 20: Training loss: 1522.2524
