 Loading feature files...
 <s> = 1
 </s> = 2
 All scenes loaded.
 Hyperparameters:Namespace(LR=0.01, alternatives=1, batch_size=100, dec='LSTM', dropout=0.0, epochs=10, hidden_sz=50, log_interval=10, model='ss1', no_cuda=False, seed=1)
 Listener0: Listener0Model(
  (scene_encoder): LinearSceneEncoder(
    (fc): Linear(in_features=280, out_features=50)
  )
  (string_encoder): LinearStringEncoder(
    (fc): Linear(in_features=1063, out_features=50)
  )
  (scorer): MLPScorer(
    (linear_4): Linear(in_features=50, out_features=50)
    (linear_5): Linear(in_features=50, out_features=50)
    (linear_3): Linear(in_features=50, out_features=1)
  )
)
 Speaker0: Speaker0Model(
  (scene_encoder): LinearSceneEncoder(
    (fc): Linear(in_features=280, out_features=50)
  )
  (string_decoder): LSTMStringDecoder(
    (embedding): Embedding(1063, 50)
    (lstm): LSTM(50, 50, num_layers=2, batch_first=True)
    (linear): Linear(in_features=50, out_features=1063)
    (dropout): Dropout(p=0.0)
  )
)
 Training Listener0...
 Epoch [1/10], Step[0/483], loss: 0.6941
 Epoch [1/10], Step[10/483], loss: 0.6776
 Epoch [1/10], Step[20/483], loss: 0.6383
 Epoch [1/10], Step[30/483], loss: 0.7078
 Epoch [1/10], Step[40/483], loss: 0.6795
 Epoch [1/10], Step[50/483], loss: 0.6994
 Epoch [1/10], Step[60/483], loss: 0.6647
 Epoch [1/10], Step[70/483], loss: 0.6897
 Epoch [1/10], Step[80/483], loss: 0.6980
 Epoch [1/10], Step[90/483], loss: 0.6917
 Epoch [1/10], Step[100/483], loss: 0.6929
 Epoch [1/10], Step[110/483], loss: 0.6921
 Epoch [1/10], Step[120/483], loss: 0.6702
 Epoch [1/10], Step[130/483], loss: 0.6821
 Epoch [1/10], Step[140/483], loss: 0.6874
 Epoch [1/10], Step[150/483], loss: 0.7863
 Epoch [1/10], Step[160/483], loss: 0.6898
 Epoch [1/10], Step[170/483], loss: 0.7013
 Epoch [1/10], Step[180/483], loss: 0.6667
 Epoch [1/10], Step[190/483], loss: 0.7019
 Epoch [1/10], Step[200/483], loss: 0.6871
 Epoch [1/10], Step[210/483], loss: 0.7180
 Epoch [1/10], Step[220/483], loss: 0.6974
 Epoch [1/10], Step[230/483], loss: 0.7056
 Epoch [1/10], Step[240/483], loss: 0.6865
 Epoch [1/10], Step[250/483], loss: 0.6763
 Epoch [1/10], Step[260/483], loss: 0.5423
 Epoch [1/10], Step[270/483], loss: 0.5702
 Epoch [1/10], Step[280/483], loss: 0.6928
 Epoch [1/10], Step[290/483], loss: 0.7801
 Epoch [1/10], Step[300/483], loss: 0.7385
 Epoch [1/10], Step[310/483], loss: 0.8253
 Epoch [1/10], Step[320/483], loss: 0.7007
 Epoch [1/10], Step[330/483], loss: 0.7047
 Epoch [1/10], Step[340/483], loss: 0.6079
 Epoch [1/10], Step[350/483], loss: 0.7573
 Epoch [1/10], Step[360/483], loss: 0.7182
 Epoch [1/10], Step[370/483], loss: 0.6569
 Epoch [1/10], Step[380/483], loss: 0.6362
 Epoch [1/10], Step[390/483], loss: 0.7600
 Epoch [1/10], Step[400/483], loss: 0.7472
 Epoch [1/10], Step[410/483], loss: 0.7240
 Epoch [1/10], Step[420/483], loss: 0.7428
 Epoch [1/10], Step[430/483], loss: 0.6761
 Epoch [1/10], Step[440/483], loss: 0.6808
 Epoch [1/10], Step[450/483], loss: 0.8331
 Epoch [1/10], Step[460/483], loss: 0.7496
 Epoch [1/10], Step[470/483], loss: 0.6720
 Epoch [1/10], Step[480/483], loss: 0.7281
 ====> Epoch 1: Training loss: 339.2163
 Epoch [2/10], Step[0/483], loss: 0.6973
 Epoch [2/10], Step[10/483], loss: 0.7070
 Epoch [2/10], Step[20/483], loss: 0.6056
 Epoch [2/10], Step[30/483], loss: 0.5827
 Epoch [2/10], Step[40/483], loss: 0.7069
 Epoch [2/10], Step[50/483], loss: 0.6363
 Epoch [2/10], Step[60/483], loss: 0.6934
 Epoch [2/10], Step[70/483], loss: 0.6220
 Epoch [2/10], Step[80/483], loss: 0.6389
 Epoch [2/10], Step[90/483], loss: 0.6248
 Epoch [2/10], Step[100/483], loss: 0.6440
 Epoch [2/10], Step[110/483], loss: 0.6432
 Epoch [2/10], Step[120/483], loss: 0.6800
 Epoch [2/10], Step[130/483], loss: 0.6822
 Epoch [2/10], Step[140/483], loss: 0.6143
 Epoch [2/10], Step[150/483], loss: 0.8634
 Epoch [2/10], Step[160/483], loss: 0.6273
 Epoch [2/10], Step[170/483], loss: 0.8850
 Epoch [2/10], Step[180/483], loss: 0.4784
 Epoch [2/10], Step[190/483], loss: 0.6494
 Epoch [2/10], Step[200/483], loss: 0.6809
 Epoch [2/10], Step[210/483], loss: 0.6705
 Epoch [2/10], Step[220/483], loss: 0.6921
 Epoch [2/10], Step[230/483], loss: 0.5465
 Epoch [2/10], Step[240/483], loss: 0.5674
 Epoch [2/10], Step[250/483], loss: 0.5097
 Epoch [2/10], Step[260/483], loss: 0.5229
 Epoch [2/10], Step[270/483], loss: 0.5507
 Epoch [2/10], Step[280/483], loss: 0.7689
 Epoch [2/10], Step[290/483], loss: 0.5398
 Epoch [2/10], Step[300/483], loss: 0.5137
 Epoch [2/10], Step[310/483], loss: 0.7170
 Epoch [2/10], Step[320/483], loss: 0.6221
 Epoch [2/10], Step[330/483], loss: 0.5035
 Epoch [2/10], Step[340/483], loss: 0.4963
 Epoch [2/10], Step[350/483], loss: 0.6481
 Epoch [2/10], Step[360/483], loss: 0.6835
 Epoch [2/10], Step[370/483], loss: 0.3529
 Epoch [2/10], Step[380/483], loss: 0.5455
 Epoch [2/10], Step[390/483], loss: 0.6668
 Epoch [2/10], Step[400/483], loss: 0.6437
 Epoch [2/10], Step[410/483], loss: 0.5810
 Epoch [2/10], Step[420/483], loss: 0.4524
 Epoch [2/10], Step[430/483], loss: 0.6509
 Epoch [2/10], Step[440/483], loss: 0.6281
 Epoch [2/10], Step[450/483], loss: 0.4551
 Epoch [2/10], Step[460/483], loss: 0.6102
 Epoch [2/10], Step[470/483], loss: 0.4445
 Epoch [2/10], Step[480/483], loss: 0.5417
 ====> Epoch 2: Training loss: 300.3767
 Epoch [3/10], Step[0/483], loss: 0.5671
 Epoch [3/10], Step[10/483], loss: 0.4697
 Epoch [3/10], Step[20/483], loss: 0.6166
 Epoch [3/10], Step[30/483], loss: 0.5227
 Epoch [3/10], Step[40/483], loss: 0.3509
 Epoch [3/10], Step[50/483], loss: 0.6589
 Epoch [3/10], Step[60/483], loss: 0.5378
 Epoch [3/10], Step[70/483], loss: 0.4132
 Epoch [3/10], Step[80/483], loss: 0.5588
 Epoch [3/10], Step[90/483], loss: 0.4342
 Epoch [3/10], Step[100/483], loss: 0.5412
 Epoch [3/10], Step[110/483], loss: 0.5800
 Epoch [3/10], Step[120/483], loss: 0.4892
 Epoch [3/10], Step[130/483], loss: 0.5104
 Epoch [3/10], Step[140/483], loss: 0.5922
 Epoch [3/10], Step[150/483], loss: 0.6558
 Epoch [3/10], Step[160/483], loss: 0.4572
 Epoch [3/10], Step[170/483], loss: 0.6148
 Epoch [3/10], Step[180/483], loss: 0.4428
 Epoch [3/10], Step[190/483], loss: 0.5328
 Epoch [3/10], Step[200/483], loss: 0.5929
 Epoch [3/10], Step[210/483], loss: 0.4702
 Epoch [3/10], Step[220/483], loss: 0.5983
 Epoch [3/10], Step[230/483], loss: 0.5242
 Epoch [3/10], Step[240/483], loss: 0.5001
 Epoch [3/10], Step[250/483], loss: 0.4906
 Epoch [3/10], Step[260/483], loss: 0.3494
 Epoch [3/10], Step[270/483], loss: 0.3720
 Epoch [3/10], Step[280/483], loss: 0.5053
 Epoch [3/10], Step[290/483], loss: 0.4645
 Epoch [3/10], Step[300/483], loss: 0.3730
 Epoch [3/10], Step[310/483], loss: 0.4970
 Epoch [3/10], Step[320/483], loss: 0.6124
 Epoch [3/10], Step[330/483], loss: 0.5221
 Epoch [3/10], Step[340/483], loss: 0.3330
 Epoch [3/10], Step[350/483], loss: 0.5916
 Epoch [3/10], Step[360/483], loss: 0.5854
 Epoch [3/10], Step[370/483], loss: 0.3187
 Epoch [3/10], Step[380/483], loss: 0.5149
 Epoch [3/10], Step[390/483], loss: 0.5279
 Epoch [3/10], Step[400/483], loss: 0.7485
 Epoch [3/10], Step[410/483], loss: 0.4264
 Epoch [3/10], Step[420/483], loss: 0.3859
 Epoch [3/10], Step[430/483], loss: 0.4675
 Epoch [3/10], Step[440/483], loss: 0.5628
 Epoch [3/10], Step[450/483], loss: 0.4725
 Epoch [3/10], Step[460/483], loss: 0.4670
 Epoch [3/10], Step[470/483], loss: 0.4094
 Epoch [3/10], Step[480/483], loss: 0.4203
 ====> Epoch 3: Training loss: 246.0767
 Epoch [4/10], Step[0/483], loss: 0.4513
 Epoch [4/10], Step[10/483], loss: 0.3808
 Epoch [4/10], Step[20/483], loss: 0.5708
 Epoch [4/10], Step[30/483], loss: 0.4912
 Epoch [4/10], Step[40/483], loss: 0.2880
 Epoch [4/10], Step[50/483], loss: 0.5437
 Epoch [4/10], Step[60/483], loss: 0.4234
 Epoch [4/10], Step[70/483], loss: 0.4314
 Epoch [4/10], Step[80/483], loss: 0.3846
 Epoch [4/10], Step[90/483], loss: 0.4335
 Epoch [4/10], Step[100/483], loss: 0.4881
 Epoch [4/10], Step[110/483], loss: 0.6997
 Epoch [4/10], Step[120/483], loss: 0.4898
 Epoch [4/10], Step[130/483], loss: 0.5006
 Epoch [4/10], Step[140/483], loss: 0.3075
 Epoch [4/10], Step[150/483], loss: 0.5074
 Epoch [4/10], Step[160/483], loss: 0.5007
 Epoch [4/10], Step[170/483], loss: 0.6161
 Epoch [4/10], Step[180/483], loss: 0.2460
 Epoch [4/10], Step[190/483], loss: 0.5435
 Epoch [4/10], Step[200/483], loss: 0.5147
 Epoch [4/10], Step[210/483], loss: 0.4050
 Epoch [4/10], Step[220/483], loss: 0.5312
 Epoch [4/10], Step[230/483], loss: 0.4874
 Epoch [4/10], Step[240/483], loss: 0.5084
 Epoch [4/10], Step[250/483], loss: 0.3727
 Epoch [4/10], Step[260/483], loss: 0.2136
 Epoch [4/10], Step[270/483], loss: 0.4044
 Epoch [4/10], Step[280/483], loss: 0.4913
 Epoch [4/10], Step[290/483], loss: 0.4392
 Epoch [4/10], Step[300/483], loss: 0.3744
 Epoch [4/10], Step[310/483], loss: 0.5745
 Epoch [4/10], Step[320/483], loss: 0.4992
 Epoch [4/10], Step[330/483], loss: 0.4593
 Epoch [4/10], Step[340/483], loss: 0.2680
 Epoch [4/10], Step[350/483], loss: 0.6676
 Epoch [4/10], Step[360/483], loss: 0.3821
 Epoch [4/10], Step[370/483], loss: 0.2750
 Epoch [4/10], Step[380/483], loss: 0.5410
 Epoch [4/10], Step[390/483], loss: 0.5325
 Epoch [4/10], Step[400/483], loss: 0.4375
 Epoch [4/10], Step[410/483], loss: 0.3747
 Epoch [4/10], Step[420/483], loss: 0.4493
 Epoch [4/10], Step[430/483], loss: 0.4358
 Epoch [4/10], Step[440/483], loss: 0.5094
 Epoch [4/10], Step[450/483], loss: 0.3015
 Epoch [4/10], Step[460/483], loss: 0.5141
 Epoch [4/10], Step[470/483], loss: 0.3212
 Epoch [4/10], Step[480/483], loss: 0.3166
 ====> Epoch 4: Training loss: 212.3518
 Epoch [5/10], Step[0/483], loss: 0.4353
 Epoch [5/10], Step[10/483], loss: 0.4056
 Epoch [5/10], Step[20/483], loss: 0.3792
 Epoch [5/10], Step[30/483], loss: 0.4500
 Epoch [5/10], Step[40/483], loss: 0.3786
 Epoch [5/10], Step[50/483], loss: 0.3557
 Epoch [5/10], Step[60/483], loss: 0.3681
 Epoch [5/10], Step[70/483], loss: 0.2827
 Epoch [5/10], Step[80/483], loss: 0.3372
 Epoch [5/10], Step[90/483], loss: 0.3910
 Epoch [5/10], Step[100/483], loss: 0.3497
 Epoch [5/10], Step[110/483], loss: 0.4684
 Epoch [5/10], Step[120/483], loss: 0.3440
 Epoch [5/10], Step[130/483], loss: 0.3404
 Epoch [5/10], Step[140/483], loss: 0.3567
 Epoch [5/10], Step[150/483], loss: 0.5276
 Epoch [5/10], Step[160/483], loss: 0.4903
 Epoch [5/10], Step[170/483], loss: 0.5574
 Epoch [5/10], Step[180/483], loss: 0.1519
 Epoch [5/10], Step[190/483], loss: 0.3888
 Epoch [5/10], Step[200/483], loss: 0.3293
 Epoch [5/10], Step[210/483], loss: 0.3116
 Epoch [5/10], Step[220/483], loss: 0.4842
 Epoch [5/10], Step[230/483], loss: 0.4490
 Epoch [5/10], Step[240/483], loss: 0.3080
 Epoch [5/10], Step[250/483], loss: 0.3895
 Epoch [5/10], Step[260/483], loss: 0.2656
 Epoch [5/10], Step[270/483], loss: 0.2934
 Epoch [5/10], Step[280/483], loss: 0.4553
 Epoch [5/10], Step[290/483], loss: 0.3917
 Epoch [5/10], Step[300/483], loss: 0.3450
 Epoch [5/10], Step[310/483], loss: 0.3474
 Epoch [5/10], Step[320/483], loss: 0.5158
 Epoch [5/10], Step[330/483], loss: 0.3586
 Epoch [5/10], Step[340/483], loss: 0.2358
 Epoch [5/10], Step[350/483], loss: 0.3462
 Epoch [5/10], Step[360/483], loss: 0.2933
 Epoch [5/10], Step[370/483], loss: 0.2352
 Epoch [5/10], Step[380/483], loss: 0.3205
 Epoch [5/10], Step[390/483], loss: 0.2683
 Epoch [5/10], Step[400/483], loss: 0.3910
 Epoch [5/10], Step[410/483], loss: 0.4265
 Epoch [5/10], Step[420/483], loss: 0.4121
 Epoch [5/10], Step[430/483], loss: 0.2991
 Epoch [5/10], Step[440/483], loss: 0.3313
 Epoch [5/10], Step[450/483], loss: 0.3244
 Epoch [5/10], Step[460/483], loss: 0.3630
 Epoch [5/10], Step[470/483], loss: 0.2111
 Epoch [5/10], Step[480/483], loss: 0.2639
 ====> Epoch 5: Training loss: 178.4756
 Epoch [6/10], Step[0/483], loss: 0.3649
 Epoch [6/10], Step[10/483], loss: 0.3051
 Epoch [6/10], Step[20/483], loss: 0.3336
 Epoch [6/10], Step[30/483], loss: 0.2899
 Epoch [6/10], Step[40/483], loss: 0.2605
 Epoch [6/10], Step[50/483], loss: 0.3304
 Epoch [6/10], Step[60/483], loss: 0.3502
 Epoch [6/10], Step[70/483], loss: 0.2083
 Epoch [6/10], Step[80/483], loss: 0.2683
 Epoch [6/10], Step[90/483], loss: 0.2898
 Epoch [6/10], Step[100/483], loss: 0.4861
 Epoch [6/10], Step[110/483], loss: 0.4663
 Epoch [6/10], Step[120/483], loss: 0.2505
 Epoch [6/10], Step[130/483], loss: 0.3415
 Epoch [6/10], Step[140/483], loss: 0.4179
 Epoch [6/10], Step[150/483], loss: 0.3887
 Epoch [6/10], Step[160/483], loss: 0.2852
 Epoch [6/10], Step[170/483], loss: 0.3293
 Epoch [6/10], Step[180/483], loss: 0.2389
 Epoch [6/10], Step[190/483], loss: 0.3629
 Epoch [6/10], Step[200/483], loss: 0.3624
 Epoch [6/10], Step[210/483], loss: 0.3286
 Epoch [6/10], Step[220/483], loss: 0.3385
 Epoch [6/10], Step[230/483], loss: 0.3888
 Epoch [6/10], Step[240/483], loss: 0.3066
 Epoch [6/10], Step[250/483], loss: 0.3163
 Epoch [6/10], Step[260/483], loss: 0.2141
 Epoch [6/10], Step[270/483], loss: 0.2403
 Epoch [6/10], Step[280/483], loss: 0.4528
 Epoch [6/10], Step[290/483], loss: 0.3839
 Epoch [6/10], Step[300/483], loss: 0.2916
 Epoch [6/10], Step[310/483], loss: 0.2710
 Epoch [6/10], Step[320/483], loss: 0.4586
 Epoch [6/10], Step[330/483], loss: 0.2834
 Epoch [6/10], Step[340/483], loss: 0.2950
 Epoch [6/10], Step[350/483], loss: 0.4164
 Epoch [6/10], Step[360/483], loss: 0.2738
 Epoch [6/10], Step[370/483], loss: 0.2172
 Epoch [6/10], Step[380/483], loss: 0.2789
 Epoch [6/10], Step[390/483], loss: 0.2976
 Epoch [6/10], Step[400/483], loss: 0.2871
 Epoch [6/10], Step[410/483], loss: 0.5101
 Epoch [6/10], Step[420/483], loss: 0.2667
 Epoch [6/10], Step[430/483], loss: 0.2520
 Epoch [6/10], Step[440/483], loss: 0.2422
 Epoch [6/10], Step[450/483], loss: 0.2757
 Epoch [6/10], Step[460/483], loss: 0.3282
 Epoch [6/10], Step[470/483], loss: 0.1585
 Epoch [6/10], Step[480/483], loss: 0.2159
 ====> Epoch 6: Training loss: 154.9758
 Epoch [7/10], Step[0/483], loss: 0.2734
 Epoch [7/10], Step[10/483], loss: 0.3105
 Epoch [7/10], Step[20/483], loss: 0.2954
 Epoch [7/10], Step[30/483], loss: 0.3211
 Epoch [7/10], Step[40/483], loss: 0.2273
 Epoch [7/10], Step[50/483], loss: 0.2841
 Epoch [7/10], Step[60/483], loss: 0.2959
 Epoch [7/10], Step[70/483], loss: 0.2844
 Epoch [7/10], Step[80/483], loss: 0.2406
 Epoch [7/10], Step[90/483], loss: 0.3429
 Epoch [7/10], Step[100/483], loss: 0.4019
 Epoch [7/10], Step[110/483], loss: 0.2740
 Epoch [7/10], Step[120/483], loss: 0.2748
 Epoch [7/10], Step[130/483], loss: 0.3414
 Epoch [7/10], Step[140/483], loss: 0.2452
 Epoch [7/10], Step[150/483], loss: 0.3698
 Epoch [7/10], Step[160/483], loss: 0.3513
 Epoch [7/10], Step[170/483], loss: 0.3402
 Epoch [7/10], Step[180/483], loss: 0.1262
 Epoch [7/10], Step[190/483], loss: 0.4081
 Epoch [7/10], Step[200/483], loss: 0.3082
 Epoch [7/10], Step[210/483], loss: 0.2554
 Epoch [7/10], Step[220/483], loss: 0.3418
 Epoch [7/10], Step[230/483], loss: 0.4472
 Epoch [7/10], Step[240/483], loss: 0.3286
 Epoch [7/10], Step[250/483], loss: 0.2130
 Epoch [7/10], Step[260/483], loss: 0.1683
 Epoch [7/10], Step[270/483], loss: 0.2362
 Epoch [7/10], Step[280/483], loss: 0.2912
 Epoch [7/10], Step[290/483], loss: 0.3130
 Epoch [7/10], Step[300/483], loss: 0.3252
 Epoch [7/10], Step[310/483], loss: 0.3762
 Epoch [7/10], Step[320/483], loss: 0.3964
 Epoch [7/10], Step[330/483], loss: 0.3298
 Epoch [7/10], Step[340/483], loss: 0.2001
 Epoch [7/10], Step[350/483], loss: 0.2731
 Epoch [7/10], Step[360/483], loss: 0.2602
 Epoch [7/10], Step[370/483], loss: 0.2591
 Epoch [7/10], Step[380/483], loss: 0.2848
 Epoch [7/10], Step[390/483], loss: 0.3238
 Epoch [7/10], Step[400/483], loss: 0.2978
 Epoch [7/10], Step[410/483], loss: 0.4048
 Epoch [7/10], Step[420/483], loss: 0.3226
 Epoch [7/10], Step[430/483], loss: 0.2741
 Epoch [7/10], Step[440/483], loss: 0.3423
 Epoch [7/10], Step[450/483], loss: 0.2819
 Epoch [7/10], Step[460/483], loss: 0.2630
 Epoch [7/10], Step[470/483], loss: 0.2209
 Epoch [7/10], Step[480/483], loss: 0.2406
 ====> Epoch 7: Training loss: 142.7537
 Epoch [8/10], Step[0/483], loss: 0.2793
 Epoch [8/10], Step[10/483], loss: 0.1734
 Epoch [8/10], Step[20/483], loss: 0.2725
 Epoch [8/10], Step[30/483], loss: 0.2758
 Epoch [8/10], Step[40/483], loss: 0.2545
 Epoch [8/10], Step[50/483], loss: 0.2208
 Epoch [8/10], Step[60/483], loss: 0.3546
 Epoch [8/10], Step[70/483], loss: 0.2981
 Epoch [8/10], Step[80/483], loss: 0.2060
 Epoch [8/10], Step[90/483], loss: 0.3191
 Epoch [8/10], Step[100/483], loss: 0.2675
 Epoch [8/10], Step[110/483], loss: 0.3006
 Epoch [8/10], Step[120/483], loss: 0.2372
 Epoch [8/10], Step[130/483], loss: 0.3616
 Epoch [8/10], Step[140/483], loss: 0.2228
 Epoch [8/10], Step[150/483], loss: 0.3419
 Epoch [8/10], Step[160/483], loss: 0.2536
 Epoch [8/10], Step[170/483], loss: 0.3378
 Epoch [8/10], Step[180/483], loss: 0.2920
 Epoch [8/10], Step[190/483], loss: 0.2618
 Epoch [8/10], Step[200/483], loss: 0.2602
 Epoch [8/10], Step[210/483], loss: 0.2159
 Epoch [8/10], Step[220/483], loss: 0.3850
 Epoch [8/10], Step[230/483], loss: 0.3339
 Epoch [8/10], Step[240/483], loss: 0.2915
 Epoch [8/10], Step[250/483], loss: 0.2111
 Epoch [8/10], Step[260/483], loss: 0.2354
 Epoch [8/10], Step[270/483], loss: 0.3743
 Epoch [8/10], Step[280/483], loss: 0.2474
 Epoch [8/10], Step[290/483], loss: 0.2548
 Epoch [8/10], Step[300/483], loss: 0.1869
 Epoch [8/10], Step[310/483], loss: 0.3961
 Epoch [8/10], Step[320/483], loss: 0.3132
 Epoch [8/10], Step[330/483], loss: 0.4134
 Epoch [8/10], Step[340/483], loss: 0.2185
 Epoch [8/10], Step[350/483], loss: 0.2290
 Epoch [8/10], Step[360/483], loss: 0.2474
 Epoch [8/10], Step[370/483], loss: 0.2567
 Epoch [8/10], Step[380/483], loss: 0.2920
 Epoch [8/10], Step[390/483], loss: 0.3071
 Epoch [8/10], Step[400/483], loss: 0.2782
 Epoch [8/10], Step[410/483], loss: 0.2451
 Epoch [8/10], Step[420/483], loss: 0.3304
 Epoch [8/10], Step[430/483], loss: 0.3232
 Epoch [8/10], Step[440/483], loss: 0.3179
 Epoch [8/10], Step[450/483], loss: 0.2120
 Epoch [8/10], Step[460/483], loss: 0.2273
 Epoch [8/10], Step[470/483], loss: 0.1678
 Epoch [8/10], Step[480/483], loss: 0.2239
 ====> Epoch 8: Training loss: 133.5687
 Epoch [9/10], Step[0/483], loss: 0.3024
 Epoch [9/10], Step[10/483], loss: 0.2968
 Epoch [9/10], Step[20/483], loss: 0.2445
 Epoch [9/10], Step[30/483], loss: 0.2786
 Epoch [9/10], Step[40/483], loss: 0.2435
 Epoch [9/10], Step[50/483], loss: 0.2693
 Epoch [9/10], Step[60/483], loss: 0.2648
 Epoch [9/10], Step[70/483], loss: 0.2001
 Epoch [9/10], Step[80/483], loss: 0.3188
 Epoch [9/10], Step[90/483], loss: 0.2027
 Epoch [9/10], Step[100/483], loss: 0.2827
 Epoch [9/10], Step[110/483], loss: 0.4156
 Epoch [9/10], Step[120/483], loss: 0.2589
 Epoch [9/10], Step[130/483], loss: 0.3030
 Epoch [9/10], Step[140/483], loss: 0.2402
 Epoch [9/10], Step[150/483], loss: 0.2499
 Epoch [9/10], Step[160/483], loss: 0.2022
 Epoch [9/10], Step[170/483], loss: 0.3076
 Epoch [9/10], Step[180/483], loss: 0.1666
 Epoch [9/10], Step[190/483], loss: 0.2967
 Epoch [9/10], Step[200/483], loss: 0.2588
 Epoch [9/10], Step[210/483], loss: 0.2845
 Epoch [9/10], Step[220/483], loss: 0.3138
 Epoch [9/10], Step[230/483], loss: 0.2438
 Epoch [9/10], Step[240/483], loss: 0.2094
 Epoch [9/10], Step[250/483], loss: 0.3189
 Epoch [9/10], Step[260/483], loss: 0.1213
 Epoch [9/10], Step[270/483], loss: 0.2120
 Epoch [9/10], Step[280/483], loss: 0.2723
 Epoch [9/10], Step[290/483], loss: 0.2772
 Epoch [9/10], Step[300/483], loss: 0.2362
 Epoch [9/10], Step[310/483], loss: 0.3801
 Epoch [9/10], Step[320/483], loss: 0.2637
 Epoch [9/10], Step[330/483], loss: 0.2783
 Epoch [9/10], Step[340/483], loss: 0.2115
 Epoch [9/10], Step[350/483], loss: 0.3586
 Epoch [9/10], Step[360/483], loss: 0.3025
 Epoch [9/10], Step[370/483], loss: 0.2625
 Epoch [9/10], Step[380/483], loss: 0.2154
 Epoch [9/10], Step[390/483], loss: 0.3497
 Epoch [9/10], Step[400/483], loss: 0.2568
 Epoch [9/10], Step[410/483], loss: 0.1839
 Epoch [9/10], Step[420/483], loss: 0.2780
 Epoch [9/10], Step[430/483], loss: 0.2236
 Epoch [9/10], Step[440/483], loss: 0.3378
 Epoch [9/10], Step[450/483], loss: 0.2709
 Epoch [9/10], Step[460/483], loss: 0.2630
 Epoch [9/10], Step[470/483], loss: 0.1911
 Epoch [9/10], Step[480/483], loss: 0.2442
 ====> Epoch 9: Training loss: 125.4536
 Epoch [10/10], Step[0/483], loss: 0.2109
 Epoch [10/10], Step[10/483], loss: 0.1476
 Epoch [10/10], Step[20/483], loss: 0.2066
 Epoch [10/10], Step[30/483], loss: 0.2077
 Epoch [10/10], Step[40/483], loss: 0.2029
 Epoch [10/10], Step[50/483], loss: 0.2110
 Epoch [10/10], Step[60/483], loss: 0.3677
 Epoch [10/10], Step[70/483], loss: 0.2113
 Epoch [10/10], Step[80/483], loss: 0.2593
 Epoch [10/10], Step[90/483], loss: 0.2376
 Epoch [10/10], Step[100/483], loss: 0.2712
 Epoch [10/10], Step[110/483], loss: 0.2778
 Epoch [10/10], Step[120/483], loss: 0.2018
 Epoch [10/10], Step[130/483], loss: 0.2550
 Epoch [10/10], Step[140/483], loss: 0.1634
 Epoch [10/10], Step[150/483], loss: 0.2122
 Epoch [10/10], Step[160/483], loss: 0.3003
 Epoch [10/10], Step[170/483], loss: 0.2745
 Epoch [10/10], Step[180/483], loss: 0.2299
 Epoch [10/10], Step[190/483], loss: 0.3458
 Epoch [10/10], Step[200/483], loss: 0.2708
 Epoch [10/10], Step[210/483], loss: 0.2017
 Epoch [10/10], Step[220/483], loss: 0.2604
 Epoch [10/10], Step[230/483], loss: 0.2664
 Epoch [10/10], Step[240/483], loss: 0.2618
 Epoch [10/10], Step[250/483], loss: 0.2071
 Epoch [10/10], Step[260/483], loss: 0.2207
 Epoch [10/10], Step[270/483], loss: 0.2339
 Epoch [10/10], Step[280/483], loss: 0.3000
 Epoch [10/10], Step[290/483], loss: 0.2920
 Epoch [10/10], Step[300/483], loss: 0.2514
 Epoch [10/10], Step[310/483], loss: 0.2910
 Epoch [10/10], Step[320/483], loss: 0.3223
 Epoch [10/10], Step[330/483], loss: 0.2866
 Epoch [10/10], Step[340/483], loss: 0.1780
 Epoch [10/10], Step[350/483], loss: 0.3524
 Epoch [10/10], Step[360/483], loss: 0.2530
 Epoch [10/10], Step[370/483], loss: 0.1471
 Epoch [10/10], Step[380/483], loss: 0.3770
 Epoch [10/10], Step[390/483], loss: 0.2904
 Epoch [10/10], Step[400/483], loss: 0.2203
 Epoch [10/10], Step[410/483], loss: 0.3038
 Epoch [10/10], Step[420/483], loss: 0.2223
 Epoch [10/10], Step[430/483], loss: 0.2463
 Epoch [10/10], Step[440/483], loss: 0.2966
 Epoch [10/10], Step[450/483], loss: 0.2677
 Epoch [10/10], Step[460/483], loss: 0.2791
 Epoch [10/10], Step[470/483], loss: 0.1985
 Epoch [10/10], Step[480/483], loss: 0.0919
 ====> Epoch 10: Training loss: 122.4339
 Training Speaker0...
 Epoch [1/10], Step[0/483], loss: 7.0192
 Epoch [1/10], Step[10/483], loss: 3.4777
 Epoch [1/10], Step[20/483], loss: 2.7098
 Epoch [1/10], Step[30/483], loss: 2.1576
 Epoch [1/10], Step[40/483], loss: 2.0322
 Epoch [1/10], Step[50/483], loss: 2.1175
 Epoch [1/10], Step[60/483], loss: 2.0447
 Epoch [1/10], Step[70/483], loss: 1.8153
 Epoch [1/10], Step[80/483], loss: 1.9790
 Epoch [1/10], Step[90/483], loss: 1.3548
 Epoch [1/10], Step[100/483], loss: 1.2919
 Epoch [1/10], Step[110/483], loss: 1.4760
 Epoch [1/10], Step[120/483], loss: 1.5637
 Epoch [1/10], Step[130/483], loss: 1.2702
 Epoch [1/10], Step[140/483], loss: 1.4726
 Epoch [1/10], Step[150/483], loss: 1.4816
 Epoch [1/10], Step[160/483], loss: 1.5797
 Epoch [1/10], Step[170/483], loss: 1.0627
 Epoch [1/10], Step[180/483], loss: 1.4433
 Epoch [1/10], Step[190/483], loss: 1.4349
 Epoch [1/10], Step[200/483], loss: 0.9942
 Epoch [1/10], Step[210/483], loss: 1.2451
 Epoch [1/10], Step[220/483], loss: 1.0739
 Epoch [1/10], Step[230/483], loss: 1.0240
 Epoch [1/10], Step[240/483], loss: 1.2514
 Epoch [1/10], Step[250/483], loss: 1.3258
 Epoch [1/10], Step[260/483], loss: 0.8255
 Epoch [1/10], Step[270/483], loss: 1.2346
 Epoch [1/10], Step[280/483], loss: 1.0291
 Epoch [1/10], Step[290/483], loss: 0.8647
 Epoch [1/10], Step[300/483], loss: 1.2141
 Epoch [1/10], Step[310/483], loss: 1.2134
 Epoch [1/10], Step[320/483], loss: 1.3373
 Epoch [1/10], Step[330/483], loss: 1.1642
 Epoch [1/10], Step[340/483], loss: 1.2240
 Epoch [1/10], Step[350/483], loss: 1.1655
 Epoch [1/10], Step[360/483], loss: 0.8916
 Epoch [1/10], Step[370/483], loss: 0.7564
 Epoch [1/10], Step[380/483], loss: 1.0199
 Epoch [1/10], Step[390/483], loss: 1.1345
 Epoch [1/10], Step[400/483], loss: 1.0912
 Epoch [1/10], Step[410/483], loss: 0.8559
 Epoch [1/10], Step[420/483], loss: 1.0775
 Epoch [1/10], Step[430/483], loss: 1.0501
 Epoch [1/10], Step[440/483], loss: 1.1404
 Epoch [1/10], Step[450/483], loss: 0.9238
 Epoch [1/10], Step[460/483], loss: 1.1991
 Epoch [1/10], Step[470/483], loss: 1.3201
 Epoch [1/10], Step[480/483], loss: 0.7462
 ====> Epoch 1: Training loss: 689.4526
 Epoch [2/10], Step[0/483], loss: 0.9325
 Epoch [2/10], Step[10/483], loss: 1.0583
 Epoch [2/10], Step[20/483], loss: 1.1423
 Epoch [2/10], Step[30/483], loss: 1.0908
 Epoch [2/10], Step[40/483], loss: 1.1410
 Epoch [2/10], Step[50/483], loss: 1.2097
 Epoch [2/10], Step[60/483], loss: 1.3339
 Epoch [2/10], Step[70/483], loss: 1.1749
 Epoch [2/10], Step[80/483], loss: 1.3670
 Epoch [2/10], Step[90/483], loss: 0.9023
 Epoch [2/10], Step[100/483], loss: 0.9175
 Epoch [2/10], Step[110/483], loss: 1.1014
 Epoch [2/10], Step[120/483], loss: 1.2068
 Epoch [2/10], Step[130/483], loss: 0.9782
 Epoch [2/10], Step[140/483], loss: 1.1244
 Epoch [2/10], Step[150/483], loss: 1.1772
 Epoch [2/10], Step[160/483], loss: 1.2968
 Epoch [2/10], Step[170/483], loss: 0.8613
 Epoch [2/10], Step[180/483], loss: 1.1990
 Epoch [2/10], Step[190/483], loss: 1.1805
 Epoch [2/10], Step[200/483], loss: 0.8512
 Epoch [2/10], Step[210/483], loss: 1.0484
 Epoch [2/10], Step[220/483], loss: 0.9322
 Epoch [2/10], Step[230/483], loss: 0.8396
 Epoch [2/10], Step[240/483], loss: 1.0820
 Epoch [2/10], Step[250/483], loss: 1.0913
 Epoch [2/10], Step[260/483], loss: 0.7000
 Epoch [2/10], Step[270/483], loss: 1.0380
 Epoch [2/10], Step[280/483], loss: 0.9028
 Epoch [2/10], Step[290/483], loss: 0.7409
 Epoch [2/10], Step[300/483], loss: 1.0649
 Epoch [2/10], Step[310/483], loss: 1.0862
 Epoch [2/10], Step[320/483], loss: 1.1498
 Epoch [2/10], Step[330/483], loss: 1.0371
 Epoch [2/10], Step[340/483], loss: 1.0823
 Epoch [2/10], Step[350/483], loss: 1.0348
 Epoch [2/10], Step[360/483], loss: 0.8006
 Epoch [2/10], Step[370/483], loss: 0.6833
 Epoch [2/10], Step[380/483], loss: 0.9453
 Epoch [2/10], Step[390/483], loss: 1.0366
 Epoch [2/10], Step[400/483], loss: 0.9781
 Epoch [2/10], Step[410/483], loss: 0.7900
 Epoch [2/10], Step[420/483], loss: 0.9634
 Epoch [2/10], Step[430/483], loss: 0.9859
 Epoch [2/10], Step[440/483], loss: 1.0111
 Epoch [2/10], Step[450/483], loss: 0.8365
 Epoch [2/10], Step[460/483], loss: 1.1128
 Epoch [2/10], Step[470/483], loss: 1.1774
 Epoch [2/10], Step[480/483], loss: 0.6840
 ====> Epoch 2: Training loss: 506.6455
 Epoch [3/10], Step[0/483], loss: 0.8456
 Epoch [3/10], Step[10/483], loss: 0.9715
 Epoch [3/10], Step[20/483], loss: 1.0597
 Epoch [3/10], Step[30/483], loss: 1.0132
 Epoch [3/10], Step[40/483], loss: 1.0389
 Epoch [3/10], Step[50/483], loss: 1.1141
 Epoch [3/10], Step[60/483], loss: 1.2250
 Epoch [3/10], Step[70/483], loss: 1.1018
 Epoch [3/10], Step[80/483], loss: 1.2677
 Epoch [3/10], Step[90/483], loss: 0.8423
 Epoch [3/10], Step[100/483], loss: 0.8589
 Epoch [3/10], Step[110/483], loss: 1.0140
 Epoch [3/10], Step[120/483], loss: 1.1228
 Epoch [3/10], Step[130/483], loss: 0.9136
 Epoch [3/10], Step[140/483], loss: 1.0420
 Epoch [3/10], Step[150/483], loss: 1.0846
 Epoch [3/10], Step[160/483], loss: 1.2095
 Epoch [3/10], Step[170/483], loss: 0.8064
 Epoch [3/10], Step[180/483], loss: 1.1425
 Epoch [3/10], Step[190/483], loss: 1.1089
 Epoch [3/10], Step[200/483], loss: 0.8014
 Epoch [3/10], Step[210/483], loss: 1.0027
 Epoch [3/10], Step[220/483], loss: 0.8857
 Epoch [3/10], Step[230/483], loss: 0.7884
 Epoch [3/10], Step[240/483], loss: 1.0047
 Epoch [3/10], Step[250/483], loss: 1.0444
 Epoch [3/10], Step[260/483], loss: 0.6514
 Epoch [3/10], Step[270/483], loss: 0.9680
 Epoch [3/10], Step[280/483], loss: 0.8534
 Epoch [3/10], Step[290/483], loss: 0.7085
 Epoch [3/10], Step[300/483], loss: 1.0277
 Epoch [3/10], Step[310/483], loss: 1.0262
 Epoch [3/10], Step[320/483], loss: 1.0973
 Epoch [3/10], Step[330/483], loss: 0.9831
 Epoch [3/10], Step[340/483], loss: 1.0213
 Epoch [3/10], Step[350/483], loss: 0.9859
 Epoch [3/10], Step[360/483], loss: 0.7589
 Epoch [3/10], Step[370/483], loss: 0.6508
 Epoch [3/10], Step[380/483], loss: 0.9052
 Epoch [3/10], Step[390/483], loss: 0.9882
 Epoch [3/10], Step[400/483], loss: 0.9277
 Epoch [3/10], Step[410/483], loss: 0.7611
 Epoch [3/10], Step[420/483], loss: 0.9172
 Epoch [3/10], Step[430/483], loss: 0.9403
 Epoch [3/10], Step[440/483], loss: 0.9529
 Epoch [3/10], Step[450/483], loss: 0.7978
 Epoch [3/10], Step[460/483], loss: 1.0741
 Epoch [3/10], Step[470/483], loss: 1.1139
 Epoch [3/10], Step[480/483], loss: 0.6476
 ====> Epoch 3: Training loss: 475.5337
 Epoch [4/10], Step[0/483], loss: 0.8178
 Epoch [4/10], Step[10/483], loss: 0.9251
 Epoch [4/10], Step[20/483], loss: 1.0163
 Epoch [4/10], Step[30/483], loss: 0.9524
 Epoch [4/10], Step[40/483], loss: 1.0117
 Epoch [4/10], Step[50/483], loss: 1.0727
 Epoch [4/10], Step[60/483], loss: 1.1597
 Epoch [4/10], Step[70/483], loss: 1.0516
 Epoch [4/10], Step[80/483], loss: 1.2123
 Epoch [4/10], Step[90/483], loss: 0.8116
 Epoch [4/10], Step[100/483], loss: 0.8264
 Epoch [4/10], Step[110/483], loss: 0.9714
 Epoch [4/10], Step[120/483], loss: 1.0679
 Epoch [4/10], Step[130/483], loss: 0.8843
 Epoch [4/10], Step[140/483], loss: 1.0075
 Epoch [4/10], Step[150/483], loss: 1.0222
 Epoch [4/10], Step[160/483], loss: 1.1665
 Epoch [4/10], Step[170/483], loss: 0.7681
 Epoch [4/10], Step[180/483], loss: 1.0938
 Epoch [4/10], Step[190/483], loss: 1.0662
 Epoch [4/10], Step[200/483], loss: 0.7633
 Epoch [4/10], Step[210/483], loss: 0.9721
 Epoch [4/10], Step[220/483], loss: 0.8539
 Epoch [4/10], Step[230/483], loss: 0.7535
 Epoch [4/10], Step[240/483], loss: 0.9613
 Epoch [4/10], Step[250/483], loss: 1.0099
 Epoch [4/10], Step[260/483], loss: 0.6308
 Epoch [4/10], Step[270/483], loss: 0.9367
 Epoch [4/10], Step[280/483], loss: 0.8364
 Epoch [4/10], Step[290/483], loss: 0.6848
 Epoch [4/10], Step[300/483], loss: 1.0016
 Epoch [4/10], Step[310/483], loss: 0.9764
 Epoch [4/10], Step[320/483], loss: 1.0622
 Epoch [4/10], Step[330/483], loss: 0.9565
 Epoch [4/10], Step[340/483], loss: 0.9819
 Epoch [4/10], Step[350/483], loss: 0.9494
 Epoch [4/10], Step[360/483], loss: 0.7316
 Epoch [4/10], Step[370/483], loss: 0.6269
 Epoch [4/10], Step[380/483], loss: 0.8649
 Epoch [4/10], Step[390/483], loss: 0.9538
 Epoch [4/10], Step[400/483], loss: 0.9069
 Epoch [4/10], Step[410/483], loss: 0.7335
 Epoch [4/10], Step[420/483], loss: 0.8946
 Epoch [4/10], Step[430/483], loss: 0.9192
 Epoch [4/10], Step[440/483], loss: 0.9139
 Epoch [4/10], Step[450/483], loss: 0.7646
 Epoch [4/10], Step[460/483], loss: 1.0325
 Epoch [4/10], Step[470/483], loss: 1.0645
 Epoch [4/10], Step[480/483], loss: 0.6291
 ====> Epoch 4: Training loss: 456.7602
 Epoch [5/10], Step[0/483], loss: 0.8043
 Epoch [5/10], Step[10/483], loss: 0.8898
 Epoch [5/10], Step[20/483], loss: 0.9787
 Epoch [5/10], Step[30/483], loss: 0.9237
 Epoch [5/10], Step[40/483], loss: 0.9732
 Epoch [5/10], Step[50/483], loss: 1.0366
 Epoch [5/10], Step[60/483], loss: 1.1239
 Epoch [5/10], Step[70/483], loss: 1.0215
 Epoch [5/10], Step[80/483], loss: 1.1628
 Epoch [5/10], Step[90/483], loss: 0.7838
 Epoch [5/10], Step[100/483], loss: 0.8137
 Epoch [5/10], Step[110/483], loss: 0.9502
 Epoch [5/10], Step[120/483], loss: 1.0299
 Epoch [5/10], Step[130/483], loss: 0.8556
 Epoch [5/10], Step[140/483], loss: 0.9710
 Epoch [5/10], Step[150/483], loss: 0.9839
 Epoch [5/10], Step[160/483], loss: 1.1214
 Epoch [5/10], Step[170/483], loss: 0.7531
 Epoch [5/10], Step[180/483], loss: 1.0583
 Epoch [5/10], Step[190/483], loss: 1.0345
 Epoch [5/10], Step[200/483], loss: 0.7355
 Epoch [5/10], Step[210/483], loss: 0.9478
 Epoch [5/10], Step[220/483], loss: 0.8324
 Epoch [5/10], Step[230/483], loss: 0.7366
 Epoch [5/10], Step[240/483], loss: 0.9315
 Epoch [5/10], Step[250/483], loss: 0.9760
 Epoch [5/10], Step[260/483], loss: 0.6104
 Epoch [5/10], Step[270/483], loss: 0.9081
 Epoch [5/10], Step[280/483], loss: 0.8054
 Epoch [5/10], Step[290/483], loss: 0.6561
 Epoch [5/10], Step[300/483], loss: 0.9857
 Epoch [5/10], Step[310/483], loss: 0.9477
 Epoch [5/10], Step[320/483], loss: 1.0233
 Epoch [5/10], Step[330/483], loss: 0.9441
 Epoch [5/10], Step[340/483], loss: 0.9630
 Epoch [5/10], Step[350/483], loss: 0.9251
 Epoch [5/10], Step[360/483], loss: 0.7105
 Epoch [5/10], Step[370/483], loss: 0.6098
 Epoch [5/10], Step[380/483], loss: 0.8334
 Epoch [5/10], Step[390/483], loss: 0.9237
 Epoch [5/10], Step[400/483], loss: 0.8856
 Epoch [5/10], Step[410/483], loss: 0.7262
 Epoch [5/10], Step[420/483], loss: 0.8731
 Epoch [5/10], Step[430/483], loss: 0.8953
 Epoch [5/10], Step[440/483], loss: 0.8918
 Epoch [5/10], Step[450/483], loss: 0.7392
 Epoch [5/10], Step[460/483], loss: 1.0055
 Epoch [5/10], Step[470/483], loss: 1.0354
 Epoch [5/10], Step[480/483], loss: 0.6214
 ====> Epoch 5: Training loss: 444.0288
 Epoch [6/10], Step[0/483], loss: 0.7890
 Epoch [6/10], Step[10/483], loss: 0.8628
 Epoch [6/10], Step[20/483], loss: 0.9612
 Epoch [6/10], Step[30/483], loss: 0.9106
 Epoch [6/10], Step[40/483], loss: 0.9449
 Epoch [6/10], Step[50/483], loss: 1.0042
 Epoch [6/10], Step[60/483], loss: 1.0823
 Epoch [6/10], Step[70/483], loss: 0.9870
 Epoch [6/10], Step[80/483], loss: 1.1176
 Epoch [6/10], Step[90/483], loss: 0.7610
 Epoch [6/10], Step[100/483], loss: 0.7924
 Epoch [6/10], Step[110/483], loss: 0.9329
 Epoch [6/10], Step[120/483], loss: 1.0058
 Epoch [6/10], Step[130/483], loss: 0.8463
 Epoch [6/10], Step[140/483], loss: 0.9515
 Epoch [6/10], Step[150/483], loss: 0.9719
 Epoch [6/10], Step[160/483], loss: 1.0932
 Epoch [6/10], Step[170/483], loss: 0.7306
 Epoch [6/10], Step[180/483], loss: 1.0319
 Epoch [6/10], Step[190/483], loss: 1.0109
 Epoch [6/10], Step[200/483], loss: 0.7248
 Epoch [6/10], Step[210/483], loss: 0.9246
 Epoch [6/10], Step[220/483], loss: 0.8167
 Epoch [6/10], Step[230/483], loss: 0.7276
 Epoch [6/10], Step[240/483], loss: 0.9059
 Epoch [6/10], Step[250/483], loss: 0.9495
 Epoch [6/10], Step[260/483], loss: 0.6072
 Epoch [6/10], Step[270/483], loss: 0.8947
 Epoch [6/10], Step[280/483], loss: 0.7920
 Epoch [6/10], Step[290/483], loss: 0.6478
 Epoch [6/10], Step[300/483], loss: 0.9665
 Epoch [6/10], Step[310/483], loss: 0.9281
 Epoch [6/10], Step[320/483], loss: 1.0026
 Epoch [6/10], Step[330/483], loss: 0.9279
 Epoch [6/10], Step[340/483], loss: 0.9357
 Epoch [6/10], Step[350/483], loss: 0.8955
 Epoch [6/10], Step[360/483], loss: 0.6795
 Epoch [6/10], Step[370/483], loss: 0.6018
 Epoch [6/10], Step[380/483], loss: 0.8188
 Epoch [6/10], Step[390/483], loss: 0.8983
 Epoch [6/10], Step[400/483], loss: 0.8558
 Epoch [6/10], Step[410/483], loss: 0.7126
 Epoch [6/10], Step[420/483], loss: 0.8571
 Epoch [6/10], Step[430/483], loss: 0.8764
 Epoch [6/10], Step[440/483], loss: 0.8685
 Epoch [6/10], Step[450/483], loss: 0.7257
 Epoch [6/10], Step[460/483], loss: 0.9965
 Epoch [6/10], Step[470/483], loss: 1.0288
 Epoch [6/10], Step[480/483], loss: 0.6043
 ====> Epoch 6: Training loss: 434.5491
 Epoch [7/10], Step[0/483], loss: 0.7683
 Epoch [7/10], Step[10/483], loss: 0.8567
 Epoch [7/10], Step[20/483], loss: 0.9255
 Epoch [7/10], Step[30/483], loss: 0.8815
 Epoch [7/10], Step[40/483], loss: 0.9304
 Epoch [7/10], Step[50/483], loss: 0.9875
 Epoch [7/10], Step[60/483], loss: 1.0741
 Epoch [7/10], Step[70/483], loss: 0.9757
 Epoch [7/10], Step[80/483], loss: 1.1027
 Epoch [7/10], Step[90/483], loss: 0.7526
 Epoch [7/10], Step[100/483], loss: 0.7790
 Epoch [7/10], Step[110/483], loss: 0.9150
 Epoch [7/10], Step[120/483], loss: 0.9881
 Epoch [7/10], Step[130/483], loss: 0.8429
 Epoch [7/10], Step[140/483], loss: 0.9461
 Epoch [7/10], Step[150/483], loss: 0.9459
 Epoch [7/10], Step[160/483], loss: 1.0680
 Epoch [7/10], Step[170/483], loss: 0.7123
 Epoch [7/10], Step[180/483], loss: 1.0104
 Epoch [7/10], Step[190/483], loss: 1.0065
 Epoch [7/10], Step[200/483], loss: 0.7136
 Epoch [7/10], Step[210/483], loss: 0.8982
 Epoch [7/10], Step[220/483], loss: 0.8075
 Epoch [7/10], Step[230/483], loss: 0.7273
 Epoch [7/10], Step[240/483], loss: 0.9013
 Epoch [7/10], Step[250/483], loss: 0.9329
 Epoch [7/10], Step[260/483], loss: 0.5999
 Epoch [7/10], Step[270/483], loss: 0.8793
 Epoch [7/10], Step[280/483], loss: 0.7896
 Epoch [7/10], Step[290/483], loss: 0.6421
 Epoch [7/10], Step[300/483], loss: 0.9521
 Epoch [7/10], Step[310/483], loss: 0.9114
 Epoch [7/10], Step[320/483], loss: 0.9774
 Epoch [7/10], Step[330/483], loss: 0.9128
 Epoch [7/10], Step[340/483], loss: 0.9140
 Epoch [7/10], Step[350/483], loss: 0.8703
 Epoch [7/10], Step[360/483], loss: 0.6637
 Epoch [7/10], Step[370/483], loss: 0.5989
 Epoch [7/10], Step[380/483], loss: 0.7974
 Epoch [7/10], Step[390/483], loss: 0.8794
 Epoch [7/10], Step[400/483], loss: 0.8483
 Epoch [7/10], Step[410/483], loss: 0.7089
 Epoch [7/10], Step[420/483], loss: 0.8415
 Epoch [7/10], Step[430/483], loss: 0.8593
 Epoch [7/10], Step[440/483], loss: 0.8429
 Epoch [7/10], Step[450/483], loss: 0.7051
 Epoch [7/10], Step[460/483], loss: 0.9814
 Epoch [7/10], Step[470/483], loss: 1.0025
 Epoch [7/10], Step[480/483], loss: 0.6025
 ====> Epoch 7: Training loss: 426.7846
 Epoch [8/10], Step[0/483], loss: 0.7668
 Epoch [8/10], Step[10/483], loss: 0.8427
 Epoch [8/10], Step[20/483], loss: 0.9162
 Epoch [8/10], Step[30/483], loss: 0.8715
 Epoch [8/10], Step[40/483], loss: 0.9216
 Epoch [8/10], Step[50/483], loss: 0.9651
 Epoch [8/10], Step[60/483], loss: 1.0542
 Epoch [8/10], Step[70/483], loss: 0.9509
 Epoch [8/10], Step[80/483], loss: 1.0824
 Epoch [8/10], Step[90/483], loss: 0.7390
 Epoch [8/10], Step[100/483], loss: 0.7648
 Epoch [8/10], Step[110/483], loss: 0.9028
 Epoch [8/10], Step[120/483], loss: 0.9667
 Epoch [8/10], Step[130/483], loss: 0.8260
 Epoch [8/10], Step[140/483], loss: 0.9362
 Epoch [8/10], Step[150/483], loss: 0.9317
 Epoch [8/10], Step[160/483], loss: 1.0512
 Epoch [8/10], Step[170/483], loss: 0.7105
 Epoch [8/10], Step[180/483], loss: 1.0080
 Epoch [8/10], Step[190/483], loss: 0.9800
 Epoch [8/10], Step[200/483], loss: 0.7048
 Epoch [8/10], Step[210/483], loss: 0.8972
 Epoch [8/10], Step[220/483], loss: 0.7945
 Epoch [8/10], Step[230/483], loss: 0.7126
 Epoch [8/10], Step[240/483], loss: 0.8967
 Epoch [8/10], Step[250/483], loss: 0.9250
 Epoch [8/10], Step[260/483], loss: 0.5937
 Epoch [8/10], Step[270/483], loss: 0.8660
 Epoch [8/10], Step[280/483], loss: 0.7786
 Epoch [8/10], Step[290/483], loss: 0.6410
 Epoch [8/10], Step[300/483], loss: 0.9288
 Epoch [8/10], Step[310/483], loss: 0.8939
 Epoch [8/10], Step[320/483], loss: 0.9489
 Epoch [8/10], Step[330/483], loss: 0.9137
 Epoch [8/10], Step[340/483], loss: 0.8900
 Epoch [8/10], Step[350/483], loss: 0.8724
 Epoch [8/10], Step[360/483], loss: 0.6608
 Epoch [8/10], Step[370/483], loss: 0.5944
 Epoch [8/10], Step[380/483], loss: 0.7837
 Epoch [8/10], Step[390/483], loss: 0.8693
 Epoch [8/10], Step[400/483], loss: 0.8296
 Epoch [8/10], Step[410/483], loss: 0.6924
 Epoch [8/10], Step[420/483], loss: 0.8220
 Epoch [8/10], Step[430/483], loss: 0.8398
 Epoch [8/10], Step[440/483], loss: 0.8350
 Epoch [8/10], Step[450/483], loss: 0.7048
 Epoch [8/10], Step[460/483], loss: 0.9661
 Epoch [8/10], Step[470/483], loss: 1.0007
 Epoch [8/10], Step[480/483], loss: 0.5940
 ====> Epoch 8: Training loss: 420.8455
 Epoch [9/10], Step[0/483], loss: 0.7524
 Epoch [9/10], Step[10/483], loss: 0.8442
 Epoch [9/10], Step[20/483], loss: 0.9031
 Epoch [9/10], Step[30/483], loss: 0.8633
 Epoch [9/10], Step[40/483], loss: 0.9143
 Epoch [9/10], Step[50/483], loss: 0.9540
 Epoch [9/10], Step[60/483], loss: 1.0333
 Epoch [9/10], Step[70/483], loss: 0.9509
 Epoch [9/10], Step[80/483], loss: 1.0619
 Epoch [9/10], Step[90/483], loss: 0.7373
 Epoch [9/10], Step[100/483], loss: 0.7495
 Epoch [9/10], Step[110/483], loss: 0.8881
 Epoch [9/10], Step[120/483], loss: 0.9563
 Epoch [9/10], Step[130/483], loss: 0.8218
 Epoch [9/10], Step[140/483], loss: 0.9266
 Epoch [9/10], Step[150/483], loss: 0.9326
 Epoch [9/10], Step[160/483], loss: 1.0297
 Epoch [9/10], Step[170/483], loss: 0.6978
 Epoch [9/10], Step[180/483], loss: 1.0095
 Epoch [9/10], Step[190/483], loss: 0.9786
 Epoch [9/10], Step[200/483], loss: 0.6975
 Epoch [9/10], Step[210/483], loss: 0.8956
 Epoch [9/10], Step[220/483], loss: 0.7732
 Epoch [9/10], Step[230/483], loss: 0.7134
 Epoch [9/10], Step[240/483], loss: 0.8810
 Epoch [9/10], Step[250/483], loss: 0.9125
 Epoch [9/10], Step[260/483], loss: 0.5922
 Epoch [9/10], Step[270/483], loss: 0.8543
 Epoch [9/10], Step[280/483], loss: 0.7715
 Epoch [9/10], Step[290/483], loss: 0.6343
 Epoch [9/10], Step[300/483], loss: 0.9435
 Epoch [9/10], Step[310/483], loss: 0.8865
 Epoch [9/10], Step[320/483], loss: 0.9349
 Epoch [9/10], Step[330/483], loss: 0.8983
 Epoch [9/10], Step[340/483], loss: 0.8716
 Epoch [9/10], Step[350/483], loss: 0.8646
 Epoch [9/10], Step[360/483], loss: 0.6529
 Epoch [9/10], Step[370/483], loss: 0.5838
 Epoch [9/10], Step[380/483], loss: 0.7713
 Epoch [9/10], Step[390/483], loss: 0.8497
 Epoch [9/10], Step[400/483], loss: 0.8158
 Epoch [9/10], Step[410/483], loss: 0.6897
 Epoch [9/10], Step[420/483], loss: 0.8139
 Epoch [9/10], Step[430/483], loss: 0.8326
 Epoch [9/10], Step[440/483], loss: 0.8351
 Epoch [9/10], Step[450/483], loss: 0.6899
 Epoch [9/10], Step[460/483], loss: 0.9591
 Epoch [9/10], Step[470/483], loss: 0.9867
 Epoch [9/10], Step[480/483], loss: 0.5925
 ====> Epoch 9: Training loss: 415.5240
 Epoch [10/10], Step[0/483], loss: 0.7459
 Epoch [10/10], Step[10/483], loss: 0.8303
 Epoch [10/10], Step[20/483], loss: 0.8861
 Epoch [10/10], Step[30/483], loss: 0.8583
 Epoch [10/10], Step[40/483], loss: 0.9070
 Epoch [10/10], Step[50/483], loss: 0.9404
 Epoch [10/10], Step[60/483], loss: 1.0105
 Epoch [10/10], Step[70/483], loss: 0.9346
 Epoch [10/10], Step[80/483], loss: 1.0519
 Epoch [10/10], Step[90/483], loss: 0.7246
 Epoch [10/10], Step[100/483], loss: 0.7474
 Epoch [10/10], Step[110/483], loss: 0.8793
 Epoch [10/10], Step[120/483], loss: 0.9390
 Epoch [10/10], Step[130/483], loss: 0.8103
 Epoch [10/10], Step[140/483], loss: 0.9140
 Epoch [10/10], Step[150/483], loss: 0.9285
 Epoch [10/10], Step[160/483], loss: 0.9990
 Epoch [10/10], Step[170/483], loss: 0.7002
 Epoch [10/10], Step[180/483], loss: 0.9913
 Epoch [10/10], Step[190/483], loss: 0.9571
 Epoch [10/10], Step[200/483], loss: 0.6922
 Epoch [10/10], Step[210/483], loss: 0.8728
 Epoch [10/10], Step[220/483], loss: 0.7668
 Epoch [10/10], Step[230/483], loss: 0.6977
 Epoch [10/10], Step[240/483], loss: 0.8708
 Epoch [10/10], Step[250/483], loss: 0.9113
 Epoch [10/10], Step[260/483], loss: 0.5860
 Epoch [10/10], Step[270/483], loss: 0.8509
 Epoch [10/10], Step[280/483], loss: 0.7579
 Epoch [10/10], Step[290/483], loss: 0.6316
 Epoch [10/10], Step[300/483], loss: 0.9131
 Epoch [10/10], Step[310/483], loss: 0.8899
 Epoch [10/10], Step[320/483], loss: 0.9311
 Epoch [10/10], Step[330/483], loss: 0.8790
 Epoch [10/10], Step[340/483], loss: 0.8712
 Epoch [10/10], Step[350/483], loss: 0.8535
 Epoch [10/10], Step[360/483], loss: 0.6465
 Epoch [10/10], Step[370/483], loss: 0.5710
 Epoch [10/10], Step[380/483], loss: 0.7619
 Epoch [10/10], Step[390/483], loss: 0.8487
 Epoch [10/10], Step[400/483], loss: 0.8086
 Epoch [10/10], Step[410/483], loss: 0.6861
 Epoch [10/10], Step[420/483], loss: 0.7923
 Epoch [10/10], Step[430/483], loss: 0.8254
 Epoch [10/10], Step[440/483], loss: 0.8332
 Epoch [10/10], Step[450/483], loss: 0.6879
 Epoch [10/10], Step[460/483], loss: 0.9475
 Epoch [10/10], Step[470/483], loss: 0.9704
 Epoch [10/10], Step[480/483], loss: 0.5874
 ====> Epoch 10: Training loss: 411.2421
 SamplingSpeaker1Model: SamplingSpeaker1Model(
  (listener0): Listener0Model(
    (scene_encoder): LinearSceneEncoder(
      (fc): Linear(in_features=280, out_features=50)
    )
    (string_encoder): LinearStringEncoder(
      (fc): Linear(in_features=1063, out_features=50)
    )
    (scorer): MLPScorer(
      (linear_4): Linear(in_features=50, out_features=50)
      (linear_5): Linear(in_features=50, out_features=50)
      (linear_3): Linear(in_features=50, out_features=1)
    )
  )
  (speaker0): Speaker0Model(
    (scene_encoder): LinearSceneEncoder(
      (fc): Linear(in_features=280, out_features=50)
    )
    (string_decoder): LSTMStringDecoder(
      (embedding): Embedding(1063, 50)
      (lstm): LSTM(50, 50, num_layers=2, batch_first=True)
      (linear): Linear(in_features=50, out_features=1063)
      (dropout): Dropout(p=0.0)
    )
  )
)
