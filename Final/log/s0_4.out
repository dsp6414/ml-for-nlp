 Loading feature files...
 All scenes loaded.
 Hyperparameters:Namespace(LR=0.01, alternatives=1, batch_size=100, dec='MLP', dropout=0.0, epochs=20, hidden_sz=50, log_interval=10, model='s0', no_cuda=False, seed=1)
 Speaker0: Listener0Model(
  (scene_encoder): LinearSceneEncoder(
    (fc): Linear(in_features=280, out_features=50)
  )
  (string_encoder): LinearStringEncoder(
    (fc): Linear(in_features=2713, out_features=50)
  )
  (scorer): MLPScorer(
    (linear_4): Linear(in_features=50, out_features=50)
    (linear_5): Linear(in_features=50, out_features=50)
    (linear_3): Linear(in_features=50, out_features=1)
  )
)
 Epoch [1/20], Step[0/483], loss: 7.8993
 Epoch [1/20], Step[10/483], loss: 4.0760
 Epoch [1/20], Step[20/483], loss: 3.5038
 Epoch [1/20], Step[30/483], loss: 3.3388
 Epoch [1/20], Step[40/483], loss: 3.4370
 Epoch [1/20], Step[50/483], loss: 3.6484
 Epoch [1/20], Step[60/483], loss: 3.4424
 Epoch [1/20], Step[70/483], loss: 3.4012
 Epoch [1/20], Step[80/483], loss: 3.7013
 Epoch [1/20], Step[90/483], loss: 2.8655
 Epoch [1/20], Step[100/483], loss: 2.7507
 Epoch [1/20], Step[110/483], loss: 3.1531
 Epoch [1/20], Step[120/483], loss: 3.4870
 Epoch [1/20], Step[130/483], loss: 3.2801
 Epoch [1/20], Step[140/483], loss: 3.4261
 Epoch [1/20], Step[150/483], loss: 3.5229
 Epoch [1/20], Step[160/483], loss: 3.5162
 Epoch [1/20], Step[170/483], loss: 2.6078
 Epoch [1/20], Step[180/483], loss: 3.4259
 Epoch [1/20], Step[190/483], loss: 3.4867
 Epoch [1/20], Step[200/483], loss: 2.5775
 Epoch [1/20], Step[210/483], loss: 3.1051
 Epoch [1/20], Step[220/483], loss: 2.9674
 Epoch [1/20], Step[230/483], loss: 2.7319
 Epoch [1/20], Step[240/483], loss: 3.2435
 Epoch [1/20], Step[250/483], loss: 3.4973
 Epoch [1/20], Step[260/483], loss: 2.4023
 Epoch [1/20], Step[270/483], loss: 3.3262
 Epoch [1/20], Step[280/483], loss: 2.8690
 Epoch [1/20], Step[290/483], loss: 2.5193
 Epoch [1/20], Step[300/483], loss: 3.3817
 Epoch [1/20], Step[310/483], loss: 3.3701
 Epoch [1/20], Step[320/483], loss: 3.4883
 Epoch [1/20], Step[330/483], loss: 3.3244
 Epoch [1/20], Step[340/483], loss: 3.6154
 Epoch [1/20], Step[350/483], loss: 3.2658
 Epoch [1/20], Step[360/483], loss: 2.7049
 Epoch [1/20], Step[370/483], loss: 2.3678
 Epoch [1/20], Step[380/483], loss: 3.1125
 Epoch [1/20], Step[390/483], loss: 3.3177
 Epoch [1/20], Step[400/483], loss: 3.3456
 Epoch [1/20], Step[410/483], loss: 2.7308
 Epoch [1/20], Step[420/483], loss: 2.9733
 Epoch [1/20], Step[430/483], loss: 2.8926
 Epoch [1/20], Step[440/483], loss: 3.4196
 Epoch [1/20], Step[450/483], loss: 2.6171
 Epoch [1/20], Step[460/483], loss: 3.2882
 Epoch [1/20], Step[470/483], loss: 3.5367
 Epoch [1/20], Step[480/483], loss: 2.3945
 ====> Epoch 1: Training loss: 1577.7497
 Epoch [2/20], Step[0/483], loss: 2.9640
 Epoch [2/20], Step[10/483], loss: 3.0794
 Epoch [2/20], Step[20/483], loss: 3.3197
 Epoch [2/20], Step[30/483], loss: 3.1751
 Epoch [2/20], Step[40/483], loss: 3.2269
 Epoch [2/20], Step[50/483], loss: 3.5408
 Epoch [2/20], Step[60/483], loss: 3.4255
 Epoch [2/20], Step[70/483], loss: 3.3533
 Epoch [2/20], Step[80/483], loss: 3.6927
 Epoch [2/20], Step[90/483], loss: 2.8538
 Epoch [2/20], Step[100/483], loss: 2.7260
 Epoch [2/20], Step[110/483], loss: 3.0697
 Epoch [2/20], Step[120/483], loss: 3.4523
 Epoch [2/20], Step[130/483], loss: 3.2794
 Epoch [2/20], Step[140/483], loss: 3.4617
 Epoch [2/20], Step[150/483], loss: 3.4652
 Epoch [2/20], Step[160/483], loss: 3.4736
 Epoch [2/20], Step[170/483], loss: 2.5735
 Epoch [2/20], Step[180/483], loss: 3.3724
 Epoch [2/20], Step[190/483], loss: 3.4229
 Epoch [2/20], Step[200/483], loss: 2.5551
 Epoch [2/20], Step[210/483], loss: 3.0659
 Epoch [2/20], Step[220/483], loss: 2.9513
 Epoch [2/20], Step[230/483], loss: 2.6701
 Epoch [2/20], Step[240/483], loss: 3.1988
 Epoch [2/20], Step[250/483], loss: 3.4495
 Epoch [2/20], Step[260/483], loss: 2.3796
 Epoch [2/20], Step[270/483], loss: 3.3198
 Epoch [2/20], Step[280/483], loss: 2.8555
 Epoch [2/20], Step[290/483], loss: 2.4811
 Epoch [2/20], Step[300/483], loss: 3.3670
 Epoch [2/20], Step[310/483], loss: 3.3501
 Epoch [2/20], Step[320/483], loss: 3.4591
 Epoch [2/20], Step[330/483], loss: 3.2772
 Epoch [2/20], Step[340/483], loss: 3.5879
 Epoch [2/20], Step[350/483], loss: 3.2406
 Epoch [2/20], Step[360/483], loss: 2.6606
 Epoch [2/20], Step[370/483], loss: 2.2995
 Epoch [2/20], Step[380/483], loss: 3.0571
 Epoch [2/20], Step[390/483], loss: 3.2851
 Epoch [2/20], Step[400/483], loss: 3.3141
 Epoch [2/20], Step[410/483], loss: 2.6770
 Epoch [2/20], Step[420/483], loss: 2.9455
 Epoch [2/20], Step[430/483], loss: 2.8883
 Epoch [2/20], Step[440/483], loss: 3.3796
 Epoch [2/20], Step[450/483], loss: 2.5768
 Epoch [2/20], Step[460/483], loss: 3.2684
 Epoch [2/20], Step[470/483], loss: 3.5286
 Epoch [2/20], Step[480/483], loss: 2.3164
 ====> Epoch 2: Training loss: 1535.8241
 Epoch [3/20], Step[0/483], loss: 2.9534
 Epoch [3/20], Step[10/483], loss: 3.0689
 Epoch [3/20], Step[20/483], loss: 3.2863
 Epoch [3/20], Step[30/483], loss: 3.1653
 Epoch [3/20], Step[40/483], loss: 3.1871
 Epoch [3/20], Step[50/483], loss: 3.4811
 Epoch [3/20], Step[60/483], loss: 3.3858
 Epoch [3/20], Step[70/483], loss: 3.3619
 Epoch [3/20], Step[80/483], loss: 3.6621
 Epoch [3/20], Step[90/483], loss: 2.8484
 Epoch [3/20], Step[100/483], loss: 2.7233
 Epoch [3/20], Step[110/483], loss: 3.0827
 Epoch [3/20], Step[120/483], loss: 3.4572
 Epoch [3/20], Step[130/483], loss: 3.2623
 Epoch [3/20], Step[140/483], loss: 3.3832
 Epoch [3/20], Step[150/483], loss: 3.4355
 Epoch [3/20], Step[160/483], loss: 3.4743
 Epoch [3/20], Step[170/483], loss: 2.5725
 Epoch [3/20], Step[180/483], loss: 3.3694
 Epoch [3/20], Step[190/483], loss: 3.4718
 Epoch [3/20], Step[200/483], loss: 2.5497
 Epoch [3/20], Step[210/483], loss: 3.0665
 Epoch [3/20], Step[220/483], loss: 2.9484
 Epoch [3/20], Step[230/483], loss: 2.6658
 Epoch [3/20], Step[240/483], loss: 3.1992
 Epoch [3/20], Step[250/483], loss: 3.4532
 Epoch [3/20], Step[260/483], loss: 2.3670
 Epoch [3/20], Step[270/483], loss: 3.3104
 Epoch [3/20], Step[280/483], loss: 2.8446
 Epoch [3/20], Step[290/483], loss: 2.4843
 Epoch [3/20], Step[300/483], loss: 3.3727
 Epoch [3/20], Step[310/483], loss: 3.3434
 Epoch [3/20], Step[320/483], loss: 3.4517
 Epoch [3/20], Step[330/483], loss: 3.2547
 Epoch [3/20], Step[340/483], loss: 3.6181
 Epoch [3/20], Step[350/483], loss: 3.2300
 Epoch [3/20], Step[360/483], loss: 2.6380
 Epoch [3/20], Step[370/483], loss: 2.3010
 Epoch [3/20], Step[380/483], loss: 3.0576
 Epoch [3/20], Step[390/483], loss: 3.2760
 Epoch [3/20], Step[400/483], loss: 3.3029
 Epoch [3/20], Step[410/483], loss: 2.6698
 Epoch [3/20], Step[420/483], loss: 2.9511
 Epoch [3/20], Step[430/483], loss: 2.8858
 Epoch [3/20], Step[440/483], loss: 3.3810
 Epoch [3/20], Step[450/483], loss: 2.5673
 Epoch [3/20], Step[460/483], loss: 3.2568
 Epoch [3/20], Step[470/483], loss: 3.5210
 Epoch [3/20], Step[480/483], loss: 2.3100
 ====> Epoch 3: Training loss: 1531.5235
 Epoch [4/20], Step[0/483], loss: 2.9562
 Epoch [4/20], Step[10/483], loss: 3.0616
 Epoch [4/20], Step[20/483], loss: 3.2811
 Epoch [4/20], Step[30/483], loss: 3.1627
 Epoch [4/20], Step[40/483], loss: 3.1791
 Epoch [4/20], Step[50/483], loss: 3.4858
 Epoch [4/20], Step[60/483], loss: 3.3842
 Epoch [4/20], Step[70/483], loss: 3.3293
 Epoch [4/20], Step[80/483], loss: 3.6666
 Epoch [4/20], Step[90/483], loss: 2.8336
 Epoch [4/20], Step[100/483], loss: 2.7185
 Epoch [4/20], Step[110/483], loss: 3.0905
 Epoch [4/20], Step[120/483], loss: 3.4545
 Epoch [4/20], Step[130/483], loss: 3.2515
 Epoch [4/20], Step[140/483], loss: 3.3881
 Epoch [4/20], Step[150/483], loss: 3.4409
 Epoch [4/20], Step[160/483], loss: 3.4603
 Epoch [4/20], Step[170/483], loss: 2.5744
 Epoch [4/20], Step[180/483], loss: 3.3676
 Epoch [4/20], Step[190/483], loss: 3.4152
 Epoch [4/20], Step[200/483], loss: 2.5478
 Epoch [4/20], Step[210/483], loss: 3.0571
 Epoch [4/20], Step[220/483], loss: 2.9529
 Epoch [4/20], Step[230/483], loss: 2.6572
 Epoch [4/20], Step[240/483], loss: 3.1987
 Epoch [4/20], Step[250/483], loss: 3.4521
 Epoch [4/20], Step[260/483], loss: 2.3764
 Epoch [4/20], Step[270/483], loss: 3.2988
 Epoch [4/20], Step[280/483], loss: 2.8425
 Epoch [4/20], Step[290/483], loss: 2.4816
 Epoch [4/20], Step[300/483], loss: 3.3609
 Epoch [4/20], Step[310/483], loss: 3.3468
 Epoch [4/20], Step[320/483], loss: 3.4443
 Epoch [4/20], Step[330/483], loss: 3.2541
 Epoch [4/20], Step[340/483], loss: 3.5813
 Epoch [4/20], Step[350/483], loss: 3.2152
 Epoch [4/20], Step[360/483], loss: 2.6524
 Epoch [4/20], Step[370/483], loss: 2.2864
 Epoch [4/20], Step[380/483], loss: 3.0469
 Epoch [4/20], Step[390/483], loss: 3.2675
 Epoch [4/20], Step[400/483], loss: 3.3043
 Epoch [4/20], Step[410/483], loss: 2.6687
 Epoch [4/20], Step[420/483], loss: 2.9546
 Epoch [4/20], Step[430/483], loss: 2.8708
 Epoch [4/20], Step[440/483], loss: 3.3736
 Epoch [4/20], Step[450/483], loss: 2.5556
 Epoch [4/20], Step[460/483], loss: 3.2564
 Epoch [4/20], Step[470/483], loss: 3.5159
 Epoch [4/20], Step[480/483], loss: 2.3135
 ====> Epoch 4: Training loss: 1528.6097
 Epoch [5/20], Step[0/483], loss: 2.9534
 Epoch [5/20], Step[10/483], loss: 3.0613
 Epoch [5/20], Step[20/483], loss: 3.2821
 Epoch [5/20], Step[30/483], loss: 3.1598
 Epoch [5/20], Step[40/483], loss: 3.1834
 Epoch [5/20], Step[50/483], loss: 3.4823
 Epoch [5/20], Step[60/483], loss: 3.3802
 Epoch [5/20], Step[70/483], loss: 3.3343
 Epoch [5/20], Step[80/483], loss: 3.6365
 Epoch [5/20], Step[90/483], loss: 2.8285
 Epoch [5/20], Step[100/483], loss: 2.7101
 Epoch [5/20], Step[110/483], loss: 3.0706
 Epoch [5/20], Step[120/483], loss: 3.4419
 Epoch [5/20], Step[130/483], loss: 3.2528
 Epoch [5/20], Step[140/483], loss: 3.3898
 Epoch [5/20], Step[150/483], loss: 3.4358
 Epoch [5/20], Step[160/483], loss: 3.4388
 Epoch [5/20], Step[170/483], loss: 2.5730
 Epoch [5/20], Step[180/483], loss: 3.3581
 Epoch [5/20], Step[190/483], loss: 3.4134
 Epoch [5/20], Step[200/483], loss: 2.5571
 Epoch [5/20], Step[210/483], loss: 3.0566
 Epoch [5/20], Step[220/483], loss: 2.9480
 Epoch [5/20], Step[230/483], loss: 2.6698
 Epoch [5/20], Step[240/483], loss: 3.1898
 Epoch [5/20], Step[250/483], loss: 3.4571
 Epoch [5/20], Step[260/483], loss: 2.3620
 Epoch [5/20], Step[270/483], loss: 3.3046
 Epoch [5/20], Step[280/483], loss: 2.8442
 Epoch [5/20], Step[290/483], loss: 2.4881
 Epoch [5/20], Step[300/483], loss: 3.3642
 Epoch [5/20], Step[310/483], loss: 3.3446
 Epoch [5/20], Step[320/483], loss: 3.4589
 Epoch [5/20], Step[330/483], loss: 3.2487
 Epoch [5/20], Step[340/483], loss: 3.5864
 Epoch [5/20], Step[350/483], loss: 3.2136
 Epoch [5/20], Step[360/483], loss: 2.6453
 Epoch [5/20], Step[370/483], loss: 2.2821
 Epoch [5/20], Step[380/483], loss: 3.0487
 Epoch [5/20], Step[390/483], loss: 3.2720
 Epoch [5/20], Step[400/483], loss: 3.2964
 Epoch [5/20], Step[410/483], loss: 2.6731
 Epoch [5/20], Step[420/483], loss: 2.9554
 Epoch [5/20], Step[430/483], loss: 2.8678
 Epoch [5/20], Step[440/483], loss: 3.3762
 Epoch [5/20], Step[450/483], loss: 2.5575
 Epoch [5/20], Step[460/483], loss: 3.2486
 Epoch [5/20], Step[470/483], loss: 3.5182
 Epoch [5/20], Step[480/483], loss: 2.3041
 ====> Epoch 5: Training loss: 1527.4427
 Epoch [6/20], Step[0/483], loss: 2.9516
 Epoch [6/20], Step[10/483], loss: 3.0633
 Epoch [6/20], Step[20/483], loss: 3.2785
 Epoch [6/20], Step[30/483], loss: 3.1538
 Epoch [6/20], Step[40/483], loss: 3.2108
 Epoch [6/20], Step[50/483], loss: 3.4859
 Epoch [6/20], Step[60/483], loss: 3.3817
 Epoch [6/20], Step[70/483], loss: 3.3293
 Epoch [6/20], Step[80/483], loss: 3.6440
 Epoch [6/20], Step[90/483], loss: 2.8263
 Epoch [6/20], Step[100/483], loss: 2.7085
 Epoch [6/20], Step[110/483], loss: 3.0650
 Epoch [6/20], Step[120/483], loss: 3.4293
 Epoch [6/20], Step[130/483], loss: 3.2450
 Epoch [6/20], Step[140/483], loss: 3.3816
 Epoch [6/20], Step[150/483], loss: 3.4290
 Epoch [6/20], Step[160/483], loss: 3.4399
 Epoch [6/20], Step[170/483], loss: 2.5873
 Epoch [6/20], Step[180/483], loss: 3.3629
 Epoch [6/20], Step[190/483], loss: 3.4067
 Epoch [6/20], Step[200/483], loss: 2.5615
 Epoch [6/20], Step[210/483], loss: 3.0414
 Epoch [6/20], Step[220/483], loss: 2.9545
 Epoch [6/20], Step[230/483], loss: 2.6625
 Epoch [6/20], Step[240/483], loss: 3.1920
 Epoch [6/20], Step[250/483], loss: 3.4502
 Epoch [6/20], Step[260/483], loss: 2.3714
 Epoch [6/20], Step[270/483], loss: 3.3063
 Epoch [6/20], Step[280/483], loss: 2.8418
 Epoch [6/20], Step[290/483], loss: 2.4922
 Epoch [6/20], Step[300/483], loss: 3.3571
 Epoch [6/20], Step[310/483], loss: 3.3534
 Epoch [6/20], Step[320/483], loss: 3.4564
 Epoch [6/20], Step[330/483], loss: 3.4747
 Epoch [6/20], Step[340/483], loss: 4.2165
 Epoch [6/20], Step[350/483], loss: 3.2228
 Epoch [6/20], Step[360/483], loss: 2.7393
 Epoch [6/20], Step[370/483], loss: 2.3129
 Epoch [6/20], Step[380/483], loss: 3.1098
 Epoch [6/20], Step[390/483], loss: 3.2814
 Epoch [6/20], Step[400/483], loss: 3.3288
 Epoch [6/20], Step[410/483], loss: 2.6893
 Epoch [6/20], Step[420/483], loss: 2.9242
 Epoch [6/20], Step[430/483], loss: 2.8787
 Epoch [6/20], Step[440/483], loss: 3.3868
 Epoch [6/20], Step[450/483], loss: 2.6703
 Epoch [6/20], Step[460/483], loss: 3.2600
 Epoch [6/20], Step[470/483], loss: 3.5401
 Epoch [6/20], Step[480/483], loss: 2.3354
 ====> Epoch 6: Training loss: 1534.8510
 Epoch [7/20], Step[0/483], loss: 2.9406
 Epoch [7/20], Step[10/483], loss: 3.0635
 Epoch [7/20], Step[20/483], loss: 3.2740
 Epoch [7/20], Step[30/483], loss: 3.1666
 Epoch [7/20], Step[40/483], loss: 3.1855
 Epoch [7/20], Step[50/483], loss: 3.5026
 Epoch [7/20], Step[60/483], loss: 3.3929
 Epoch [7/20], Step[70/483], loss: 3.3560
 Epoch [7/20], Step[80/483], loss: 3.6904
 Epoch [7/20], Step[90/483], loss: 2.8507
 Epoch [7/20], Step[100/483], loss: 2.7294
 Epoch [7/20], Step[110/483], loss: 3.0720
 Epoch [7/20], Step[120/483], loss: 3.4315
 Epoch [7/20], Step[130/483], loss: 3.2528
 Epoch [7/20], Step[140/483], loss: 3.3886
 Epoch [7/20], Step[150/483], loss: 3.4326
 Epoch [7/20], Step[160/483], loss: 3.4445
 Epoch [7/20], Step[170/483], loss: 2.5708
 Epoch [7/20], Step[180/483], loss: 3.3763
 Epoch [7/20], Step[190/483], loss: 3.4085
 Epoch [7/20], Step[200/483], loss: 2.5502
 Epoch [7/20], Step[210/483], loss: 3.0489
 Epoch [7/20], Step[220/483], loss: 2.9521
 Epoch [7/20], Step[230/483], loss: 2.6510
 Epoch [7/20], Step[240/483], loss: 3.1933
 Epoch [7/20], Step[250/483], loss: 3.4452
 Epoch [7/20], Step[260/483], loss: 2.3768
 Epoch [7/20], Step[270/483], loss: 3.3078
 Epoch [7/20], Step[280/483], loss: 2.8576
 Epoch [7/20], Step[290/483], loss: 2.4741
 Epoch [7/20], Step[300/483], loss: 3.3669
 Epoch [7/20], Step[310/483], loss: 3.3355
 Epoch [7/20], Step[320/483], loss: 3.4543
 Epoch [7/20], Step[330/483], loss: 3.2489
 Epoch [7/20], Step[340/483], loss: 3.5509
 Epoch [7/20], Step[350/483], loss: 3.2176
 Epoch [7/20], Step[360/483], loss: 2.6399
 Epoch [7/20], Step[370/483], loss: 2.2929
 Epoch [7/20], Step[380/483], loss: 3.0668
 Epoch [7/20], Step[390/483], loss: 3.2807
 Epoch [7/20], Step[400/483], loss: 3.2921
 Epoch [7/20], Step[410/483], loss: 2.6653
 Epoch [7/20], Step[420/483], loss: 2.9413
 Epoch [7/20], Step[430/483], loss: 2.8691
 Epoch [7/20], Step[440/483], loss: 3.3840
 Epoch [7/20], Step[450/483], loss: 2.5584
 Epoch [7/20], Step[460/483], loss: 3.2515
 Epoch [7/20], Step[470/483], loss: 3.5384
 Epoch [7/20], Step[480/483], loss: 2.3023
 ====> Epoch 7: Training loss: 1530.0679
 Epoch [8/20], Step[0/483], loss: 2.9385
 Epoch [8/20], Step[10/483], loss: 3.0604
 Epoch [8/20], Step[20/483], loss: 3.2665
 Epoch [8/20], Step[30/483], loss: 3.1558
 Epoch [8/20], Step[40/483], loss: 3.1665
 Epoch [8/20], Step[50/483], loss: 3.4828
 Epoch [8/20], Step[60/483], loss: 3.3893
 Epoch [8/20], Step[70/483], loss: 3.3427
 Epoch [8/20], Step[80/483], loss: 3.6493
 Epoch [8/20], Step[90/483], loss: 2.8271
 Epoch [8/20], Step[100/483], loss: 2.7235
 Epoch [8/20], Step[110/483], loss: 3.0663
 Epoch [8/20], Step[120/483], loss: 3.4237
 Epoch [8/20], Step[130/483], loss: 3.2422
 Epoch [8/20], Step[140/483], loss: 3.4001
 Epoch [8/20], Step[150/483], loss: 3.4338
 Epoch [8/20], Step[160/483], loss: 3.4470
 Epoch [8/20], Step[170/483], loss: 2.5920
 Epoch [8/20], Step[180/483], loss: 3.3657
 Epoch [8/20], Step[190/483], loss: 3.4128
 Epoch [8/20], Step[200/483], loss: 2.5501
 Epoch [8/20], Step[210/483], loss: 3.0302
 Epoch [8/20], Step[220/483], loss: 2.9471
 Epoch [8/20], Step[230/483], loss: 2.6550
 Epoch [8/20], Step[240/483], loss: 3.1900
 Epoch [8/20], Step[250/483], loss: 3.4471
 Epoch [8/20], Step[260/483], loss: 2.3672
 Epoch [8/20], Step[270/483], loss: 3.3091
 Epoch [8/20], Step[280/483], loss: 2.8423
 Epoch [8/20], Step[290/483], loss: 2.4729
 Epoch [8/20], Step[300/483], loss: 3.3717
 Epoch [8/20], Step[310/483], loss: 3.3334
 Epoch [8/20], Step[320/483], loss: 3.4444
 Epoch [8/20], Step[330/483], loss: 3.2736
 Epoch [8/20], Step[340/483], loss: 3.5750
 Epoch [8/20], Step[350/483], loss: 3.2110
 Epoch [8/20], Step[360/483], loss: 2.6408
 Epoch [8/20], Step[370/483], loss: 2.3047
 Epoch [8/20], Step[380/483], loss: 3.0563
 Epoch [8/20], Step[390/483], loss: 3.2698
 Epoch [8/20], Step[400/483], loss: 3.2889
 Epoch [8/20], Step[410/483], loss: 2.6702
 Epoch [8/20], Step[420/483], loss: 2.9203
 Epoch [8/20], Step[430/483], loss: 2.8786
 Epoch [8/20], Step[440/483], loss: 3.3639
 Epoch [8/20], Step[450/483], loss: 2.5604
 Epoch [8/20], Step[460/483], loss: 3.2453
 Epoch [8/20], Step[470/483], loss: 3.5385
 Epoch [8/20], Step[480/483], loss: 2.3043
 ====> Epoch 8: Training loss: 1528.8843
 Epoch [9/20], Step[0/483], loss: 2.9418
 Epoch [9/20], Step[10/483], loss: 3.0538
 Epoch [9/20], Step[20/483], loss: 3.2788
 Epoch [9/20], Step[30/483], loss: 3.1561
 Epoch [9/20], Step[40/483], loss: 3.1630
 Epoch [9/20], Step[50/483], loss: 3.4924
 Epoch [9/20], Step[60/483], loss: 3.3823
 Epoch [9/20], Step[70/483], loss: 3.3429
 Epoch [9/20], Step[80/483], loss: 3.6466
 Epoch [9/20], Step[90/483], loss: 2.8336
 Epoch [9/20], Step[100/483], loss: 2.7037
 Epoch [9/20], Step[110/483], loss: 3.0568
 Epoch [9/20], Step[120/483], loss: 3.4422
 Epoch [9/20], Step[130/483], loss: 3.2356
 Epoch [9/20], Step[140/483], loss: 3.3737
 Epoch [9/20], Step[150/483], loss: 3.4224
 Epoch [9/20], Step[160/483], loss: 3.4570
 Epoch [9/20], Step[170/483], loss: 2.5824
 Epoch [9/20], Step[180/483], loss: 3.3676
 Epoch [9/20], Step[190/483], loss: 3.4136
 Epoch [9/20], Step[200/483], loss: 2.5554
 Epoch [9/20], Step[210/483], loss: 3.0358
 Epoch [9/20], Step[220/483], loss: 2.9559
 Epoch [9/20], Step[230/483], loss: 2.6643
 Epoch [9/20], Step[240/483], loss: 3.1893
 Epoch [9/20], Step[250/483], loss: 3.4274
 Epoch [9/20], Step[260/483], loss: 2.3760
 Epoch [9/20], Step[270/483], loss: 3.3064
 Epoch [9/20], Step[280/483], loss: 2.8366
 Epoch [9/20], Step[290/483], loss: 2.4839
 Epoch [9/20], Step[300/483], loss: 3.3645
 Epoch [9/20], Step[310/483], loss: 3.3554
 Epoch [9/20], Step[320/483], loss: 3.4290
 Epoch [9/20], Step[330/483], loss: 3.2521
 Epoch [9/20], Step[340/483], loss: 3.5685
 Epoch [9/20], Step[350/483], loss: 3.2045
 Epoch [9/20], Step[360/483], loss: 2.6330
 Epoch [9/20], Step[370/483], loss: 2.3005
 Epoch [9/20], Step[380/483], loss: 3.0444
 Epoch [9/20], Step[390/483], loss: 3.2714
 Epoch [9/20], Step[400/483], loss: 3.2895
 Epoch [9/20], Step[410/483], loss: 2.6762
 Epoch [9/20], Step[420/483], loss: 2.9698
 Epoch [9/20], Step[430/483], loss: 2.8735
 Epoch [9/20], Step[440/483], loss: 3.3608
 Epoch [9/20], Step[450/483], loss: 2.5526
 Epoch [9/20], Step[460/483], loss: 3.2435
 Epoch [9/20], Step[470/483], loss: 3.5340
 Epoch [9/20], Step[480/483], loss: 2.2892
 ====> Epoch 9: Training loss: 1527.2444
 Epoch [10/20], Step[0/483], loss: 2.9423
 Epoch [10/20], Step[10/483], loss: 3.0469
 Epoch [10/20], Step[20/483], loss: 3.2817
 Epoch [10/20], Step[30/483], loss: 3.1501
 Epoch [10/20], Step[40/483], loss: 3.1465
 Epoch [10/20], Step[50/483], loss: 3.4860
 Epoch [10/20], Step[60/483], loss: 3.3829
 Epoch [10/20], Step[70/483], loss: 3.3319
 Epoch [10/20], Step[80/483], loss: 3.6470
 Epoch [10/20], Step[90/483], loss: 2.8329
 Epoch [10/20], Step[100/483], loss: 2.7100
 Epoch [10/20], Step[110/483], loss: 3.0554
 Epoch [10/20], Step[120/483], loss: 3.4350
 Epoch [10/20], Step[130/483], loss: 3.2332
 Epoch [10/20], Step[140/483], loss: 3.3766
 Epoch [10/20], Step[150/483], loss: 3.4292
 Epoch [10/20], Step[160/483], loss: 3.4416
 Epoch [10/20], Step[170/483], loss: 2.5839
 Epoch [10/20], Step[180/483], loss: 3.3663
 Epoch [10/20], Step[190/483], loss: 3.4109
 Epoch [10/20], Step[200/483], loss: 2.5493
 Epoch [10/20], Step[210/483], loss: 3.0298
 Epoch [10/20], Step[220/483], loss: 2.9533
 Epoch [10/20], Step[230/483], loss: 2.6783
 Epoch [10/20], Step[240/483], loss: 3.1844
 Epoch [10/20], Step[250/483], loss: 3.4253
 Epoch [10/20], Step[260/483], loss: 2.3700
 Epoch [10/20], Step[270/483], loss: 3.3079
 Epoch [10/20], Step[280/483], loss: 2.8399
 Epoch [10/20], Step[290/483], loss: 2.4985
 Epoch [10/20], Step[300/483], loss: 3.3595
 Epoch [10/20], Step[310/483], loss: 3.3625
 Epoch [10/20], Step[320/483], loss: 3.4306
 Epoch [10/20], Step[330/483], loss: 3.2614
 Epoch [10/20], Step[340/483], loss: 3.5649
 Epoch [10/20], Step[350/483], loss: 3.1898
 Epoch [10/20], Step[360/483], loss: 2.6405
 Epoch [10/20], Step[370/483], loss: 2.3028
 Epoch [10/20], Step[380/483], loss: 3.0412
 Epoch [10/20], Step[390/483], loss: 3.2704
 Epoch [10/20], Step[400/483], loss: 3.2855
 Epoch [10/20], Step[410/483], loss: 2.6684
 Epoch [10/20], Step[420/483], loss: 2.9220
 Epoch [10/20], Step[430/483], loss: 2.8704
 Epoch [10/20], Step[440/483], loss: 3.3648
 Epoch [10/20], Step[450/483], loss: 2.5803
 Epoch [10/20], Step[460/483], loss: 3.2417
 Epoch [10/20], Step[470/483], loss: 3.5362
 Epoch [10/20], Step[480/483], loss: 2.3032
 ====> Epoch 10: Training loss: 1526.5728
 Epoch [11/20], Step[0/483], loss: 2.9399
 Epoch [11/20], Step[10/483], loss: 3.0569
 Epoch [11/20], Step[20/483], loss: 3.2884
 Epoch [11/20], Step[30/483], loss: 3.1522
 Epoch [11/20], Step[40/483], loss: 3.1740
 Epoch [11/20], Step[50/483], loss: 3.5197
 Epoch [11/20], Step[60/483], loss: 3.3772
 Epoch [11/20], Step[70/483], loss: 3.3490
 Epoch [11/20], Step[80/483], loss: 3.6566
 Epoch [11/20], Step[90/483], loss: 2.8493
 Epoch [11/20], Step[100/483], loss: 2.7153
 Epoch [11/20], Step[110/483], loss: 3.0513
 Epoch [11/20], Step[120/483], loss: 3.4274
 Epoch [11/20], Step[130/483], loss: 3.2396
 Epoch [11/20], Step[140/483], loss: 3.3838
 Epoch [11/20], Step[150/483], loss: 3.4212
 Epoch [11/20], Step[160/483], loss: 3.4473
 Epoch [11/20], Step[170/483], loss: 2.5756
 Epoch [11/20], Step[180/483], loss: 3.3641
 Epoch [11/20], Step[190/483], loss: 3.4052
 Epoch [11/20], Step[200/483], loss: 2.5402
 Epoch [11/20], Step[210/483], loss: 3.0320
 Epoch [11/20], Step[220/483], loss: 2.9515
 Epoch [11/20], Step[230/483], loss: 2.6640
 Epoch [11/20], Step[240/483], loss: 3.1859
 Epoch [11/20], Step[250/483], loss: 3.4422
 Epoch [11/20], Step[260/483], loss: 2.3640
 Epoch [11/20], Step[270/483], loss: 3.3104
 Epoch [11/20], Step[280/483], loss: 2.8440
 Epoch [11/20], Step[290/483], loss: 2.4761
 Epoch [11/20], Step[300/483], loss: 3.3684
 Epoch [11/20], Step[310/483], loss: 3.3384
 Epoch [11/20], Step[320/483], loss: 3.4287
 Epoch [11/20], Step[330/483], loss: 3.2533
 Epoch [11/20], Step[340/483], loss: 3.5594
 Epoch [11/20], Step[350/483], loss: 3.1748
 Epoch [11/20], Step[360/483], loss: 2.6405
 Epoch [11/20], Step[370/483], loss: 2.2866
 Epoch [11/20], Step[380/483], loss: 3.0387
 Epoch [11/20], Step[390/483], loss: 3.2687
 Epoch [11/20], Step[400/483], loss: 3.2879
 Epoch [11/20], Step[410/483], loss: 2.6788
 Epoch [11/20], Step[420/483], loss: 2.9254
 Epoch [11/20], Step[430/483], loss: 2.8884
 Epoch [11/20], Step[440/483], loss: 3.3609
 Epoch [11/20], Step[450/483], loss: 2.5521
 Epoch [11/20], Step[460/483], loss: 3.2501
 Epoch [11/20], Step[470/483], loss: 3.5301
 Epoch [11/20], Step[480/483], loss: 2.2837
 ====> Epoch 11: Training loss: 1526.3366
 Epoch [12/20], Step[0/483], loss: 2.9380
 Epoch [12/20], Step[10/483], loss: 3.0569
 Epoch [12/20], Step[20/483], loss: 3.2786
 Epoch [12/20], Step[30/483], loss: 3.1528
 Epoch [12/20], Step[40/483], loss: 3.1563
 Epoch [12/20], Step[50/483], loss: 3.4833
 Epoch [12/20], Step[60/483], loss: 3.3778
 Epoch [12/20], Step[70/483], loss: 3.3377
 Epoch [12/20], Step[80/483], loss: 3.6448
 Epoch [12/20], Step[90/483], loss: 2.8493
 Epoch [12/20], Step[100/483], loss: 2.7203
 Epoch [12/20], Step[110/483], loss: 3.0487
 Epoch [12/20], Step[120/483], loss: 3.4271
 Epoch [12/20], Step[130/483], loss: 3.2298
 Epoch [12/20], Step[140/483], loss: 3.3749
 Epoch [12/20], Step[150/483], loss: 3.4249
 Epoch [12/20], Step[160/483], loss: 3.4399
 Epoch [12/20], Step[170/483], loss: 2.5793
 Epoch [12/20], Step[180/483], loss: 3.3652
 Epoch [12/20], Step[190/483], loss: 3.4059
 Epoch [12/20], Step[200/483], loss: 2.5482
 Epoch [12/20], Step[210/483], loss: 3.0411
 Epoch [12/20], Step[220/483], loss: 2.9473
 Epoch [12/20], Step[230/483], loss: 2.6762
 Epoch [12/20], Step[240/483], loss: 3.1825
 Epoch [12/20], Step[250/483], loss: 3.4397
 Epoch [12/20], Step[260/483], loss: 2.3670
 Epoch [12/20], Step[270/483], loss: 3.3119
 Epoch [12/20], Step[280/483], loss: 2.8374
 Epoch [12/20], Step[290/483], loss: 2.4976
 Epoch [12/20], Step[300/483], loss: 3.3501
 Epoch [12/20], Step[310/483], loss: 3.3650
 Epoch [12/20], Step[320/483], loss: 3.4353
 Epoch [12/20], Step[330/483], loss: 3.2490
 Epoch [12/20], Step[340/483], loss: 3.5615
 Epoch [12/20], Step[350/483], loss: 3.1848
 Epoch [12/20], Step[360/483], loss: 2.6391
 Epoch [12/20], Step[370/483], loss: 2.3032
 Epoch [12/20], Step[380/483], loss: 3.0436
 Epoch [12/20], Step[390/483], loss: 3.2672
 Epoch [12/20], Step[400/483], loss: 3.2876
 Epoch [12/20], Step[410/483], loss: 2.6785
 Epoch [12/20], Step[420/483], loss: 2.9275
 Epoch [12/20], Step[430/483], loss: 2.8823
 Epoch [12/20], Step[440/483], loss: 3.3669
 Epoch [12/20], Step[450/483], loss: 2.5653
 Epoch [12/20], Step[460/483], loss: 3.2522
 Epoch [12/20], Step[470/483], loss: 3.5341
 Epoch [12/20], Step[480/483], loss: 2.3010
 ====> Epoch 12: Training loss: 1526.6878
 Epoch [13/20], Step[0/483], loss: 2.9373
 Epoch [13/20], Step[10/483], loss: 3.0584
 Epoch [13/20], Step[20/483], loss: 3.2824
 Epoch [13/20], Step[30/483], loss: 3.1562
 Epoch [13/20], Step[40/483], loss: 3.1816
 Epoch [13/20], Step[50/483], loss: 3.4850
 Epoch [13/20], Step[60/483], loss: 3.3835
 Epoch [13/20], Step[70/483], loss: 3.3496
 Epoch [13/20], Step[80/483], loss: 3.6517
 Epoch [13/20], Step[90/483], loss: 2.8475
 Epoch [13/20], Step[100/483], loss: 2.7176
 Epoch [13/20], Step[110/483], loss: 3.0477
 Epoch [13/20], Step[120/483], loss: 3.4277
 Epoch [13/20], Step[130/483], loss: 3.2345
 Epoch [13/20], Step[140/483], loss: 3.3795
 Epoch [13/20], Step[150/483], loss: 3.4258
 Epoch [13/20], Step[160/483], loss: 3.4465
 Epoch [13/20], Step[170/483], loss: 2.5758
 Epoch [13/20], Step[180/483], loss: 3.3666
 Epoch [13/20], Step[190/483], loss: 3.4073
 Epoch [13/20], Step[200/483], loss: 2.5466
 Epoch [13/20], Step[210/483], loss: 3.0371
 Epoch [13/20], Step[220/483], loss: 2.9514
 Epoch [13/20], Step[230/483], loss: 2.6724
 Epoch [13/20], Step[240/483], loss: 3.1843
 Epoch [13/20], Step[250/483], loss: 3.4437
 Epoch [13/20], Step[260/483], loss: 2.3743
 Epoch [13/20], Step[270/483], loss: 3.3035
 Epoch [13/20], Step[280/483], loss: 2.8510
 Epoch [13/20], Step[290/483], loss: 2.4906
 Epoch [13/20], Step[300/483], loss: 3.3635
 Epoch [13/20], Step[310/483], loss: 3.3534
 Epoch [13/20], Step[320/483], loss: 3.4390
 Epoch [13/20], Step[330/483], loss: 3.2508
 Epoch [13/20], Step[340/483], loss: 3.5629
 Epoch [13/20], Step[350/483], loss: 3.2206
 Epoch [13/20], Step[360/483], loss: 2.6469
 Epoch [13/20], Step[370/483], loss: 2.2863
 Epoch [13/20], Step[380/483], loss: 3.0428
 Epoch [13/20], Step[390/483], loss: 3.2623
 Epoch [13/20], Step[400/483], loss: 3.2819
 Epoch [13/20], Step[410/483], loss: 2.6692
 Epoch [13/20], Step[420/483], loss: 2.9499
 Epoch [13/20], Step[430/483], loss: 2.8769
 Epoch [13/20], Step[440/483], loss: 3.3639
 Epoch [13/20], Step[450/483], loss: 2.5470
 Epoch [13/20], Step[460/483], loss: 3.2460
 Epoch [13/20], Step[470/483], loss: 3.5298
 Epoch [13/20], Step[480/483], loss: 2.3003
 ====> Epoch 13: Training loss: 1526.2818
 Epoch [14/20], Step[0/483], loss: 2.9418
 Epoch [14/20], Step[10/483], loss: 3.0549
 Epoch [14/20], Step[20/483], loss: 3.2757
 Epoch [14/20], Step[30/483], loss: 3.1490
 Epoch [14/20], Step[40/483], loss: 3.1516
 Epoch [14/20], Step[50/483], loss: 3.4803
 Epoch [14/20], Step[60/483], loss: 3.3786
 Epoch [14/20], Step[70/483], loss: 3.3318
 Epoch [14/20], Step[80/483], loss: 3.6450
 Epoch [14/20], Step[90/483], loss: 2.8444
 Epoch [14/20], Step[100/483], loss: 2.7163
 Epoch [14/20], Step[110/483], loss: 3.0495
 Epoch [14/20], Step[120/483], loss: 3.4347
 Epoch [14/20], Step[130/483], loss: 3.2348
 Epoch [14/20], Step[140/483], loss: 3.3838
 Epoch [14/20], Step[150/483], loss: 3.4284
 Epoch [14/20], Step[160/483], loss: 3.4458
 Epoch [14/20], Step[170/483], loss: 2.5743
 Epoch [14/20], Step[180/483], loss: 3.3673
 Epoch [14/20], Step[190/483], loss: 3.4009
 Epoch [14/20], Step[200/483], loss: 2.5506
 Epoch [14/20], Step[210/483], loss: 3.0302
 Epoch [14/20], Step[220/483], loss: 2.9548
 Epoch [14/20], Step[230/483], loss: 2.6877
 Epoch [14/20], Step[240/483], loss: 3.1881
 Epoch [14/20], Step[250/483], loss: 3.4347
 Epoch [14/20], Step[260/483], loss: 2.3632
 Epoch [14/20], Step[270/483], loss: 3.3017
 Epoch [14/20], Step[280/483], loss: 2.8436
 Epoch [14/20], Step[290/483], loss: 2.4904
 Epoch [14/20], Step[300/483], loss: 3.3469
 Epoch [14/20], Step[310/483], loss: 3.3511
 Epoch [14/20], Step[320/483], loss: 3.4199
 Epoch [14/20], Step[330/483], loss: 3.2508
 Epoch [14/20], Step[340/483], loss: 3.5528
 Epoch [14/20], Step[350/483], loss: 3.2230
 Epoch [14/20], Step[360/483], loss: 2.6486
 Epoch [14/20], Step[370/483], loss: 2.2738
 Epoch [14/20], Step[380/483], loss: 3.0517
 Epoch [14/20], Step[390/483], loss: 3.2679
 Epoch [14/20], Step[400/483], loss: 3.2850
 Epoch [14/20], Step[410/483], loss: 2.6721
 Epoch [14/20], Step[420/483], loss: 2.9413
 Epoch [14/20], Step[430/483], loss: 2.8687
 Epoch [14/20], Step[440/483], loss: 3.3616
 Epoch [14/20], Step[450/483], loss: 2.5489
 Epoch [14/20], Step[460/483], loss: 3.2445
 Epoch [14/20], Step[470/483], loss: 3.5374
 Epoch [14/20], Step[480/483], loss: 2.3077
 ====> Epoch 14: Training loss: 1525.6813
 Epoch [15/20], Step[0/483], loss: 2.9395
 Epoch [15/20], Step[10/483], loss: 3.0506
 Epoch [15/20], Step[20/483], loss: 3.2673
 Epoch [15/20], Step[30/483], loss: 3.1474
 Epoch [15/20], Step[40/483], loss: 3.1505
 Epoch [15/20], Step[50/483], loss: 3.4806
 Epoch [15/20], Step[60/483], loss: 3.3863
 Epoch [15/20], Step[70/483], loss: 3.3282
 Epoch [15/20], Step[80/483], loss: 3.6457
 Epoch [15/20], Step[90/483], loss: 2.8499
 Epoch [15/20], Step[100/483], loss: 2.7139
 Epoch [15/20], Step[110/483], loss: 3.0488
 Epoch [15/20], Step[120/483], loss: 3.4249
 Epoch [15/20], Step[130/483], loss: 3.2291
 Epoch [15/20], Step[140/483], loss: 3.3957
 Epoch [15/20], Step[150/483], loss: 3.4354
 Epoch [15/20], Step[160/483], loss: 3.4431
 Epoch [15/20], Step[170/483], loss: 2.5741
 Epoch [15/20], Step[180/483], loss: 3.3624
 Epoch [15/20], Step[190/483], loss: 3.4074
 Epoch [15/20], Step[200/483], loss: 2.5532
 Epoch [15/20], Step[210/483], loss: 3.0254
 Epoch [15/20], Step[220/483], loss: 2.9471
 Epoch [15/20], Step[230/483], loss: 2.6795
 Epoch [15/20], Step[240/483], loss: 3.1852
 Epoch [15/20], Step[250/483], loss: 3.4301
 Epoch [15/20], Step[260/483], loss: 2.3629
 Epoch [15/20], Step[270/483], loss: 3.3082
 Epoch [15/20], Step[280/483], loss: 2.8424
 Epoch [15/20], Step[290/483], loss: 2.4903
 Epoch [15/20], Step[300/483], loss: 3.3386
 Epoch [15/20], Step[310/483], loss: 3.3461
 Epoch [15/20], Step[320/483], loss: 3.4265
 Epoch [15/20], Step[330/483], loss: 3.2542
 Epoch [15/20], Step[340/483], loss: 3.5527
 Epoch [15/20], Step[350/483], loss: 3.2012
 Epoch [15/20], Step[360/483], loss: 2.6521
 Epoch [15/20], Step[370/483], loss: 2.2714
 Epoch [15/20], Step[380/483], loss: 3.0453
 Epoch [15/20], Step[390/483], loss: 3.2644
 Epoch [15/20], Step[400/483], loss: 3.2852
 Epoch [15/20], Step[410/483], loss: 2.6721
 Epoch [15/20], Step[420/483], loss: 2.9452
 Epoch [15/20], Step[430/483], loss: 2.8675
 Epoch [15/20], Step[440/483], loss: 3.3682
 Epoch [15/20], Step[450/483], loss: 2.5472
 Epoch [15/20], Step[460/483], loss: 3.2452
 Epoch [15/20], Step[470/483], loss: 3.5362
 Epoch [15/20], Step[480/483], loss: 2.3080
 ====> Epoch 15: Training loss: 1525.1189
 Epoch [16/20], Step[0/483], loss: 2.9432
 Epoch [16/20], Step[10/483], loss: 3.0554
 Epoch [16/20], Step[20/483], loss: 3.2830
 Epoch [16/20], Step[30/483], loss: 3.1456
 Epoch [16/20], Step[40/483], loss: 3.1491
 Epoch [16/20], Step[50/483], loss: 3.4801
 Epoch [16/20], Step[60/483], loss: 3.3834
 Epoch [16/20], Step[70/483], loss: 3.3282
 Epoch [16/20], Step[80/483], loss: 3.6486
 Epoch [16/20], Step[90/483], loss: 2.8450
 Epoch [16/20], Step[100/483], loss: 2.7129
 Epoch [16/20], Step[110/483], loss: 3.0529
 Epoch [16/20], Step[120/483], loss: 3.4293
 Epoch [16/20], Step[130/483], loss: 3.2366
 Epoch [16/20], Step[140/483], loss: 3.3825
 Epoch [16/20], Step[150/483], loss: 3.4320
 Epoch [16/20], Step[160/483], loss: 3.4429
 Epoch [16/20], Step[170/483], loss: 2.5757
 Epoch [16/20], Step[180/483], loss: 3.3623
 Epoch [16/20], Step[190/483], loss: 3.4076
 Epoch [16/20], Step[200/483], loss: 2.5583
 Epoch [16/20], Step[210/483], loss: 3.0185
 Epoch [16/20], Step[220/483], loss: 2.9432
 Epoch [16/20], Step[230/483], loss: 2.6705
 Epoch [16/20], Step[240/483], loss: 3.1852
 Epoch [16/20], Step[250/483], loss: 3.4371
 Epoch [16/20], Step[260/483], loss: 2.3619
 Epoch [16/20], Step[270/483], loss: 3.2998
 Epoch [16/20], Step[280/483], loss: 2.8450
 Epoch [16/20], Step[290/483], loss: 2.4993
 Epoch [16/20], Step[300/483], loss: 3.3400
 Epoch [16/20], Step[310/483], loss: 3.3508
 Epoch [16/20], Step[320/483], loss: 3.4202
 Epoch [16/20], Step[330/483], loss: 3.2486
 Epoch [16/20], Step[340/483], loss: 3.5500
 Epoch [16/20], Step[350/483], loss: 3.2189
 Epoch [16/20], Step[360/483], loss: 2.6481
 Epoch [16/20], Step[370/483], loss: 2.2726
 Epoch [16/20], Step[380/483], loss: 3.0407
 Epoch [16/20], Step[390/483], loss: 3.2695
 Epoch [16/20], Step[400/483], loss: 3.2818
 Epoch [16/20], Step[410/483], loss: 2.6689
 Epoch [16/20], Step[420/483], loss: 2.9435
 Epoch [16/20], Step[430/483], loss: 2.8687
 Epoch [16/20], Step[440/483], loss: 3.3611
 Epoch [16/20], Step[450/483], loss: 2.5503
 Epoch [16/20], Step[460/483], loss: 3.2416
 Epoch [16/20], Step[470/483], loss: 3.5303
 Epoch [16/20], Step[480/483], loss: 2.3157
 ====> Epoch 16: Training loss: 1525.0596
 Epoch [17/20], Step[0/483], loss: 2.9389
 Epoch [17/20], Step[10/483], loss: 3.0544
 Epoch [17/20], Step[20/483], loss: 3.2820
 Epoch [17/20], Step[30/483], loss: 3.1464
 Epoch [17/20], Step[40/483], loss: 3.1468
 Epoch [17/20], Step[50/483], loss: 3.4767
 Epoch [17/20], Step[60/483], loss: 3.3852
 Epoch [17/20], Step[70/483], loss: 3.3295
 Epoch [17/20], Step[80/483], loss: 3.6470
 Epoch [17/20], Step[90/483], loss: 2.8461
 Epoch [17/20], Step[100/483], loss: 2.7177
 Epoch [17/20], Step[110/483], loss: 3.0487
 Epoch [17/20], Step[120/483], loss: 3.4305
 Epoch [17/20], Step[130/483], loss: 3.2280
 Epoch [17/20], Step[140/483], loss: 3.3791
 Epoch [17/20], Step[150/483], loss: 3.4298
 Epoch [17/20], Step[160/483], loss: 3.4380
 Epoch [17/20], Step[170/483], loss: 2.5788
 Epoch [17/20], Step[180/483], loss: 3.3619
 Epoch [17/20], Step[190/483], loss: 3.4060
 Epoch [17/20], Step[200/483], loss: 2.5563
 Epoch [17/20], Step[210/483], loss: 3.0181
 Epoch [17/20], Step[220/483], loss: 2.9443
 Epoch [17/20], Step[230/483], loss: 2.6660
 Epoch [17/20], Step[240/483], loss: 3.1851
 Epoch [17/20], Step[250/483], loss: 3.4300
 Epoch [17/20], Step[260/483], loss: 2.3645
 Epoch [17/20], Step[270/483], loss: 3.3001
 Epoch [17/20], Step[280/483], loss: 2.8469
 Epoch [17/20], Step[290/483], loss: 2.4947
 Epoch [17/20], Step[300/483], loss: 3.3296
 Epoch [17/20], Step[310/483], loss: 3.3461
 Epoch [17/20], Step[320/483], loss: 3.4170
 Epoch [17/20], Step[330/483], loss: 3.2435
 Epoch [17/20], Step[340/483], loss: 3.5508
 Epoch [17/20], Step[350/483], loss: 3.2161
 Epoch [17/20], Step[360/483], loss: 2.6450
 Epoch [17/20], Step[370/483], loss: 2.2775
 Epoch [17/20], Step[380/483], loss: 3.0419
 Epoch [17/20], Step[390/483], loss: 3.2700
 Epoch [17/20], Step[400/483], loss: 3.2825
 Epoch [17/20], Step[410/483], loss: 2.6693
 Epoch [17/20], Step[420/483], loss: 2.9449
 Epoch [17/20], Step[430/483], loss: 2.8660
 Epoch [17/20], Step[440/483], loss: 3.3645
 Epoch [17/20], Step[450/483], loss: 2.5488
 Epoch [17/20], Step[460/483], loss: 3.2438
 Epoch [17/20], Step[470/483], loss: 3.5389
 Epoch [17/20], Step[480/483], loss: 2.3157
 ====> Epoch 17: Training loss: 1524.6320
 Epoch [18/20], Step[0/483], loss: 2.9435
 Epoch [18/20], Step[10/483], loss: 3.0554
 Epoch [18/20], Step[20/483], loss: 3.2860
 Epoch [18/20], Step[30/483], loss: 3.1472
 Epoch [18/20], Step[40/483], loss: 3.1479
 Epoch [18/20], Step[50/483], loss: 3.4756
 Epoch [18/20], Step[60/483], loss: 3.3896
 Epoch [18/20], Step[70/483], loss: 3.3287
 Epoch [18/20], Step[80/483], loss: 3.6457
 Epoch [18/20], Step[90/483], loss: 2.8480
 Epoch [18/20], Step[100/483], loss: 2.7121
 Epoch [18/20], Step[110/483], loss: 3.0479
 Epoch [18/20], Step[120/483], loss: 3.4287
 Epoch [18/20], Step[130/483], loss: 3.2304
 Epoch [18/20], Step[140/483], loss: 3.3810
 Epoch [18/20], Step[150/483], loss: 3.4274
 Epoch [18/20], Step[160/483], loss: 3.4387
 Epoch [18/20], Step[170/483], loss: 2.5759
 Epoch [18/20], Step[180/483], loss: 3.3628
 Epoch [18/20], Step[190/483], loss: 3.4034
 Epoch [18/20], Step[200/483], loss: 2.5525
 Epoch [18/20], Step[210/483], loss: 3.0190
 Epoch [18/20], Step[220/483], loss: 2.9477
 Epoch [18/20], Step[230/483], loss: 2.6665
 Epoch [18/20], Step[240/483], loss: 3.1834
 Epoch [18/20], Step[250/483], loss: 3.4311
 Epoch [18/20], Step[260/483], loss: 2.3682
 Epoch [18/20], Step[270/483], loss: 3.2971
 Epoch [18/20], Step[280/483], loss: 2.8466
 Epoch [18/20], Step[290/483], loss: 2.4988
 Epoch [18/20], Step[300/483], loss: 3.3304
 Epoch [18/20], Step[310/483], loss: 3.3437
 Epoch [18/20], Step[320/483], loss: 3.4209
 Epoch [18/20], Step[330/483], loss: 3.2489
 Epoch [18/20], Step[340/483], loss: 3.5416
 Epoch [18/20], Step[350/483], loss: 3.2130
 Epoch [18/20], Step[360/483], loss: 2.6511
 Epoch [18/20], Step[370/483], loss: 2.2677
 Epoch [18/20], Step[380/483], loss: 3.0395
 Epoch [18/20], Step[390/483], loss: 3.2721
 Epoch [18/20], Step[400/483], loss: 3.2829
 Epoch [18/20], Step[410/483], loss: 2.6704
 Epoch [18/20], Step[420/483], loss: 2.9282
 Epoch [18/20], Step[430/483], loss: 2.8675
 Epoch [18/20], Step[440/483], loss: 3.3631
 Epoch [18/20], Step[450/483], loss: 2.5537
 Epoch [18/20], Step[460/483], loss: 3.2452
 Epoch [18/20], Step[470/483], loss: 3.5304
 Epoch [18/20], Step[480/483], loss: 2.3058
 ====> Epoch 18: Training loss: 1524.6818
 Epoch [19/20], Step[0/483], loss: 2.9427
 Epoch [19/20], Step[10/483], loss: 3.0545
 Epoch [19/20], Step[20/483], loss: 3.2838
 Epoch [19/20], Step[30/483], loss: 3.1506
 Epoch [19/20], Step[40/483], loss: 3.1461
 Epoch [19/20], Step[50/483], loss: 3.4847
 Epoch [19/20], Step[60/483], loss: 3.3864
 Epoch [19/20], Step[70/483], loss: 3.3279
 Epoch [19/20], Step[80/483], loss: 3.6436
 Epoch [19/20], Step[90/483], loss: 2.8467
 Epoch [19/20], Step[100/483], loss: 2.7137
 Epoch [19/20], Step[110/483], loss: 3.0502
 Epoch [19/20], Step[120/483], loss: 3.4421
 Epoch [19/20], Step[130/483], loss: 3.2267
 Epoch [19/20], Step[140/483], loss: 3.3827
 Epoch [19/20], Step[150/483], loss: 3.4276
 Epoch [19/20], Step[160/483], loss: 3.4414
 Epoch [19/20], Step[170/483], loss: 2.5791
 Epoch [19/20], Step[180/483], loss: 3.3658
 Epoch [19/20], Step[190/483], loss: 3.4036
 Epoch [19/20], Step[200/483], loss: 2.5478
 Epoch [19/20], Step[210/483], loss: 3.0224
 Epoch [19/20], Step[220/483], loss: 2.9482
 Epoch [19/20], Step[230/483], loss: 2.6695
 Epoch [19/20], Step[240/483], loss: 3.1833
 Epoch [19/20], Step[250/483], loss: 3.4337
 Epoch [19/20], Step[260/483], loss: 2.3681
 Epoch [19/20], Step[270/483], loss: 3.2992
 Epoch [19/20], Step[280/483], loss: 2.8448
 Epoch [19/20], Step[290/483], loss: 2.4973
 Epoch [19/20], Step[300/483], loss: 3.3175
 Epoch [19/20], Step[310/483], loss: 3.3423
 Epoch [19/20], Step[320/483], loss: 3.4204
 Epoch [19/20], Step[330/483], loss: 3.2440
 Epoch [19/20], Step[340/483], loss: 3.5470
 Epoch [19/20], Step[350/483], loss: 3.2145
 Epoch [19/20], Step[360/483], loss: 2.6412
 Epoch [19/20], Step[370/483], loss: 2.2703
 Epoch [19/20], Step[380/483], loss: 3.0367
 Epoch [19/20], Step[390/483], loss: 3.2707
 Epoch [19/20], Step[400/483], loss: 3.2842
 Epoch [19/20], Step[410/483], loss: 2.6718
 Epoch [19/20], Step[420/483], loss: 2.9476
 Epoch [19/20], Step[430/483], loss: 2.8666
 Epoch [19/20], Step[440/483], loss: 3.3631
 Epoch [19/20], Step[450/483], loss: 2.5506
 Epoch [19/20], Step[460/483], loss: 3.2430
 Epoch [19/20], Step[470/483], loss: 3.5357
 Epoch [19/20], Step[480/483], loss: 2.3223
 ====> Epoch 19: Training loss: 1524.9416
 Epoch [20/20], Step[0/483], loss: 2.9379
 Epoch [20/20], Step[10/483], loss: 3.0562
 Epoch [20/20], Step[20/483], loss: 3.2797
 Epoch [20/20], Step[30/483], loss: 3.1500
 Epoch [20/20], Step[40/483], loss: 3.1499
 Epoch [20/20], Step[50/483], loss: 3.4762
 Epoch [20/20], Step[60/483], loss: 3.3857
 Epoch [20/20], Step[70/483], loss: 3.3276
 Epoch [20/20], Step[80/483], loss: 3.6518
 Epoch [20/20], Step[90/483], loss: 2.8534
 Epoch [20/20], Step[100/483], loss: 2.7070
 Epoch [20/20], Step[110/483], loss: 3.0529
 Epoch [20/20], Step[120/483], loss: 3.4363
 Epoch [20/20], Step[130/483], loss: 3.2355
 Epoch [20/20], Step[140/483], loss: 3.3830
 Epoch [20/20], Step[150/483], loss: 3.4399
 Epoch [20/20], Step[160/483], loss: 3.4377
 Epoch [20/20], Step[170/483], loss: 2.5716
 Epoch [20/20], Step[180/483], loss: 3.3661
 Epoch [20/20], Step[190/483], loss: 3.4017
 Epoch [20/20], Step[200/483], loss: 2.5608
 Epoch [20/20], Step[210/483], loss: 3.0175
 Epoch [20/20], Step[220/483], loss: 2.9422
 Epoch [20/20], Step[230/483], loss: 2.6643
 Epoch [20/20], Step[240/483], loss: 3.1844
 Epoch [20/20], Step[250/483], loss: 3.4315
 Epoch [20/20], Step[260/483], loss: 2.3777
 Epoch [20/20], Step[270/483], loss: 3.2972
 Epoch [20/20], Step[280/483], loss: 2.8496
 Epoch [20/20], Step[290/483], loss: 2.4951
 Epoch [20/20], Step[300/483], loss: 3.3226
 Epoch [20/20], Step[310/483], loss: 3.3419
 Epoch [20/20], Step[320/483], loss: 3.4342
 Epoch [20/20], Step[330/483], loss: 3.2434
 Epoch [20/20], Step[340/483], loss: 3.5465
 Epoch [20/20], Step[350/483], loss: 3.2154
 Epoch [20/20], Step[360/483], loss: 2.6407
 Epoch [20/20], Step[370/483], loss: 2.2956
 Epoch [20/20], Step[380/483], loss: 3.0387
 Epoch [20/20], Step[390/483], loss: 3.2735
 Epoch [20/20], Step[400/483], loss: 3.2879
 Epoch [20/20], Step[410/483], loss: 2.6698
 Epoch [20/20], Step[420/483], loss: 2.9343
 Epoch [20/20], Step[430/483], loss: 2.8646
 Epoch [20/20], Step[440/483], loss: 3.3625
 Epoch [20/20], Step[450/483], loss: 2.5511
 Epoch [20/20], Step[460/483], loss: 3.2422
 Epoch [20/20], Step[470/483], loss: 3.5399
 Epoch [20/20], Step[480/483], loss: 2.3243
 ====> Epoch 20: Training loss: 1524.5685
