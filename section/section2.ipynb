{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: RNNs in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "1. Build a simple RNN classifier\n",
    "2. Learn about PyTorch's in-built RNN modules (LSTM etc.)\n",
    "\n",
    "(Roughly follows http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Building an RNN sentiment classifier\n",
    "#### Part 1.1: Generating the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll generate some toy data. The task will be to recall an integer at a certain position in a sequence. \n",
    "For a sequence a<sub>1</sub> a<sub>12</sub> a<sub>3</sub> a<sub>4</sub> a<sub>5</sub> the output might be a<sub>3</sub>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of training examples\n",
    "n_train = 2000\n",
    "\n",
    "# number of validation examples\n",
    "n_val = 1000\n",
    "\n",
    "# length of each sequence\n",
    "n_length = 10\n",
    "\n",
    "# examples per batch\n",
    "n_batch = 32\n",
    "\n",
    "# size of the vocabulary\n",
    "n_vocab = 20\n",
    "\n",
    "# position to be recalled\n",
    "answer_pos = n_length-1\n",
    "\n",
    "# generate random sequences\n",
    "train_seq = Variable(torch.Tensor(n_train, n_length).random_(0, n_vocab).long())\n",
    "val_seq = Variable(torch.Tensor(n_val, n_length).random_(0, n_vocab).long())\n",
    "\n",
    "# choose the correct labels\n",
    "train_labels = train_seq.clone()[:, answer_pos]\n",
    "val_labels = val_seq.clone()[:, answer_pos]\n",
    "\n",
    "# group data into batches\n",
    "train_iter = []\n",
    "for i in range(0, n_train, n_batch):\n",
    "    batch_seq = train_seq[i:i+n_batch]\n",
    "    batch_labels = train_labels[i:i+n_batch]\n",
    "    if (batch_seq.size()[0] == n_batch):\n",
    "        train_iter.append([batch_seq, batch_labels])\n",
    "    \n",
    "val_iter = []\n",
    "for i in range(0, n_val, n_batch):\n",
    "    batch_seq = val_seq[i:i+n_batch]\n",
    "    batch_labels = val_labels[i:i+n_batch]\n",
    "    if (batch_seq.size()[0] == n_batch):\n",
    "        val_iter.append([batch_seq, batch_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2 Build the model (version 1)\n",
    "\n",
    "The RNN module will be a PyTorch model like any other, with init a forward functions. This network:\n",
    "1. Takes as input the word at a particular point in the sequence, as well as the hidden state at the previous state of the network\n",
    "2. Uses nn.Embedding to get a vector for the word\n",
    "3. Concatenate the embedding and the hidden state\n",
    "4. Apply a linear layer to get the next hidden state\n",
    "5. Apply a linear layer to get the output\n",
    "6. Output both "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, vocab_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, input_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        combined = torch.cat((embedded, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.3: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize and train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_batch(model, criterion, optim, batch, label):\n",
    "    # initialize hidden vector\n",
    "    hidden = Variable(torch.zeros(n_batch, n_hidden))\n",
    "\n",
    "    # clear gradients\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    # calculate forward pass\n",
    "    for i in range(batch.size()[1]):\n",
    "        output, hidden = model(batch[:, i], hidden)\n",
    "\n",
    "    # calculate loss    \n",
    "    loss = criterion(output, label)\n",
    "\n",
    "    # backpropagate and step\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, criterion, optim):\n",
    "    for e in range(n_epochs):\n",
    "        batches = 0\n",
    "        epoch_loss = 0\n",
    "        avg_loss = 0\n",
    "        for batch, label in train_iter:\n",
    "            batch_loss = train_batch(model, criterion, optim, batch, label)\n",
    "            batches += 1\n",
    "            epoch_loss += batch_loss\n",
    "            avg_loss = ((avg_loss * (batches - 1)) + batch_loss) / batches\n",
    "        \n",
    "        print(\"Epoch \", e, \" Loss: \", epoch_loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch ', 0, ' Loss: ', 130.83899009227753)\n",
      "('Epoch ', 1, ' Loss: ', 50.6698834002018)\n",
      "('Epoch ', 2, ' Loss: ', 25.83748033642769)\n",
      "('Epoch ', 3, ' Loss: ', 16.157479643821716)\n",
      "('Epoch ', 4, ' Loss: ', 11.279053911566734)\n",
      "('Epoch ', 5, ' Loss: ', 8.458589285612106)\n",
      "('Epoch ', 6, ' Loss: ', 6.6770948469638824)\n",
      "('Epoch ', 7, ' Loss: ', 5.472653165459633)\n",
      "('Epoch ', 8, ' Loss: ', 4.613367971032858)\n",
      "('Epoch ', 9, ' Loss: ', 3.9737212359905243)\n",
      "('Epoch ', 10, ' Loss: ', 3.4811954759061337)\n",
      "('Epoch ', 11, ' Loss: ', 3.091461520642042)\n",
      "('Epoch ', 12, ' Loss: ', 2.7760953195393085)\n",
      "('Epoch ', 13, ' Loss: ', 2.5161177664995193)\n",
      "('Epoch ', 14, ' Loss: ', 2.2984154634177685)\n",
      "('Epoch ', 15, ' Loss: ', 2.1136592347174883)\n",
      "('Epoch ', 16, ' Loss: ', 1.9550437536090612)\n",
      "('Epoch ', 17, ' Loss: ', 1.817494686692953)\n",
      "('Epoch ', 18, ' Loss: ', 1.6971564795821905)\n",
      "('Epoch ', 19, ' Loss: ', 1.5910500511527061)\n",
      "('Epoch ', 20, ' Loss: ', 1.4968388937413692)\n",
      "('Epoch ', 21, ' Loss: ', 1.412664981558919)\n",
      "('Epoch ', 22, ' Loss: ', 1.337035108357668)\n",
      "('Epoch ', 23, ' Loss: ', 1.2687344178557396)\n",
      "('Epoch ', 24, ' Loss: ', 1.206766014918685)\n",
      "('Epoch ', 25, ' Loss: ', 1.1503042690455914)\n",
      "('Epoch ', 26, ' Loss: ', 1.098659067414701)\n",
      "('Epoch ', 27, ' Loss: ', 1.0512499082833529)\n",
      "('Epoch ', 28, ' Loss: ', 1.0075852684676647)\n",
      "('Epoch ', 29, ' Loss: ', 0.9672463806346059)\n"
     ]
    }
   ],
   "source": [
    "# size of the hidden vector\n",
    "n_hidden = 3\n",
    "\n",
    "# initialize the network\n",
    "rnn = RNN(n_vocab, n_hidden, n_vocab, n_vocab)\n",
    "\n",
    "n_epochs = 30\n",
    "learning_rate = .05\n",
    "criterion = nn.NLLLoss()\n",
    "optim = torch.optim.SGD(rnn.parameters(), lr = learning_rate)\n",
    "\n",
    "train(rnn, criterion, optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.4: Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model is similar to training it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_batch(batch, label):\n",
    "    if (batch.size()[0] != n_batch):\n",
    "        return 0, 0\n",
    "    \n",
    "    # initialize hidden state\n",
    "    hidden = Variable(torch.zeros(n_batch, n_hidden))\n",
    "    \n",
    "    # calculate forward pass\n",
    "    for i in range(batch[0].size()[0]):\n",
    "        output, hidden = rnn(batch[:, i], hidden)\n",
    "        \n",
    "    # calculate predictions\n",
    "    _, pred = output.max(1)\n",
    "\n",
    "    # calculate number of correct predictions\n",
    "    correct = (pred == label).long().sum().data[0]\n",
    "    return correct, n_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then calculate the total score by looping through the batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Percent correct: ', 1)\n"
     ]
    }
   ],
   "source": [
    "# Test loop\n",
    "\n",
    "batch_num = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(val_iter)):\n",
    "    batch, label = val_iter[i]\n",
    "    batch_correct, batch_size = test_batch(batch, label)\n",
    "    batch_num += 1\n",
    "    correct += batch_correct\n",
    "    total += batch_size\n",
    "    \n",
    "print(\"Percent correct: \", correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Using PyTorch RNN modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's RNN capabilities live [here](http://pytorch.org/docs/master/nn.html#recurrent-layers). We can use it as follows (note that the input is batched along the **second** dimension):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.2262 -0.0881  0.3527 -0.0746 -0.4060  0.2240  0.0088  0.0468 -0.1581\n",
      "  0.3601 -0.0734  0.1484 -0.1260 -0.2714  0.0355  0.3520  0.2879  0.2156\n",
      " -0.2936 -0.0323 -0.0249 -0.1940 -0.2356  0.2073  0.1414  0.0973 -0.4495\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.3059  0.2751  0.0088 -0.0933 -0.1202 -0.4326  0.0313 -0.1018  0.0146\n",
      " -0.1165 -0.3390 -0.0310 -0.0821 -0.0676  0.3112 -0.4281  0.3291 -0.0294\n",
      "  0.2613 -0.5185 -0.4656  0.0107 -0.0335  0.3838 -0.1666  0.4846 -0.4016\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.0253 -0.0641\n",
      "  0.2978 -0.2378\n",
      "  0.2789 -0.1280\n",
      "\n",
      "(1 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.1780 -0.0809  0.1363 -0.0823 -0.2173  0.0859  0.0469  0.0392 -0.1529\n",
      "  0.1640 -0.1072  0.2449 -0.1035 -0.2663  0.1396  0.0673  0.1351  0.0464\n",
      " -0.0974 -0.0674  0.0780 -0.1079 -0.0513  0.3051  0.0788  0.0656 -0.1932\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.1391  0.2459  0.0167  0.0307 -0.1048 -0.1051 -0.0197 -0.1417 -0.0278\n",
      "  0.0538 -0.0898  0.0028  0.0194 -0.0913  0.2075 -0.1135  0.2126  0.0112\n",
      "  0.2114 -0.1550 -0.3285  0.0861 -0.0571  0.2302 -0.0149  0.2655 -0.2434\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.0830 -0.0509\n",
      "  0.0407 -0.1349\n",
      "  0.0731 -0.1002\n",
      "\n",
      "(2 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.1264 -0.0497  0.0273 -0.0774 -0.1202  0.0960  0.0548  0.0409 -0.1159\n",
      "  0.1114 -0.0586  0.1856 -0.0862 -0.1172  0.1337  0.0367  0.0918 -0.0063\n",
      " -0.0262 -0.0291  0.0956 -0.0823 -0.0084  0.2503  0.0454  0.0575 -0.1136\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.0019  0.2107  0.0158  0.0648 -0.0554  0.0245 -0.0358 -0.0953 -0.0404\n",
      "  0.1176  0.0829  0.0088  0.0598 -0.0913  0.1814 -0.0680  0.1092  0.0232\n",
      "  0.1818  0.0150 -0.2370  0.0949 -0.0587  0.2238  0.0260  0.1967 -0.1442\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.1303 -0.0685\n",
      " -0.0736 -0.0603\n",
      "  0.0045 -0.0806\n",
      "\n",
      "(3 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0858 -0.0594  0.0051 -0.0482 -0.0687  0.1103  0.0580  0.0346 -0.0745\n",
      "  0.0644 -0.0307  0.1185 -0.0592 -0.0608  0.1527  0.0331  0.0638 -0.0143\n",
      "  0.0019 -0.0040  0.0574 -0.0653 -0.0191  0.2079  0.0348  0.0626 -0.0646\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.0765  0.1862 -0.0101  0.0754 -0.0306  0.0947 -0.0397 -0.0686 -0.0374\n",
      "  0.1522  0.1218  0.0057  0.0624 -0.0802  0.1617 -0.0465  0.0496  0.0296\n",
      "  0.1810  0.0854 -0.1534  0.0869 -0.0624  0.2114  0.0236  0.1303 -0.0773\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.1465 -0.0608\n",
      " -0.1183 -0.0426\n",
      " -0.0562 -0.0603\n",
      "\n",
      "(4 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0673 -0.0828 -0.0025 -0.0250 -0.0340  0.1306  0.0669  0.0217 -0.0547\n",
      "  0.0474 -0.0680  0.0599 -0.0272 -0.0433  0.1370  0.0395  0.0434 -0.0201\n",
      " -0.0089  0.0100  0.0279 -0.0384 -0.0471  0.1534  0.0153  0.0835 -0.0405\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.1162  0.1692 -0.0280  0.0784 -0.0112  0.1332 -0.0486 -0.0353 -0.0159\n",
      "  0.1270  0.1387 -0.0061  0.0728 -0.0544  0.1498 -0.0662 -0.0078  0.0281\n",
      "  0.1618  0.1038 -0.1137  0.0565 -0.0385  0.1921  0.0085  0.0563 -0.0658\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.1592 -0.0642\n",
      " -0.1420 -0.0145\n",
      " -0.0803 -0.0285\n",
      "[torch.FloatTensor of size 5x3x20]\n",
      ", (Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0843  0.0841 -0.0495  0.0846  0.1582 -0.0157  0.0605 -0.1624 -0.0826\n",
      "  0.1106 -0.1686 -0.0172  0.1249 -0.0312 -0.0956  0.1374  0.0018 -0.2894\n",
      "  0.0535 -0.0016 -0.2588 -0.0331 -0.1402 -0.0393  0.2627  0.1392  0.0326\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.0309 -0.1057 -0.0750  0.0089  0.0451 -0.0659 -0.2053  0.0676 -0.1388\n",
      "  0.1462  0.1241  0.0449  0.0897 -0.2387 -0.0162 -0.1310 -0.0099 -0.1569\n",
      "  0.3764 -0.1451  0.0133 -0.0552 -0.0462 -0.1855 -0.0048 -0.0766 -0.1491\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.0139 -0.0740\n",
      " -0.0747 -0.1643\n",
      " -0.3045 -0.1161\n",
      "\n",
      "(1 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0673 -0.0828 -0.0025 -0.0250 -0.0340  0.1306  0.0669  0.0217 -0.0547\n",
      "  0.0474 -0.0680  0.0599 -0.0272 -0.0433  0.1370  0.0395  0.0434 -0.0201\n",
      " -0.0089  0.0100  0.0279 -0.0384 -0.0471  0.1534  0.0153  0.0835 -0.0405\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.1162  0.1692 -0.0280  0.0784 -0.0112  0.1332 -0.0486 -0.0353 -0.0159\n",
      "  0.1270  0.1387 -0.0061  0.0728 -0.0544  0.1498 -0.0662 -0.0078  0.0281\n",
      "  0.1618  0.1038 -0.1137  0.0565 -0.0385  0.1921  0.0085  0.0563 -0.0658\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.1592 -0.0642\n",
      " -0.1420 -0.0145\n",
      " -0.0803 -0.0285\n",
      "[torch.FloatTensor of size 2x3x20]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.2207  0.1960 -0.1047  0.1655  0.4460 -0.0302  0.1549 -0.2871 -0.1701\n",
      "  0.1942 -0.3463 -0.0351  0.2095 -0.0709 -0.1659  0.3976  0.0042 -0.6653\n",
      "  0.1629 -0.0044 -0.5571 -0.1019 -0.1946 -0.1426  0.5551  0.3759  0.0495\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.0763 -0.2675 -0.1077  0.0219  0.0932 -0.2028 -0.3481  0.1734 -0.2532\n",
      "  0.2550  0.1940  0.0731  0.1637 -0.4375 -0.0393 -0.2230 -0.0353 -0.2391\n",
      "  0.6522 -0.3307  0.0399 -0.1003 -0.0692 -0.5046 -0.0100 -0.1607 -0.5782\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.0399 -0.2092\n",
      " -0.1661 -0.3206\n",
      " -0.4169 -0.3134\n",
      "\n",
      "(1 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.1406 -0.1493 -0.0042 -0.0504 -0.0636  0.2269  0.1272  0.0429 -0.1053\n",
      "  0.1038 -0.1194  0.1000 -0.0573 -0.0805  0.2478  0.0783  0.0815 -0.0397\n",
      " -0.0196  0.0173  0.0457 -0.0725 -0.0886  0.2647  0.0302  0.1520 -0.0768\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.1964  0.3305 -0.0517  0.1650 -0.0217  0.2693 -0.1110 -0.0641 -0.0323\n",
      "  0.2247  0.2547 -0.0111  0.1569 -0.1129  0.3110 -0.1532 -0.0139  0.0572\n",
      "  0.2764  0.1969 -0.1887  0.1183 -0.0803  0.4019  0.0200  0.0986 -0.1264\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.3104 -0.1525\n",
      " -0.2856 -0.0350\n",
      " -0.1583 -0.0709\n",
      "[torch.FloatTensor of size 2x3x20]\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "n_input = 10\n",
    "n_hidden = 20\n",
    "n_layers = 2\n",
    "n_batch = 3\n",
    "n_length = 5\n",
    "rnn = nn.LSTM(n_input, n_hidden, n_layers)\n",
    "input = Variable(torch.randn(n_length, n_batch, n_input))\n",
    "h0 = Variable(torch.randn(n_layers, n_batch, n_hidden))\n",
    "c0 = Variable(torch.randn(n_layers, n_batch, n_hidden))\n",
    "output, hn = rnn(input, (h0, c0))\n",
    "print(output, hn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a custom module to apply this module to our problem. This module will embed each integer, then apply the LSTM to the sequence, and then apply a linear and a softmax to get probabilities for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size, vocab_size, n_layers):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        # embed the input integers\n",
    "        embedded = self.embedding(input)\n",
    "        print(embedded.size()) # batch x length x embedding\n",
    "        \n",
    "        # put the batch along the second dimension\n",
    "        embedded = embedded.transpose(0, 1)\n",
    "        \n",
    "        # apply the LSTM\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # apply the linear and the softmax\n",
    "        output = self.softmax(self.linear(output))\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing are essentially the same as before, except that we no longer need to manually loop in the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_batch(model, criterion, optim, batch, label):\n",
    "    # initialize hidden vectors\n",
    "    hidden = (Variable(torch.zeros(n_layers, n_batch, n_hidden)), Variable(torch.zeros(n_layers, n_batch, n_hidden)))\n",
    "\n",
    "    # clear gradients\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    # calculate forward pass\n",
    "    output, hidden = model(batch, hidden)\n",
    "\n",
    "    # calculate loss    \n",
    "    loss = criterion(output[-1], label)\n",
    "\n",
    "    # backpropagate and step\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, criterion, optim):\n",
    "    for e in range(n_epochs):\n",
    "        batches = 0\n",
    "        epoch_loss = 0\n",
    "        avg_loss = 0\n",
    "        for batch, label in train_iter:\n",
    "            batch_loss = train_batch(model, criterion, optim, batch, label)\n",
    "            batches += 1\n",
    "            epoch_loss += batch_loss\n",
    "            avg_loss = ((avg_loss * (batches - 1)) + batch_loss) / batches\n",
    "        \n",
    "        print(\"Epoch \", e, \" Loss: \", epoch_loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c2a229231f9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-1cf79698faaa>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optim)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mbatches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-e8678ba4b5af>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(model, criterion, optim, batch, label)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# calculate forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Susan/anaconda/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-fa0c6562d89a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# apply the LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# apply the linear and the softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Susan/anaconda/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Susan/anaconda/lib/python2.7/site-packages/torch/nn/modules/rnn.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Susan/anaconda/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhack_onnx_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Susan/anaconda/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Susan/anaconda/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Susan/anaconda/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0;31m# hack to handle LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Susan/anaconda/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc\u001b[0m in \u001b[0;36mLSTMCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mgates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mingate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforgetgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcellgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# size of the embeddings and vectors\n",
    "n_embedding = 128\n",
    "n_hidden = 128\n",
    "\n",
    "# number of layers\n",
    "n_layers = 1\n",
    "\n",
    "# initialize LSTM\n",
    "rnn = MyLSTM(n_embedding, n_hidden, n_vocab, n_vocab, n_layers)\n",
    "\n",
    "n_epochs = 30\n",
    "learning_rate = .1\n",
    "criterion = nn.NLLLoss()\n",
    "optim = torch.optim.SGD(rnn.parameters(), lr = learning_rate)\n",
    "\n",
    "train(rnn, criterion, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test loop\n",
    "\n",
    "batch_num = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(val_iter)):\n",
    "    batch, label = val_iter[i]\n",
    "    batch_correct, batch_size = test_batch(batch, label)\n",
    "    batch_num += 1\n",
    "    correct += batch_correct\n",
    "    total += batch_size\n",
    "    \n",
    "print(\"Percent correct: \", correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [anaconda]",
   "language": "python",
   "name": "Python [anaconda]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
